[{"id":0,"href":"/docs/","title":"Introduction","section":"Introduction","content":"\rWelcome!\r#\rWelcome to the site hosting runbooks for alerts shipped with\nReason\r#\rKube-prometheus was always meant to provide the complete monitoring solution for kubernetes environments. The project already includes a lot of various components to fullfill this goal and one crucial part is including alerting rules. However what good are those alerting rules when one don\u0026rsquo;t know what to do when the alert fires?\nGoal\r#\rWe aim to ship meaningful runbook for every alert in kube-prometheus project and provide enough insight to help kube-prometheus users during incidents.\nHow to contribute?\r#\rIf you find any issues with current runbooks, please use the Edit this page link at the bottom of the runbook page.\n"},{"id":1,"href":"/runbooks/alertmanager/alertmanagerclustercrashlooping/","title":"Alertmanager Cluster Crashlooping","section":"alertmanager","content":"\rAlertmanagerClusterCrashlooping\r#\rMeaning\r#\rHalf or more of the Alertmanager instances within the same cluster are crashlooping.\nImpact\r#\rAlerts could be notified multiple time unless pods are crashing to fast and no alerts can be sent.\nDiagnosis\r#\rkubectl get pod -l app=alertmanager NAMESPACE NAME READY STATUS RESTARTS AGE default alertmanager-main-0 1/2 CrashLoopBackOff 37107 2d default alertmanager-main-1 2/2 Running 0 43d default alertmanager-main-2 2/2 Running 0 43d Find the root cause by looking to events for a given pod/deployement\nkubectl get events --field-selector involvedObject.name=alertmanager-main-0 Mitigation\r#\rMake sure pods have enough resources (CPU, MEM) to work correctly.\n"},{"id":2,"href":"/runbooks/alertmanager/alertmanagerclusterdown/","title":"Alertmanager Cluster Down","section":"alertmanager","content":"\rAlertmanagerClusterDown\r#\rMeaning\r#\rHalf or more of the Alertmanager instances within the same cluster are down.\nImpact\r#\rYou have an unstable cluster, if everything goes wrong you will lose the whole cluster.\nDiagnosis\r#\rVerify why pods are not running. You can get a big picture with events.\n$ kubectl get events --field-selector involvedObject.kind=Pod | grep alertmanager Mitigation\r#\rThere are no cheap options to mitigate this risk. Verifying any new changes in preprod before production environment should improve stability.\n"},{"id":3,"href":"/runbooks/alertmanager/alertmanagerclusterfailedtosendalerts/","title":"Alertmanager Cluster Failed To Send Alerts","section":"alertmanager","content":"\rAlertmanagerClusterFailedToSendAlerts\r#\rMeaning\r#\rAll instances failed to send notification to an integration.\nImpact\r#\rYou will not receive a notification when an alert is raised.\nDiagnosis\r#\rNo alerts are received at the integration level from the cluster.\nMitigation\r#\rDepending on the integration, correct the integration with the faulty instance (network, authorization token, firewall\u0026hellip;)\n"},{"id":4,"href":"/runbooks/alertmanager/alertmanagerconfiginconsistent/","title":"Alertmanager ConfigInconsistent","section":"alertmanager","content":"\rAlertmanagerConfigInconsistent\r#\rMeaning\r#\rThe configuration between instances inside a cluster is inconsistent.\nImpact\r#\rConfiguration inconsistency can be multiple and impact is hard to predict. Nevertheless, in most cases the alert might be lost or routed to the incorrect integration.\nDiagnosis\r#\rRun a diff tool between all alertmanager.yml that are deployed to find what is wrong. You could run a job within your CI to avoid this issue in the future.\nMitigation\r#\rDelete the incorrect secret and deploy the correct one.\n"},{"id":5,"href":"/runbooks/alertmanager/alertmanagerfailedreload/","title":"Alertmanager Failed Reload","section":"alertmanager","content":"\rAlertmanagerFailedReload\r#\rMeaning\r#\rThe alert AlertmanagerFailedReload is triggered when the Alertmanager instance for the cluster monitoring stack has consistently failed to reload its configuration for a certain period.\nImpact\r#\rThe impact depends on the type of the error you will find in the logs. Most of the time, previous configuration is still working, thanks to multiple instances, so avoid deleting existing pods.\nDiagnosis\r#\rVerify if there is an error in config-reloader container logs. Here an example with network issues.\n$ kubectl logs sts/alertmanager-main -c config-reloader level=error ts=2021-09-24T11:24:52.69629226Z caller=runutil.go:101 msg=\u0026#34;function failed. Retrying in next tick\u0026#34; err=\u0026#34;trigger reload: reload request failed: Post \\\u0026#34;http://localhost:9093/alertmanager/-/reload\\\u0026#34;: dial tcp [::1]:9093: connect: connection refused\u0026#34; You can also verify directly alertmanager.yaml file (default: /etc/alertmanager/config/alertmanager.yaml).\nMitigation\r#\rRunning amtool check-config alertmanager.yaml on your configuration file will help you detect problem related to syntax. You could also rollback alertmanager.yaml to the previous version in order to get back to a stable version.\n"},{"id":6,"href":"/runbooks/alertmanager/alertmanagerfailedtosendalerts/","title":"Alertmanager Failed To Send Alerts","section":"alertmanager","content":"\rAlertmanagerFailedToSendAlerts\r#\rMeaning\r#\rAt least one instance is unable to routed alert to the corresponding integration.\nImpact\r#\rNo impact since another instance should be able to send the notification, unless AlertmanagerClusterFailedToSendAlerts is also triggerd for the same integration.\nDiagnosis\r#\rVerify the amount of failed notification per alert-manager-[instance] for a specific integration.\nYou can look metrics exposed in prometheus console using promQL. For exemple the following query will display the number of failed notifications per instance for pager duty integration. We have 3 instances involved in the example bellow.\nrate(alertmanager_notifications_total{integration=\u0026#34;pagerduty\u0026#34;}[5m]) Mitigation\r#\rDepending on the integration, you can have a look to alert-manager logs and act (network, authorization token, firewall\u0026hellip;)\nDepending on the integration, you can have a look to alert-manager logs and act (network, authorization token, firewall\u0026hellip;)\nkubectl -n monitoring logs -l \u0026#39;alertmanager=main\u0026#39; -c alertmanager "},{"id":7,"href":"/runbooks/alertmanager/alertmanagermembersinconsistent/","title":"Alertmanager Members Inconsistent","section":"alertmanager","content":"\rAlertmanagerMembersInconsistent\r#\rMeaning\r#\rAt least one of alertmanager cluster members cannot be found.\nImpact\r#\rDiagnosis\r#\rCheck if IP addresses discovered by alertmanager cluster are the same ones as in alertmanager Service. Following example show possible inconsistency in Endpoint IP addresses:\n$ kubectl describe svc alertmanager-main Name: alertmanager-main Namespace: monitoring ... Endpoints: 10.128.2.3:9095,10.129.2.5:9095,10.131.0.44:9095 $ kubectl get pod -o wide | grep alertmanager-main alertmanager-main-0 5/5 Running 0 11d 10.129.2.6 alertmanager-main-1 5/5 Running 0 2d16h 10.131.0.44 alertmanager-main-2 5/5 Running 0 6d 10.128.2.3 Mitigation\r#\rDeleting an incorrect Endpoint should trigger its recreation with a correct IP address.\n"},{"id":8,"href":"/runbooks/prometheus-operator/configreloadersidecarerrors/","title":"Config Reloader Sidecar Errors","section":"prometheus-operator","content":"\rConfigReloaderSidecarErrors\r#\rMeaning\r#\rErrors encountered while the config-reloader sidecar attempts to sync configuration in a given namespace.\nImpact\r#\rAs a result, configuration for services such as prometheus or alertmanager maybe stale and cannot be automatically updated.'\nDiagnosis\r#\rCheck config-reloader logs and the configuration which it tries to reload.\nMitigation\r#\rUsually means new config was rejected by the controlled app because it contains errors such as unknown configuration sections or bad resource definitions.\nYou can prevent such issues with better config testing tools in CI/CD systems such as:\nyamllint yamale promtool jq yq (notice there is python and golang versions) conftest some apps have syntax checking command switch "},{"id":9,"href":"/runbooks/kubernetes/cputhrottlinghigh/","title":"CPU Throttling High","section":"kubernetes","content":"\rCPU Throttling High\r#\rMeaning\r#\rProcesses experience elevated CPU throttling.\nImpact\r#\rThe alert is purely informative and unless there is some other issue with the application, it can be skipped.\nDiagnosis\r#\rCheck if application is performing normally Check if CPU resource requests are adjusted accordingly to the app usage Check kernel version in the node Mitigation\r#\rNotice: User shouldn\u0026rsquo;t increase CPU limits unless the application is behaving erratically (another alert firing).\nFor this particular reason, the alert is inhibited by default in kube-prometheus and can be sent only if another alert in the same namespace is firing.\nWhen mixed with other alerts:\nGive specific container in the pod more CPU limits. Requests can stay the same.\nIn specific cases kubernetes node has too old kernel which is known to have issues with assigning CPU resources to the process see here\nIn certain scenarios ensure to use CPU Pinning and isolation - in short give to the container full CPU cores. Also ensure to update app so that it is aware it runs in cgroups, or explicitly set number of CPU it can use, or limit number of threads.\nLonger and more detailed info - PDF from Intel\n"},{"id":10,"href":"/runbooks/general/infoinhibitor/","title":"Info Inhibitor","section":"general","content":"\rInfoInhibitor\r#\rMeaning\r#\rThis is an alert that is used to inhibit info alerts.\nBy themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts.\nFull context\rMore information about the alert and design considerations can be found in a kube-prometheus issue\nImpact\r#\rAlert does not have any impact and it is used only as a workaround to a missing feature in alertmanager.\nDiagnosis\r#\rThis alert fires whenever there\u0026rsquo;s a severity=\u0026quot;info\u0026quot; alert, and stops firing when another alert with severity of warning or critical starts firing on the same namespace.\nMitigation\r#\rThis alert should be routed to a null receiver and configured to inhibit alerts with severity=\u0026quot;info\u0026quot;. Such configuration is available at https://github.com/prometheus-operator/kube-prometheus/blob/main/manifests/alertmanager-secret.yaml\n"},{"id":11,"href":"/runbooks/kubernetes/kubeaggregatedapidown/","title":"Kube Aggregated API Down","section":"kubernetes","content":"\rKubeAggregatedAPIDown\r#\rMeaning\r#\rKubernetes aggregated API has reported errors. It has appeared unavailable X times averaged over the past 10m.\nImpact\r#\rFrom minor such as inability to see cluster metrics to more severe such as unable to use custom metrics to scale or even unable to use cluster.\nDiagnosis\r#\rCheck networking on the node. Check firewall on the node. Investigate additional API logs. Investigate NetworkPolicies if kubeApi - additional API was not filtered out. Investigate NetworkPolicies if prometheus/additional api was not filtered out. Mitigation\r#\rTODO\nSee APIServer aggregation\n"},{"id":12,"href":"/runbooks/kubernetes/kubeaggregatedapierrors/","title":"Kube Aggregated API Errors","section":"kubernetes","content":"\rKubeAggregatedAPIErrors\r#\rMeaning\r#\rKubernetes aggregated API has reported errors. It has appeared unavailable over 4 times averaged over the past 10m.\nImpact\r#\rFrom minor such as inability to see cluster metrics to more severe such as unable to use custom metrics to scale or even unable to use cluster.\nDiagnosis\r#\rCheck networking on the node. Check firewall on the node. Investigate additional API logs. Investigate NetworkPolicies if kubeApi - additional API was not filtered out. Investigate NetworkPolicies if prometheus/additional API was not filtered out. Mitigation\r#\rTODO\nSee APIServer aggregation\n"},{"id":13,"href":"/runbooks/kubernetes/kubeapidown/","title":"Kube API Down","section":"kubernetes","content":"\rKubeAPIDown\r#\rMeaning\r#\rThe KubeAPIDown alert is triggered when all Kubernetes API servers have not been reachable by the monitoring system for more than 15 minutes.\nImpact\r#\rThis is a critical alert. The Kubernetes API is not responding. The cluster may partially or fully non-functional.\nApplications, which do not use kubernetes API directly, will continue to work. Changing kubernetes resources is not possible. in the cluster.\nServices using Kubernetes API directly will start to behave erratically.\nDiagnosis\r#\rCheck the status of the API server targets in the Prometheus UI.\nThen, confirm whether the API is also unresponsive for you:\n$ kubectl cluster-info If you can still reach the API server, there may be a network issue between the Prometheus instances and the API server pods. Check the status of the API server pods.\n$ kubectl -n kube-system get pods $ kubectl -n kube-system logs -l \u0026#39;component=kube-apiserver\u0026#39; Check networking on the node. Check firewall on the node. Investigate kube proxy logs. Investigate NetworkPolicies if prometheus/kubeApi was not filtered out. Mitigation\r#\rIf you can still reach the API server intermittently, you may be able treat this like any other failing deployment. If not, it\u0026rsquo;s possible you may have to refer to the disaster recovery documentation.\n"},{"id":14,"href":"/runbooks/kubernetes/kubeapierrorbudgetburn/","title":"Kube API Error Budget Burn","section":"kubernetes","content":"\rKubeAPIErrorBudgetBurn\r#\rImpact\r#\rThe overall availability of your Kubernetes cluster isn\u0026rsquo;t guaranteed any more. There may be too many errors returned by the APIServer and/or responses take too long for guarantee proper reconciliation.\nThis is always important; the only deciding factor is how urgent it is at the current rate\nFull context\rThis alert essentially means that a higher-than-expected percentage of the operations kube-apiserver is performing are erroring. Since random errors are inevitable, kube-apiserver has a \u0026ldquo;budget\u0026rdquo; of errors that it is allowed to make before triggering this alert.\nLearn more about Multiple Burn Rate Alerts in the SRE Workbook Chapter 5.\nCritical\r#\rFirst check the labels long and short.\nlong: 1h and short: 5m: less than ~2 days \u0026ndash; You should fix the problem as soon as possible! long: 6h and short: 30m: less than ~5 days \u0026ndash; Track this down now but no immediate fix required. Warning\r#\rFirst check the labels long and short.\nlong: 1d and short: 2h: less than ~10 days \u0026ndash; This is problematic in the long run. You should take a look in the next 24-48 hours. long: 3d and short: 6h: less than ~30 days \u0026ndash; (the entire window of the error budget) at this rate. This means that at the end of the next 30 days there won\u0026rsquo;t be any error budget left at this rate. It\u0026rsquo;s fine to leave this over the weekend and have someone take a look in the coming days at working hours. Example: If you have a 99% availability target this means that at the end of 30 days you\u0026rsquo;re going to be below 99% at this rate.\nRunbook\r#\rTake a look at the APIServer Grafana dashboard. At the very top check your current availability and how much percent of error budget is left. This should indicate the severity too. Do you see an elevated error rate in reads or writes? Do you see too many slow requests in reads or writes? Check the logs for kube-apiserver using the following Loki query: {component=\u0026quot;kube-apiserver\u0026quot;} Run debugging queries in Prometheus or Grafana Explore to dig deeper. If you don\u0026rsquo;t see anything obvious with the error rates, it might be too many slow requests. Check the queries below! Maybe it\u0026rsquo;s some dependency of the APIServer? etcd? Example Queries for slow requests\r#\rChange the rate window according to your long label from the alert. Make sure to update the alert threshold too, like \u0026gt; 0.01 to \u0026gt; 14.4 * 0.01 for example.\nSlow Read Requests\r#\rIf you don\u0026rsquo;t get any results back then there aren\u0026rsquo;t too many slow requests - that\u0026rsquo;s good. If you get results than you know what type of requests are too slow.\nCluster scoped:\n( sum(rate(apiserver_request_duration_seconds_bucket{job=\u0026#34;apiserver\u0026#34;,le=\u0026#34;40\u0026#34;,scope=\u0026#34;cluster\u0026#34;,verb=~\u0026#34;LIST|GET\u0026#34;}[3d])) - sum(rate(apiserver_request_duration_seconds_count{job=\u0026#34;apiserver\u0026#34;,verb=~\u0026#34;LIST|GET\u0026#34;}[3d])) ) / sum(rate(apiserver_request_total{job=\u0026#34;apiserver\u0026#34;,verb=~\u0026#34;LIST|GET\u0026#34;}[3d])) \u0026gt; 0.01 Namespace scoped:\n( sum(rate(apiserver_request_duration_seconds_bucket{job=\u0026#34;apiserver\u0026#34;,le=\u0026#34;5\u0026#34;,scope=\u0026#34;namespace\u0026#34;,verb=~\u0026#34;LIST|GET\u0026#34;}[3d])) - sum(rate(apiserver_request_duration_seconds_count{job=\u0026#34;apiserver\u0026#34;,verb=~\u0026#34;LIST|GET\u0026#34;}[3d])) ) / sum(rate(apiserver_request_total{job=\u0026#34;apiserver\u0026#34;,verb=~\u0026#34;LIST|GET\u0026#34;}[3d])) \u0026gt; 0.01 Resource scoped:\n( sum(rate(apiserver_request_duration_seconds_bucket{job=\u0026#34;apiserver\u0026#34;,le=\u0026#34;1\u0026#34;,scope=~\u0026#34;resource|\u0026#34;,verb=~\u0026#34;LIST|GET\u0026#34;}[3d])) or vector(0) - sum(rate(apiserver_request_duration_seconds_count{job=\u0026#34;apiserver\u0026#34;,verb=~\u0026#34;LIST|GET\u0026#34;}[3d])) ) / sum(rate(apiserver_request_total{job=\u0026#34;apiserver\u0026#34;,verb=~\u0026#34;LIST|GET\u0026#34;}[3d])) \u0026gt; 0.01 Slow Write Requests\r#\r( sum(rate(apiserver_request_duration_seconds_count{job=\u0026#34;apiserver\u0026#34;,verb=~\u0026#34;POST|PUT|PATCH|DELETE\u0026#34;}[3d])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\u0026#34;apiserver\u0026#34;,le=\u0026#34;1\u0026#34;,verb=~\u0026#34;POST|PUT|PATCH|DELETE\u0026#34;}[3d])) ) / sum(rate(apiserver_request_total{job=\u0026#34;apiserver\u0026#34;,verb=~\u0026#34;POST|PUT|PATCH|DELETE\u0026#34;}[3d])) \u0026gt; 0.01 "},{"id":15,"href":"/runbooks/kubernetes/kubeapiterminatedrequests/","title":"Kube API Terminated Requests","section":"kubernetes","content":"\rKubeAPITerminatedRequests\r#\rMeaning\r#\rThe apiserver has terminated over 20% of its incoming requests.\nImpact\r#\rClient will not be able to interact with the cluster. Some in-cluster services this may degrade or make service unavailable.\nDiagnosis\r#\rUse the apiserver_flowcontrol_rejected_requests_total metric to determine which flow schema is throttling the traffic to the API Server. The flow schema also provides information on the affected resources and subjects.\nMitigation\r#\rTODO\n"},{"id":16,"href":"/runbooks/kubernetes/kubeclientcertificateexpiration/","title":"Kube Client Certificate Expiration","section":"kubernetes","content":"\rKubeClientCertificateExpiration\r#\rMeaning\r#\rA client certificate used to authenticate to the apiserver is expiring in less than 7 days (warning alert) or 24 hours (critical alert).\nImpact\r#\rClient will not be able to interact with the cluster. In cluster services communicating with Kubernetes API may degrade or become unavailable.\nDiagnosis\r#\rCheck when certificate was issued and when it expires. Check serviceAccounts and service account tokens.\nMitigation\r#\rUpdate client certificate.\nFor in-cluster clients recreate pods.\n"},{"id":17,"href":"/runbooks/kubernetes/kubeclienterrors/","title":"Kube Client Errors","section":"kubernetes","content":"\rKubeClientErrors\r#\rMeaning\r#\rKubernetes API server client is experiencing over 1% error rate in the last 15 minutes.\nImpact\r#\rSpecific kubernetes client may malfunction. Service degradation.\nDiagnosis\r#\rUsual issues:\nnetworking errors too low resources to perform given API calls (usually too low CPU/memory requests) wrong api client (old libraries) investigate if the app does not request more data than it really requires from kubernetes API, for example it has too wide permissions and scans for resources in all namespaces. Check logs from client side (sometimes app logs).\nMitigation\r#\rTODO\n"},{"id":18,"href":"/runbooks/kubernetes/kubecontainerwaiting/","title":"Kube Container Waiting","section":"kubernetes","content":"\rKubeContainerWaiting\r#\rMeaning\r#\rContainer in pod is in Waiting state for too long.\nImpact\r#\rService degradation or unavailability.\nDiagnosis\r#\rCheck pod events via kubectl -n $NAMESPACE describe pod $POD. Check pod logs via kubectl -n $NAMESPACE logs $POD -c $CONTAINER Check for missing files such as configmaps/secrets/volumes Check for pod requests, especially special ones such as GPU. Check for node taints and capabilities. Mitigation\r#\rSee Container waiting\n"},{"id":19,"href":"/runbooks/kubernetes/kubecontrollermanagerdown/","title":"Kube Controller Manager Down","section":"kubernetes","content":"\rKubeControllerManagerDown\r#\rMeaning\r#\rKubeControllerManager has disappeared from Prometheus target discovery.\nImpact\r#\rThe cluster is not functional and Kubernetes resources cannot be reconciled.\nFull context\rMore about kube-controller-manager function can be found at https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/\nDiagnosis\r#\rTODO\nMitigation\r#\rSee old CoreOS docs in Web Archive\n"},{"id":20,"href":"/runbooks/kubernetes/kubecpuovercommit/","title":"Kube CPU Overcommit","section":"kubernetes","content":"\rKubeCPUOvercommit\r#\rMeaning\r#\rCluster has overcommitted CPU resource requests for Pods and cannot tolerate node failure.\nFull context\rTotal number of CPU requests for pods exceeds cluster capacity. In case of node failure some pods will not fit in the remaining nodes.\nImpact\r#\rThe cluster cannot tolerate node failure. In the event of a node failure, some Pods will be in Pending state.\nDiagnosis\r#\rCheck if CPU resource requests are adjusted to the app usage Check if some nodes are available and not cordoned Check if cluster-autoscaler has issues with adding new nodes Mitigation\r#\rAdd more nodes to the cluster - usually it is better to have more smaller nodes, than few bigger.\nAdd different node pools with different instance types to avoid problem when using only one instance type in the cloud.\nUse pod priorities to avoid important services from losing performance, see pod priority and preemption\nFine tune settings for special pods used with cluster-autoscaler\nPrepare performance tests for the expected workload, plan cluster capacity accordingly.\n"},{"id":21,"href":"/runbooks/kubernetes/kubecpuquotaovercommit/","title":"Kube CPU Quota Overcommit","section":"kubernetes","content":"\rKubeCPUQuotaOvercommit\r#\rMeaning\r#\rCluster has overcommitted CPU resource requests for Namespaces and cannot tolerate node failure.\nImpact\r#\rIn the event of a node failure, some Pods will be in Pending state due to a lack of available CPU resources.\nDiagnosis\r#\rCheck if CPU resource requests are adjusted to the app usage Check if some nodes are available and not cordoned Check if cluster-autoscaler has issues with adding new nodes Check if the given namespace usage grows in time more than expected Mitigation\r#\rReview existing quota for given namespace and adjust it accordingly.\nAdd more nodes to the cluster - usually it is better to have more smaller nodes, than few bigger.\nAdd different node pools with different instance types to avoid problem when using only one instance type in the cloud.\nUse pod priorities to avoid important services from losing performance, see pod priority and preemption\nFine tune settings for special pods used with cluster-autoscaler\nPrepare performance tests for the expected workload, plan cluster capacity accordingly.\n"},{"id":22,"href":"/runbooks/kubernetes/kubedaemonsetmisscheduled/","title":"Kube DaemonSet MisScheduled","section":"kubernetes","content":"\rKubeDaemonSetMisScheduled\r#\rMeaning\r#\rA number of pods of daemonset are running where they are not supposed to run.\nImpact\r#\rService degradation or unavailability. Excessive resource usage where they could be used by other apps.\nDiagnosis\r#\rUsually happens when specifying wrong pod nodeSelector/taints/affinities or node (node pools) were tainted and existing pods were not scheduled for eviction.\nCheck daemonset status via kubectl -n $NAMESPACE describe daemonset $NAME. Check DaemonSet update strategy Check the status of the pods which belong to the replica sets under the deployment. Check pod template parameters such as: pod priority - maybe it was evicted by other more important pods affinity rules - maybe due to affinities and not enough nodes it is not possible to schedule pods Check node taints and labels Check logs for node-feature-discovery and other supporting tools such as gpu-feature-discovery Mitigation\r#\rUpdate DaemonSet and apply change, delete pods manually.\n"},{"id":23,"href":"/runbooks/kubernetes/kubedaemonsetnotscheduled/","title":"Kube DaemonSet Not Scheduled","section":"kubernetes","content":"\rKubeDaemonSetNotScheduled\r#\rMeaning\r#\rA number of pods of daemonset are not scheduled.\nImpact\r#\rService degradation or unavailability.\nDiagnosis\r#\rUsually happens when specifying wrong pod taints/affinities or lack of resources on the nodes.\nCheck daemonset status via kubectl -n $NAMESPACE describe daemonset $NAME. Check DaemonSet update strategy Check the status of the pods which belong to the replica sets under the deployment. Check pod template parameters such as: pod priority - maybe it was evicted by other more important pods resources - maybe it tries to use unavailable resource, such as GPU but there is limited number of nodes with GPU affinity rules - maybe due to affinities and not enough nodes it is not possible to schedule pods Check if Horizontal Pod Autoscaler (HPA) is not triggered due to untested values (requests values). Check if cluster-autoscaler is able to create new nodes - see its logs or cluster-autoscaler status configmap. Mitigation\r#\rSet proper priority class for important dameonsets to system-node-critical.\nSee DaemonSet rolling update is stuck\nIn some rare cases you may need to change node affinities or delete pod manually if this is special daemonset which has specific pod priority class and is limited to only 1 replica (so it runs on specific node only)\nSee Debugging Pods\n"},{"id":24,"href":"/runbooks/kubernetes/kubedaemonsetrolloutstuck/","title":"Kube DaemonSet Rollout Stuck","section":"kubernetes","content":"\rKubeDaemonSetRolloutStuck\r#\rMeaning\r#\rDaemonSet update is stuck waiting for replaced pod.\nImpact\r#\rService degradation or unavailability.\nDiagnosis\r#\rCheck daemonset status via kubectl -n $NAMESPACE describe daemonset $NAME. Check DaemonSet update strategy Check the status of the pods which belong to the replica sets under the deployment. Check pod template parameters such as: pod priority - maybe it was evicted by other more important pods resources - maybe it tries to use unavailable resource, such as GPU but there is limited number of nodes with GPU affinity rules - maybe due to affinities and not enough nodes it is not possible to schedule pods pod termination grace period - if too long then pods may be for too long in terminating state Check if Horizontal Pod Autoscaler (HPA) is not triggered due to untested values (requests values). Check if cluster-autoscaler is able to create new nodes - see its logs or cluster-autoscaler status configmap. Mitigation\r#\rSee DaemonSet rolling update is stuck\nIn some rare cases you may need to change node affinities or delete pod manually if this is special daemonset which has pod priority class system-cluster-critical and is limited to only 1 replica (so it runs on specific node only)\nSee Debugging Pods\n"},{"id":25,"href":"/runbooks/kubernetes/kubedeploymentgenerationmismatch/","title":"Kube Deployment Generation Mismatch","section":"kubernetes","content":"\rKubeDeploymentGenerationMismatch\r#\rMeaning\r#\rDeployment generation mismatch due to possible roll-back.\nImpact\r#\rService degradation or unavailability.\nDiagnosis\r#\rSee Kubernetes Docs - Failed Deployment\nCheck out rollout history kubectl -n $NAMESPACE rollout history deployment $NAME Check rollout status if it is not paused Check deployment status via kubectl -n $NAMESPACE describe deployment $NAME. Check how many replicas are there declared. Investigate if new pods are not crashing. Check the status of the pods which belong to the replica sets under the deployment. Check pod template parameters such as: pod priority - maybe it was evicted by other more important pods resources - maybe it tries to use unavailable resource, such as GPU but there is limited number of nodes with GPU affinity rules - maybe due to affinities and not enough nodes it is not possible to schedule pods pod termination grace period - if too long then pods may be for too long in terminating state Check if Horizontal Pod Autoscaler (HPA) is not triggered due to untested values (requests values). Check if cluster-autoscaler is able to create new nodes - see its logs or cluster-autoscaler status configmap. Mitigation\r#\rDepending on the conditions usually adding new nodes solves the issue.\nOtherwise probably deployment or HPA definition needs to be fixed. If you can not add nodes then you can change rolling update strategy to Recreate. Sometimes manually deleting pod helps :)\nIn rare cases roll back to previous version - see Kubernetes Docs - Rolling Back\nIn extremely rare situations scale oldest ReplicaSets to 0 and delete them.\nSee Debugging Pods\n"},{"id":26,"href":"/runbooks/kubernetes/kubedeploymentreplicasmismatch/","title":"Kube Deployment Replicas Mismatch","section":"kubernetes","content":"\rKubeDeploymentReplicasMismatch\r#\rMeaning\r#\rDeployment has not matched the expected number of replicas.\nFull context\rKubernetes Deployment resource does not have number of replicas which were declared to be in operation. For example deployment is expected to have 3 replicas, but it has less than that for a noticeable period of time.\nIn rare occasions there may be more replicas than it should and system did not clean it up.\nImpact\r#\rService degradation or unavailability.\nDiagnosis\r#\rCheck deployment status via kubectl -n $NAMESPACE describe deployment $NAME. Check how many replicas are there declared. Check the status of the pods which belong to the replica sets under the deployment. Check pod template parameters such as: pod priority - maybe it was evicted by other more important pods resources - maybe it tries to use unavailable resource, such as GPU but there is limited number of nodes with GPU affinity rules - maybe due to affinities and not enough nodes it is not possible to schedule pods pod termination grace period - if too long then pods may be for too long in terminating state Check if Horizontal Pod Autoscaler (HPA) is not triggered due to untested values (requests values). Check if cluster-autoscaler is able to create new nodes - see its logs or cluster-autoscaler status configmap. Mitigation\r#\rDepending on the conditions usually adding new nodes solves the issue.\nOtherwise probably deployment or HPA definition needs to be fixed.\nSee Debugging Pods\n"},{"id":27,"href":"/runbooks/kubernetes/kubehpareplicasmismatch/","title":"Kube HPA  Replicas Mismatch","section":"kubernetes","content":"\rKubeHpaReplicasMismatch\r#\rMeaning\r#\rHorizontal Pod Autoscaler has not matched the desired number of replicas for longer than 15 minutes.\nImpact\r#\rHPA was unable to schedule desired number of pods.\nDiagnosis\r#\rCheck why HPA was unable to scale:\nnot enough nodes in the cluster hitting resource quotas in the cluster pods evicted due to pod priority Mitigation\r#\rIn case of cluster-autoscaler you may need to set up preemtive pod pools to ensure nodes are created on time.\n"},{"id":28,"href":"/runbooks/kubernetes/kubehpamaxedout/","title":"Kube HPA Maxed Out","section":"kubernetes","content":"\rKubeHpaMaxedOut\r#\rMeaning\r#\rHorizontal Pod Autoscaler has been running at max replicas for longer than 15 minutes.\nImpact\r#\rHorizontal Pod Autoscaler won\u0026rsquo;t be able to add new pods and thus scale application. Notice for some services maximizing HPA is in fact desired.\nDiagnosis\r#\rCheck why HPA was unable to scale:\nmax replicas too low too low value for requests such as CPU? Mitigation\r#\rIf using basic metrics like CPU/Memory then ensure to set proper values for requests. For memory based scaling ensure there are no memory leaks. If using custom metrics then fine-tune how app scales accordingly to it.\nUse performance tests to see how the app scales.\n"},{"id":29,"href":"/runbooks/kubernetes/kubejobcompletion/","title":"Kube Job Completion","section":"kubernetes","content":"\rKubeJobCompletion\r#\rMeaning\r#\rJob is taking more than 1h to complete.\nImpact\r#\rLong processing of batch jobs. Possible issues with scheduling next Job Diagnosis\r#\rCheck job via kubectl -n $NAMESPACE describe jobs $JOB. Check pod events via kubectl -n $NAMESPACE describe job $JOB. Mitigation\r#\rGive it more resources so it finishes faster, if applicable. See Job patterns "},{"id":30,"href":"/runbooks/kubernetes/kubejobfailed/","title":"Kube Job Failed","section":"kubernetes","content":"\rKubeJobFailed\r#\rMeaning\r#\rJob failed complete.\nImpact\r#\rFailure of processing of a scheduled task.\nDiagnosis\r#\rCheck job via kubectl -n $NAMESPACE describe jobs $JOB. Check pod events via kubectl -n $NAMESPACE describe pod $POD_FROM_JOB. Check pod logs via kubectl -n $NAMESPACE log pod $POD_FROM_JOB. Mitigation\r#\rSee Debugging Pods See Job patterns redesign job so that it is idempotent (can be re-run many times which will always produce the same output even if input differs) "},{"id":31,"href":"/runbooks/kubernetes/kubememoryovercommit/","title":"Kube Memory Overcommit","section":"kubernetes","content":"\rKubeMemoryOvercommit\r#\rMeaning\r#\rCluster has overcommitted Memory resource requests for Pods and cannot tolerate node failure.\nFull context\rTotal number of Memory requests for pods exceeds cluster capacity. In case of node failure some pods will not fit in the remaining nodes.\nImpact\r#\rThe cluster cannot tolerate node failure. In the event of a node failure, some Pods will be in Pending state.\nDiagnosis\r#\rCheck if Memory resource requests are adjusted to the app usage Check if some nodes are available and not cordoned Check if cluster-autoscaler has issues with adding new nodes Mitigation\r#\rAdd more nodes to the cluster - usually it is better to have more smaller nodes, than few bigger.\nAdd different node pools with different instance types to avoid problem when using only one instance type in the cloud.\nUse pod priorities to avoid important services from losing performance, see pod priority and preemption\nFine tune settings for special pods used with cluster-autoscaler\nPrepare performance tests for the expected workload, plan cluster capacity accordingly.\n"},{"id":32,"href":"/runbooks/kubernetes/kubememoryquotaovercommit/","title":"Kube Memory Quota Overcommit","section":"kubernetes","content":"\rKubeMemoryQuotaOvercommit\r#\rMeaning\r#\rCluster has overcommitted memory resource requests for Namespaces.\nImpact\r#\rVarious services degradation or unavailability in case of single node failure.\nDiagnosis\r#\rCheck if Memory resource requests are adjusted to the app usage Check if some nodes are available and not cordoned Check if cluster-autoscaler has issues with adding new nodes Check if the given namespace usage grows in time more than expected Mitigation\r#\rReview existing quota for given namespace and adjust it accordingly.\nAdd more nodes to the cluster - usually it is better to have more smaller nodes, than few bigger.\nAdd different node pools with different instance types to avoid problem when using only one instance type in the cloud.\nUse pod priorities to avoid important services from losing performance, see pod priority and preemption\nFine tune settings for special pods used with cluster-autoscaler\nPrepare performance tests for the expected workload, plan cluster capacity accordingly.\n"},{"id":33,"href":"/runbooks/kubernetes/kubenodenotready/","title":"Kube Node Not Ready","section":"kubernetes","content":"\rKubeNodeNotReady\r#\rMeaning\r#\rKubeNodeNotReady alert is fired when a Kubernetes node is not in Ready state for a certain period. In this case, the node is not able to host any new pods as described [here][KubeNode].\nImpact\r#\rThe performance of the cluster deployments is affected, depending on the overall workload and the type of the node.\nDiagnosis\r#\rThe notification details should list the node that\u0026rsquo;s not ready. For Example:\n- alertname = KubeNodeNotReady ... - node = node1.example.com ... Login to the cluster. Check the status of that node:\n$ kubectl get node $NODE -o yaml The output should describe why the node isn\u0026rsquo;t ready (e.g.: timeouts reaching the API or kubelet).\nMitigation\r#\rOnce, the problem was resolved that prevented node from being replaced, the instance should be terminated.\nSee KubeNode See node problem detector\n"},{"id":34,"href":"/runbooks/kubernetes/kubenodereadinessflapping/","title":"Kube Node Readiness Flapping","section":"kubernetes","content":"\rKubeNodeReadinessFlapping\r#\rMeaning\r#\rThe readiness status of node has changed few times in the last 15 minutes.\nImpact\r#\rThe performance of the cluster deployments is affected, depending on the overall workload and the type of the node.\nDiagnosis\r#\rThe notification details should list the node that\u0026rsquo;s not reachable. For Example:\n- alertname = KubeNodeUnreachable ... - node = node1.example.com ... Login to the cluster. Check the status of that node:\n$ kubectl get node $NODE -o yaml The output should describe why the node is not reachable.\nCommon failure scenarios:\ndisruptive software upgrades network patitioning due to hardware failures firewall rules virtual machines suspended due to storage area network problems system crashes / freezes due to software or hardware malfunctions Mitigation\r#\rIn case of maintenance ensure to cordon and drain node.\nIn other cases ensure storage and networking redundancy if applicable.\nSee KubeNode See node problem detector See Watchdog timer\n"},{"id":35,"href":"/runbooks/kubernetes/kubenodeunreachable/","title":"Kube Node Unreachable","section":"kubernetes","content":"\rKubeNodeUnreachable\r#\rMeaning\r#\rKubernetes node is unreachable and some workloads may be rescheduled.\nImpact\r#\rThe performance of the cluster deployments is affected, depending on the overall workload and the type of the node.\nDiagnosis\r#\rThe notification details should list the node that\u0026rsquo;s not reachable. For Example:\n- alertname = KubeNodeUnreachable ... - node = node1.example.com ... Login to the cluster. Check the status of that node:\n$ kubectl get node $NODE -o yaml The output should describe why the node is not reachable.\nCommon failure scenarios:\ndisruptive software upgrades network patitioning due to hardware failures firewall rules virtual machines suspended due to storage area network problems system crashes / freezes due to software or hardware malfunctions Mitigation\r#\rIn case of maintenance ensure to cordon and drain node.\nIn other cases ensure storage and networking redundancy if applicable.\nSee KubeNode See node problem detector See Watchdog timer\n"},{"id":36,"href":"/runbooks/kubernetes/kubepersistentvolumeerrors/","title":"Kube Persistent Volume Errors","section":"kubernetes","content":"\rKubePersistentVolumeErrors\r#\rMeaning\r#\rPersistentVolume is having issues with provisioning.\nImpact\r#\rVolue may be unavailable or have data erors (corrupted storage).\nService degradation, data loss.\nDiagnosis\r#\rCheck PV events via kubectl describe pv $PV. Check storage provider for logs. Check storage quotas in the cloud. Mitigation\r#\rIn happy scenario storage is just not provisioned as fast as expected. In worst scenario there is data corruption or data loss. Restore from backup.\n"},{"id":37,"href":"/runbooks/kubernetes/kubepersistentvolumefillingup/","title":"Kube Persistent Volume Filling Up","section":"kubernetes","content":"\rKubePersistentVolumeFillingUp\r#\rMeaning\r#\rThere can be various reasons why a volume is filling up. This runbook does not cover application specific reasons, only mitigations for volumes that are legitimately filling.\nAs always refer to recommended scenarios for given service.\nImpact\r#\rService degradation, switching to read only mode.\nDiagnosis\r#\rCheck app usage in time. Check if there are any configurations such as snapshotting, automatic data retention.\nMitigation\r#\rData retention\r#\rDeleting no longer needed data is the fastest and the cheapest solution.\nAsk the service owner if specific old data can be deleted. Enable data retention especially for snapshots, if possible.\nData export\r#\rIf data is not needed in the service but needs to be processed later then send it to somewhere else, for example to S3 bucket.\nData rebalance in the cluster\r#\rSome services automatically rebalance data on the cluster when one node fills up. Some allow to rebalance data across existing nodes, the other may require adding new nodes. If this is supported then increase number of replicas and wait for data migration or trigger it manually.\nExample services that support this:\ncassandra ceph elasticsearch/opensearch gluster hadoop kafka minio Notice: some services may require special scaling conditions such as adding twice more nodes than exist now.\nDirect Volume resizing\r#\rIf volume resizing is available, it\u0026rsquo;s easiest to increase the capacity of the volume.\nTo check if volume expansion is available, run this with your namespace and PVC-name replaced.\n$ kubectl get storageclass `kubectl -n \u0026lt;my-namespace\u0026gt; get pvc \u0026lt;my-pvc\u0026gt; -ojson | jq -r \u0026#39;.spec.storageClassName\u0026#39;` NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE standard (default) kubernetes.io/gce-pd Delete Immediate true 28d In this case ALLOWVOLUMEEXPANSION is true, so we can make use of the feature.\nTo resize the volume run:\n$ kubectl -n \u0026lt;my-namespace\u0026gt; edit pvc \u0026lt;my-pvc\u0026gt; And edit .spec.resources.requests.storage to the new desired storage size. Eventually the PVC status will say \u0026ldquo;Waiting for user to (re-)start a pod to finish file system resize of volume on node.\u0026rdquo;\nYou can check this with:\n$ kubectl -n \u0026lt;my-namespace\u0026gt; get pvc \u0026lt;my-pvc\u0026gt; Once the PVC status says to restart the respective pod, run this to restart it (this automatically finds the pod that mounts the PVC and deletes it, if you know the pod name, you can also just simply delete that pod):\n$ kubectl -n \u0026lt;my-namespace\u0026gt; delete pod `kubectl -n \u0026lt;my-namespace\u0026gt; get pod -ojson | jq -r \u0026#39;.items[] | select(.spec.volumes[] .persistentVolumeClaim.claimName==\u0026#34;\u0026lt;my-pvc\u0026gt;\u0026#34;) | .metadata.name\u0026#39;` Migrate data to a new, larger volume\r#\rWhen resizing is not available and the data is not safe to be deleted, then the only way is to create a larger volume and migrate the data.\nTODO\nPurge volume\r#\rWhen the data is ephemeral and volume expansion is not available, it may be best to purge the volume.\nWARNING/DANGER: This will permanently delete the data on the volume. Performing these steps is your responsibility.\nTODO\nMigrate data to new, larger instance pool in the same cluster\r#\rIn very specific scenarios it is better to schedule data migration in the same cluster but to a new instances. This is sometimes hard to accomplish due to the way how certain resources are managed in kubernetes.\nIn general procedure is like this:\nadd new nodes with bigger capacity than existing cluster trigger data migration scale in to 0 old instance pool and after that delete it. Migrate data to new, larger cluster\r#\rThis is most common scenario, but is much more expensive and may be a bit time consuming. Also sometimes this causes split brain issues when writing.\nIn general procedure is like this, this is only a suggestion, though:\ncreate data snapshot on existing cluster add new cluster with bigger capacity than existing cluster start data restore on new cluster based on the snapshot switch old cluster to read only mode reconfigure networking to point to new cluster trigger data migration from old cluster to new cluster to sync difference between snapshot and latest writes remove old cluster "},{"id":38,"href":"/runbooks/kubernetes/kubepodcrashlooping/","title":"Kube Pod Crash Looping","section":"kubernetes","content":"\rKubePodCrashLooping\r#\rMeaning\r#\rPod is in CrashLoop which means the app dies or is unresponsive and kubernetes tries to restart it automatically.\nImpact\r#\rService degradation or unavailability. Inability to do rolling upgrades. Certain apps will not perform required tasks such as data migrations.\nDiagnosis\r#\rCheck template via kubectl -n $NAMESPACE get pod $POD. Check pod events via kubectl -n $NAMESPACE describe pod $POD. Check pod logs via kubectl -n $NAMESPACE logs $POD -c $CONTAINER Check pod template parameters such as: pod priority resources - maybe it tries to use unavailable resource, such as GPU but there is limited number of nodes with GPU readiness and liveness probes may be incorrect - wrong port or command, check is failing too fast due to short timeout for response Other things to check:\napp responding extremely slow due to resource constraints such as memory too low, not enough CPU which is required on start app waits for other services to start, such as database misconfiguration causing app crash on start missing files such as configmaps/secrets/volumes read only filesystem wrong user permissions in container lack of special container capabilities (securityContext) app is executed in different directory than expected (for example WORKDIR from Docerkfile is not used in OpenShift) Mitigation\r#\rTalk with developers or read documentation about the app, ensure to define sane default values to start the app.\nSee Debugging Pods\n"},{"id":39,"href":"/runbooks/kubernetes/kubepodnotready/","title":"Kube Pod Not Ready","section":"kubernetes","content":"\rKubePodNotReady\r#\rMeaning\r#\rPod has been in a non-ready state for more than 15 minutes.\nState Running but not ready means readiness probe fails. State Pending means pod can not be created for specific namespace and node.\nFull context\rPod failed to reach ready state, depending on the readiness/liveness probes. See pod-lifecycle\nImpact\r#\rService degradation or unavailability. Pod not attached to service, thus not getting any traffic.\nDiagnosis\r#\rCheck template via kubectl -n $NAMESPACE get pod $POD. Check pod events via kubectl -n $NAMESPACE describe pod $POD. Check pod logs via kubectl -n $NAMESPACE logs $POD -c $CONTAINER Check pod template parameters such as: pod priority resources - maybe it tries to use unavailable resource, such as GPU but there is limited number of nodes with GPU readiness and liveness probes may be incorrect - wrong port or command, check is failing too fast due to short timeout for response stuck or long running init containers Other things to check:\napp responding extremely slow due to resource constraints such as memory too low, not enough CPU which is required on start app waits for other services to start, such as database misconfiguration causing app crash on start missing files such as configmaps/secrets/volumes read only filesystem wrong user permissions in container lack of special container capabilities (securityContext) app is executed in different directory than expected (for example WORKDIR from Docerkfile is not used in OpenShift) Mitigation\r#\rTalk with developers or read documentation about the app, ensure to define sane default values to start the app.\nSee Debugging Pods\n"},{"id":40,"href":"/runbooks/kubernetes/kubequotaalmostfull/","title":"Kube Quota Almost Full","section":"kubernetes","content":"\rKubeQuotaAlmostFull\r#\rMeaning\r#\rCluster reaches to the allowed limits for given namespace.\nImpact\r#\rIn the future deployments may not be possbile.\nDiagnosis\r#\rCheck resource usage for the namespace in given time span Mitigation\r#\rReview existing quota for given namespace and adjust it accordingly. Review resources used by the quota and fine tune them. Continue with standard capacity planning procedures. See Quotas "},{"id":41,"href":"/runbooks/kubernetes/kubequotaexceeded/","title":"Kube Quota Exceeded","section":"kubernetes","content":"\rKubeQuotaExceeded\r#\rMeaning\r#\rCluster reaches to the allowed hard limits for given namespace.\nImpact\r#\rInability to create resources in kubernetes.\nDiagnosis\r#\rCheck resource usage for the namespace in given time span Mitigation\r#\rReview existing quota for given namespace and adjust it accordingly. Review resources used by the quota and fine tune them. Continue with standard capacity planning procedures. See Quotas "},{"id":42,"href":"/runbooks/kubernetes/kubequotafullyused/","title":"Kube Quota Fully Used","section":"kubernetes","content":"\rKubeQuotaFullyUsed\r#\rMeaning\r#\rCluster reached allowed limits for given namespace.\nImpact\r#\rNew app installations may not be possible.\nDiagnosis\r#\rCheck resource usage for the namespace in given time span Mitigation\r#\rReview existing quota for given namespace and adjust it accordingly. Review resources used by the quota and fine tune them. Continue with standard capacity planning procedures. See Quotas "},{"id":43,"href":"/runbooks/kubernetes/kubeschedulerdown/","title":"Kube Scheduler Down","section":"kubernetes","content":"\rKubeSchedulerDown\r#\rMeaning\r#\rKube Scheduler has disappeared from Prometheus target discovery.\nImpact\r#\rThis is a critical alert. The cluster may partially or fully non-functional.\nDiagnosis\r#\rTo be added.\nMitigation\r#\rSee old CoreOS docs in Web Archive\n"},{"id":44,"href":"/runbooks/kube-state-metrics/kubestatemetricswatcherrors/","title":"Kube State Metric sWatch Errors","section":"kube-state-metrics","content":"\rKubeStateMetricsWatchErrors\r#\rMeaning\r#\rkube-state-metrics is experiencing errors in watch operations.\nImpact\r#\rUnable to get metrics for certain resources\nDiagnosis\r#\rCheck kube-state-metric container logs. Check service account token. Check networking rules and network policies.\nMitigation\r#\rTODO\n"},{"id":45,"href":"/runbooks/kube-state-metrics/kubestatemetricslisterrors/","title":"Kube State Metrics List Errors","section":"kube-state-metrics","content":"\rKubeStateMetricsListErrors\r#\rMeaning\r#\rkube-state-metrics is experiencing errors in list operations.\nImpact\r#\rUnable to get metrics for certain resources\nDiagnosis\r#\rCheck kube-state-metric container logs. Check service account token. Check networking rules and network policies.\nMitigation\r#\rTODO\n"},{"id":46,"href":"/runbooks/kube-state-metrics/kubestatemetricsshardingmismatch/","title":"Kube State Metrics Sharding Mismatch","section":"kube-state-metrics","content":"\rKubeStateMetricsShardingMismatch\r#\rMeaning\r#\rkube-state-metrics sharding is misconfigured.\nImpact\r#\rUnable to get metrics for certain resources. Some metrics can be unavailable.\nDiagnosis\r#\rCheck kube-state-metric container logs for each shard. Check service account token. Check networking rules and network policies.\nMitigation\r#\rTODO\n"},{"id":47,"href":"/runbooks/kube-state-metrics/kubestatemetricsshardsmissing/","title":"Kube State Metrics Shards Missing","section":"kube-state-metrics","content":"\rKubeStateMetricsShardsMissing\r#\rMeaning\r#\rkube-state-metrics shards are missing.\nImpact\r#\rUnable to get metrics for certain resources. Some metrics can be unavailable.\nDiagnosis\r#\rCheck kube-state-metric container logs for each shard. Check if certain pods were forcefully evicted. Check service account token. Check networking rules and network policies.\nMitigation\r#\rTODO\n"},{"id":48,"href":"/runbooks/kubernetes/kubestatefulsetgenerationmismatch/","title":"Kube StatefulSet Generation Mismatch","section":"kubernetes","content":"\rKubeStatefulSetGenerationMismatch\r#\rMeaning\r#\rStatefulSet generation mismatch due to possible roll-back.\nImpact\r#\rService degradation or unavailability.\nDiagnosis\r#\rSee Kubernetes Docs - Failed Deployment which can be also applied to StatefulSets to some extent\nCheck out rollout history kubectl -n $NAMESPACE rollout history statefulset $NAME Check rollout status if it is not paused Check deployment status via kubectl -n $NAMESPACE describe statefulset $NAME. Check how many replicas are there declared. Investigate if new pods are not crashing. Look at the issues with PersistentVolumes attached to StatefulSets. Check the status of the pods which belong to the replica sets under the deployment. Check pod template parameters such as: pod priority - maybe it was evicted by other more important pods resources - maybe it tries to use unavailable resource, such as GPU but there is limited number of nodes with GPU affinity rules - maybe due to affinities and not enough nodes it is not possible to schedule pods pod termination grace period - if too long then pods may be for too long in terminating state Check if Horizontal Pod Autoscaler (HPA) is not triggered due to untested values (requests values). Check if cluster-autoscaler is able to create new nodes - see its logs or cluster-autoscaler status configmap. Mitigation\r#\rStatefulsets are quite specific, and usually have special scripts on pod termination. See if there are special commands executed such as data migration, which may significantly slow down the progress.\nIn case of scale out usually adding new nodes solves the issue.\nOtherwise probably statefulset definition needs to be fixed.\nIn rare cases roll back to previous version - see Kubernetes Docs - Rolling Back\nIn extremely rare situations it may be better to delete problematic pods.\nSee Debugging Pods\n"},{"id":49,"href":"/runbooks/kubernetes/kubestatefulsetreplicasmismatch/","title":"Kube StatefulSet Replicas Mismatch","section":"kubernetes","content":"\rKubeStatefulSetReplicasMismatch\r#\rMeaning\r#\rStatefulSet has not matched the expected number of replicas.\nFull context\rKubernetes StatefulSet resource does not have number of replicas which were declared to be in operation. For example statefulset is expected to have 3 replicas, but it has less than that for a noticeable period of time.\nIn rare occasions there may be more replicas than it should and system did not clean it up.\nImpact\r#\rService degradation or unavailability.\nDiagnosis\r#\rCheck statefulset via kubectl -n $NAMESPACE describe statefulset $NAME. Check how many replicas are there declared. Check the status of the pods which belong to the replica sets under the statefulset. Check pod template parameters such as: pod priority - maybe it was evicted by other more importand pods resources - maybe it tries to use unavailabe resource, such as GPU but there is limited number of nodes with GPU affinity rules - maybe due to affinities and not enough nodes it is not possible to schedule pods pod termination grace period - if too long then pods may be for too long in terminating state Check if there are issues with attaching disks to statefulset - for example disk was in Zone A, but pod is scheduled in Zone B. Check if Horizontal Pod Autoscaler (HPA) is not triggered due to untested values (requests values). Check if cluster-autoscaler is able to create new nodes - see its logs or cluster-autoscaler status configmap. Mitigation\r#\rDepending on the conditions usually adding new nodes solves the issue.\nSet proper affinity rules to schedule pods in the same zone to avoid issues with volumes.\nSee Debugging Pods\n"},{"id":50,"href":"/runbooks/kubernetes/kubestatefulsetupdatenotrolledout/","title":"Kube StatefulSet Update Not RolledOut","section":"kubernetes","content":"\rKubeStatefulSetUpdateNotRolledOut\r#\rMeaning\r#\rStatefulSet update has not been rolled out.\nImpact\r#\rService degradation or unavailability.\nDiagnosis\r#\rCheck statefulset via kubectl -n $NAMESPACE describe statefulset $NAME. Check if statefuls update was not paused manually (see status) Check how many replicas are there declared. Check the status of the pods which belong to the replica sets under the statefulset. Check pod template parameters such as: pod priority - maybe it was evicted by other more importand pods resources - maybe it tries to use unavailabe resource, such as GPU but there is limited number of nodes with GPU affinity rules - maybe due to affinities and not enough nodes it is not possible to schedule pods pod termination grace period - if too long then pods may be for too long in terminating state Check if there are issues with attaching disks to statefulset - for example disk was in Zone A, but pod is scheduled in Zone B. Check if Horizontal Pod Autoscaler (HPA) is not triggered due to untested values (requests values). Check if cluster-autoscaler is able to create new nodes - see its logs or cluster-autoscaler status configmap. Mitigation\r#\rTODO\nSee Debugging Pods\n"},{"id":51,"href":"/runbooks/kubernetes/kubeversionmismatch/","title":"Kube Version Mismatch","section":"kubernetes","content":"\rKubeVersionMismatch\r#\rMeaning\r#\rDifferent semantic versions of Kubernetes components running. Usually happens during kubernetes cluster upgrade process.\nFull context\rKubernetes control plane nodes or worker nodes use different versions. This usually happens when kubernetes cluster is upgraded between minor and major version.\nImpact\r#\rIncompatible API versions between kubernetes components may have very broad range of issues, influencing single containers, through app stability, ending at whole cluster stability.\nDiagnosis\r#\rCheck existing kubernetes versions via kubectl get nodes and see VERSION column Check if there is ongoing kubernetes upgrade - especially in managed services in the cloud Mitigation\r#\rDrain affected nodes, then upgrade or replace them with newer ones, see Safely drain node\nEnsure to set proper control plane version and node pool versions when creating clusters.\nEnsure auto cluster updates for control plane and node pools.\nSet proper maintenance windows for the clusters.\n"},{"id":52,"href":"/runbooks/kubernetes/kubeletclientcertificateexpiration/","title":"Kubelet Client Certificate Expiration","section":"kubernetes","content":"\rKubeletClientCertificateExpiration\r#\rMeaning\r#\rClient certificate for Kubelet on node expires soon or already expired.\nImpact\r#\rNode will not be able to be used within the cluster.\nDiagnosis\r#\rCheck when certificate was issued and when it expires.\nMitigation\r#\rUpdate certificates in the cluster control nodes and the worker nodes. Refer to the documentation of the tool used to create cluster.\nAnother option is to delete node if it affects only one,\nIn extreme situations recreate cluster.\n"},{"id":53,"href":"/runbooks/kubernetes/kubeletclientcertificaterenewalerrors/","title":"Kubelet Client Certificate Renewal Errors","section":"kubernetes","content":"\rKubeletClientCertificateRenewalErrors\r#\rMeaning\r#\rKubelet on node has failed to renew its client certificate (XX errors in the last 15 minutes)\nImpact\r#\rNode will not be able to be used within the cluster.\nDiagnosis\r#\rCheck when certificate was issued and when it expires.\nMitigation\r#\rUpdate certificates in the cluster control nodes and the worker nodes. Refer to the documentation of the tool used to create cluster.\nAnother option is to delete node if it affects only one,\nIn extreme situations recreate cluster.\n"},{"id":54,"href":"/runbooks/kubernetes/kubeletdown/","title":"Kubelet Down","section":"kubernetes","content":"\rKubeletDown\r#\rMeaning\r#\rThis alert is triggered when the monitoring system has not been able to reach any of the cluster\u0026rsquo;s Kubelets for more than 15 minutes.\nImpact\r#\rThis alert represents a critical threat to the cluster\u0026rsquo;s stability. Excluding the possibility of a network issue preventing the monitoring system from scraping Kubelet metrics, multiple nodes in the cluster are likely unable to respond to configuration changes for pods and other resources, and some debugging tools are likely not functional, e.g. kubectl exec and kubectl logs.\nDiagnosis\r#\rCheck the status of nodes and for recent events on Node objects, or for recent events in general:\n$ kubectl get nodes $ kubectl describe node $NODE_NAME $ kubectl get events --field-selector \u0026#39;involvedObject.kind=Node\u0026#39; $ kubectl get events If you have SSH access to the nodes, access the logs for the Kubelet directly:\n$ journalctl -b -f -u kubelet.service Mitigation\r#\rThe mitigation depends on what is causing the Kubelets to become unresponsive. Check for wide-spread networking issues, or node level configuration issues.\nSee Kubernetes Docs - kubelet\n"},{"id":55,"href":"/runbooks/kubernetes/kubeletplegdurationhigh/","title":"Kubelet Pod Lifecycle Event Generator Duration High","section":"kubernetes","content":"\rKubeletPlegDurationHigh\r#\rMeaning\r#\rThe Kubelet Pod Lifecycle Event Generator has a 99th percentile duration of XX seconds on node.\nImpact\r#\rTODO\nDiagnosis\r#\rTODO\nMitigation\r#\rTODO\n"},{"id":56,"href":"/runbooks/kubernetes/kubeletpodstartuplatencyhigh/","title":"Kubelet Pod Start Up Latency High","section":"kubernetes","content":"\rKubeletPodStartUpLatencyHigh\r#\rMeaning\r#\rKubelet Pod startup 99th percentile latency is XX seconds on node.\nImpact\r#\rSlow pod starts.\nDiagnosis\r#\rUsually exhaused IOPS for node storage.\nMitigation\r#\rCordon and drain node and delete it. If issue persists look into the node logs.\n"},{"id":57,"href":"/runbooks/kubernetes/kubeletservercertificateexpiration/","title":"Kubelet Server Certificate Expiration","section":"kubernetes","content":"\rKubeletServerCertificateExpiration\r#\rMeaning\r#\rServer certificate for Kubelet on node expires soon or already expired.\nImpact\r#\rCritical - Cluster will be in inoperable state.\nDiagnosis\r#\rCheck when certificate was issued and when it expires.\nMitigation\r#\rUpdate certificates in the cluster control nodes and the worker nodes. Refer to the documentation of the tool used to create cluster.\nAnother option is to delete node if it affects only one,\nIn extreme situations recreate cluster.\n"},{"id":58,"href":"/runbooks/kubernetes/kubeletservercertificaterenewalerrors/","title":"Kubelet Server Certificate Renewal Errors","section":"kubernetes","content":"\rKubeletServerCertificateRenewalErrors\r#\rMeaning\r#\rKubelet on node has failed to renew its server certificate (XX errors in the last 5 minutes)\nImpact\r#\rCritical - Cluster will be in inoperable state.\nDiagnosis\r#\rCheck when certificate was issued and when it expires.\nMitigation\r#\rUpdate certificates in the cluster control nodes and the worker nodes. Refer to the documentation of the tool used to create cluster.\nAnother option is to delete node if it affects only one,\nIn extreme situations recreate cluster.\n"},{"id":59,"href":"/runbooks/kubernetes/kubelettoomanypods/","title":"Kubelet Too Many Pods","section":"kubernetes","content":"\rKubeletTooManyPods\r#\rMeaning\r#\rThe alert fires when a specific node is running \u0026gt;95% of its capacity of pods (110 by default).\nFull context\rKubelets have a configuration that limits how many Pods they can run. The default value of this is 110 Pods per Kubelet, but it is configurable (and this alert takes that configuration into account with the kube_node_status_capacity_pods metric).\nImpact\r#\rRunning many pods (more than 110) on a single node places a strain on the Container Runtime Interface (CRI), Container Network Interface (CNI), and the operating system itself. Approaching that limit may affect performance and availability of that node.\nDiagnosis\r#\rCheck the number of pods on a given node by running:\nkubectl get pods --all-namespaces --field-selector spec.nodeName=\u0026lt;node\u0026gt; Mitigation\r#\rSince Kubernetes only officially supports 110 pods per node, you should preferably move pods onto other nodes or expand your cluster with more worker nodes.\nIf you\u0026rsquo;re certain the node can handle more pods, you can raise the max pods per node limit by changing maxPods in your KubeletConfiguration (for kubeadm-based clusters) or changing the setting in your cloud provider\u0026rsquo;s dashboard (if supported).\n"},{"id":60,"href":"/runbooks/kubernetes/kubeproxydown/","title":"KubeProxy Down","section":"kubernetes","content":"\rKubeProxyDown\r#\rMeaning\r#\rThe KubeProxyDown alert is triggered when all Kubernetes Proxy instances have not been reachable by the monitoring system for more than 15 minutes.\nImpact\r#\rkube-proxy is a network proxy that runs on each node in your cluster, implementing part of the Kubernetes Service concept.\nkube-proxy maintains network rules on nodes. These network rules allow network communication to your Pods from network sessions inside or outside of your cluster.\nkube-proxy uses the operating system packet filtering layer if there is one and it\u0026rsquo;s available. Otherwise, kube-proxy forwards the traffic itself.\nDiagnosis\r#\rCheck the status of the kube-proxy daemon sets in the kube-system namespace.\nkubectl get pods -l k8s-app=kube-proxy -n kube-system Check the specific daemon-set for logs with the following command:\nkubectl logs -n kube-system kube-proxy-b9g23 Mitigation\r#\rAWS EKS\r#\rIf you are running AWS EKS cluster and you find that the kube-proxy pods are all running normally, make sure to update the kube-proxy-config cm as shown below.\nkubectl edit cm -n kube-system kube-proxy-config ... metricsBindAddress: 0.0.0.0:10249 ... This setting configures the IP address with port for the metrics server to serve on (set to \u0026lsquo;0.0.0.0:10249\u0026rsquo; for all IPv4 interfaces and \u0026lsquo;[::]:10249\u0026rsquo; for all IPv6 interfaces). More information on the documentation page\nThen just go delete kube-proxy pods and new ones will be created automatically.\nkubectl delete pod -l k8s-app=kube-proxy -n kube-system "},{"id":61,"href":"/runbooks/node/nodeclocknotsynchronising/","title":"Node Clock Not Synchronising","section":"node","content":"\rNodeClockNotSynchronising\r#\rMeaning\r#\rClock not synchronising.\nImpact\r#\rTime is not automatically synchronizing on the node. This can cause issues with handling TLS as well as problems with other time-sensitive applications.\nDiagnosis\r#\rTODO\nMitigation\r#\rSee Node Clok Skew Detected for mitigation steps.\n"},{"id":62,"href":"/runbooks/node/nodeclockskewdetected/","title":"Node Clock Skew Detected","section":"node","content":"\rNodeClockSkewDetected\r#\rMeaning\r#\rClock skew detected.\nImpact\r#\rTime is skewed on the node. This can cause issues with handling TLS as well as problems with other time-sensitive applications.\nDiagnosis\r#\rTODO\nMitigation\r#\rEnsure time synchronization service is running. Set proper time servers. Esure to sync time on server start, especially when using low power mode or hibernation.\nSome resource consuming process can cause issues on given hardware, so move it to different servers.\nOn physical servers check if on-board battery requires replacement. Check for hardware errors. Check for firmware updates. Ensure to use newer hardware (like server mainboard and so on).\n"},{"id":63,"href":"/runbooks/node/nodefiledescriptorlimit/","title":"Node File Descriptor Limit","section":"node","content":"\rNodeFileDescriptorLimit\r#\rMeaning\r#\rThis alert is triggered when a node\u0026rsquo;s kernel is found to be running out of available file descriptors \u0026ndash; a warning level alert at greater than 70% usage and a critical level alert at greater than 90% usage.\nImpact\r#\rApplications on the node may no longer be able to open and operate on files. This is likely to have severe consequences for anything scheduled on this node.\nDiagnosis\r#\rYou can open a shell on the node and use the standard Linux utilities to diagnose the issue:\n$ NODE_NAME=\u0026#39;\u0026lt;value of instance label from alert\u0026gt;\u0026#39; $ oc debug \u0026#34;node/$NODE_NAME\u0026#34; # sysctl -a | grep \u0026#39;fs.file-\u0026#39; fs.file-max = 1597016 fs.file-nr = 7104 0 1597016 # lsof -n Mitigation\r#\rReduce the number of files opened simultaneously by either adjusting application configuration or by moving some applications to other nodes.\n"},{"id":64,"href":"/runbooks/node/nodefilesystemalmostoutoffiles/","title":"Node Filesystem Almost Out Of Files","section":"node","content":"\rNodeFilesystemAlmostOutOfFiles\r#\rMeaning\r#\rThis alert is similar to the NodeFilesystemSpaceFillingUp alert, but rather than being based on a prediction that a filesystem will run out of inodes in a certain amount of time, it uses simple static thresholds. The alert will fire as at a warning level at 5% of available inodes left, and at a critical level with 3% of available inodes left.\nImpact\r#\rA node\u0026rsquo;s filesystem becoming full can have a far reaching impact, as it may cause any or all of the applications scheduled to that node to experience anything from performance degradation to full inoperability. Depending on the node and filesystem involved, this could pose a critical threat to the stability of the cluster.\nDiagnosis\r#\rMitigation\r#\rSee Node Filesystem FilesFilling Up\n"},{"id":65,"href":"/runbooks/node/nodefilesystemalmostoutofspace/","title":"Node Filesystem Almost Out Of Space","section":"node","content":"\rNodeFilesystemAlmostOutOfSpace\r#\rMeaning\r#\rThis alert is similar to the NodeFilesystemSpaceFillingUp alert, but rather than being based on a prediction that a filesystem will become full in a certain amount of time, it uses simple static thresholds. The alert will fire as at a warning level at 5% space left, and at a critical level with 3% space left.\nImpact\r#\rA node\u0026rsquo;s filesystem becoming full can have a far reaching impact, as it may cause any or all of the applications scheduled to that node to experience anything from performance degradation to full inoperability. Depending on the node and filesystem involved, this could pose a critical threat to the stability of the cluster.\nDiagnosis\r#\rMitigation\r#\rSee Node Filesystem FilesFilling Up\n"},{"id":66,"href":"/runbooks/node/nodefilesystemfilesfillingup/","title":"Node Filesystem Files Filling Up","section":"node","content":"\rNodeFilesystemFilesFillingUp\r#\rMeaning\r#\rThis alert is similar to the NodeFilesystemSpaceFillingUp alert, but predicts the filesystem will run out of inodes rather than bytes of storage space. The alert fires at a critical level when the filesystem is predicted to run out of available inodes within four hours.\nImpact\r#\rA node\u0026rsquo;s filesystem becoming full can have a far reaching impact, as it may cause any or all of the applications scheduled to that node to experience anything from performance degradation to full inoperability. Depending on the node and filesystem involved, this could pose a critical threat to the stability of the cluster.\nDiagnosis\r#\rNote the instance and mountpoint labels from the alert. You can graph the usage history of this filesystem with the following query in the OpenShift web console:\nnode_filesystem_files_free{ instance=\u0026#34;\u0026lt;value of instance label from alert\u0026gt;\u0026#34;, mountpoint=\u0026#34;\u0026lt;value of mountpoint label from alert\u0026gt;\u0026#34; } You can also open a debug session on the node and use the standard Linux utilities to locate the source of the usage:\n$ MOUNT_POINT=\u0026#39;\u0026lt;value of mountpoint label from alert\u0026gt;\u0026#39; $ NODE_NAME=\u0026#39;\u0026lt;value of instance label from alert\u0026gt;\u0026#39; $ oc debug \u0026#34;node/$NODE_NAME\u0026#34; $ df -hi \u0026#34;/host/$MOUNT_POINT\u0026#34; Note that in many cases a filesystem running out of inodes will still have available storage. Running out of inodes is often caused by many many small files being created by an application.\nMitigation\r#\rThe number of inodes allocated to a filesystem is usually based on the storage size. You may be able to solve the problem, or buy time, by increasing size of the storage volume. Otherwise, determine the application that is creating large numbers of files and adjust its configuration or provide it dedicated storage.\nSee Node Filesystem FilesFilling Up for additional mitigation steps.\n"},{"id":67,"href":"/runbooks/node/nodefilesystemspacefillingup/","title":"Node Filesystem Space Filling Up","section":"node","content":"\rNodeFilesystemSpaceFillingUp\r#\rMeaning\r#\rThis alert is based on an extrapolation of the space used in a file system. It fires if both the current usage is above a certain threshold and the extrapolation predicts to run out of space in a certain time. This is a warning-level alert if that time is less than 24h. It\u0026rsquo;s a critical alert if that time is less than 4h.\nFull context\rThe filesystem on Kubernetes nodes mainly consists of the operating system, [container ephemeral storage][1], container images, and container logs. Since Kubelet automatically handles [cleaning up old logs][2] and [deleting unused images][3], container ephemeral storage is a common cause of this alert. Although this alert may be triggered before Kubelet\u0026rsquo;s garbage collection kicks in.\nImpact\r#\rA filesystem running full is very bad for any process in need to write to the filesystem. But even before a filesystem runs full, performance is usually degrading.\nDiagnosis\r#\rStudy the recent trends of filesystem usage on a dashboard. Sometimes a periodic pattern of writing and cleaning up can trick the linear prediction into a false alert. Use the usual OS tools to investigate what directories are the worst and/or recent offenders. Is this some irregular condition, e.g. a process fails to clean up behind itself or is this organic growth? If monitoring is enabled, the following metric can be watched in PromQL.\nnode_filesystem_free_bytes Check the alert\u0026rsquo;s mountpoint label.\nMitigation\r#\rFor the case that the mountpoint label is /, /sysroot or /var; then removing unused images solves that issue:\nDebug the node by accessing the node filesystem:\n$ NODE_NAME=\u0026lt;instance label from alert\u0026gt; $ kubectl -n default debug node/$NODE_NAME $ chroot /host Remove dangling images:\n# TODO: Command needed Remove unused images:\n# TODO: Command needed Exit debug:\n$ exit $ exit 1 2 3 "},{"id":68,"href":"/runbooks/node/nodehighnumberconntrackentriesused/","title":"Node High Number Conntrack Entries Used","section":"node","content":"\rNodeHighNumberConntrackEntriesUsed\r#\rMeaning\r#\rNumber of conntrack are getting close to the limit.\nImpact\r#\rWhen reached the limit then some connections will be dropped, degrading service quality.\nDiagnosis\r#\rCheck current conntrack value on the node. Check which apps are generating a lot of connections.\nMitigation\r#\rMigrate some pods to another nodes. Bump conntrack limit directly on the node, remembering to make it persistent across node reboots.\n"},{"id":69,"href":"/runbooks/general/nodenetworkinterfaceflapping/","title":"Node Network Interface Flapping","section":"general","content":"\rNodeNetworkInterfaceFlapping\r#\rMeaning\r#\rNetwork interface is often changing its status\nImpact\r#\rApplications on the node may no longer be able to operate with other services. Network attached storage performance issues or even data loss.\nDiagnosis\r#\rInvestigate networking issues on the node and to connected hardware. Check physical cables, check networking firewall rules and so on.\nMitigation\r#\rCordon and drain node to migrate apps from it.\n"},{"id":70,"href":"/runbooks/node/nodenetworkreceiveerrs/","title":"Node Network Receive Errors","section":"node","content":"\rNodeNetworkReceiveErrs\r#\rMeaning\r#\rNetwork interface is reporting many receive errors.\nImpact\r#\rApplications on the node may no longer be able to operate with other services. Network attached storage performance issues or even data loss.\nDiagnosis\r#\rInvestigate networking issues on the node and to connected hardware. Check physical cables, check networking firewall rules and so on.\nMitigation\r#\rIn general mitigation landscape is quite vast, some suggestions:\nEnsure some node capacity is left unallocated (cpu/memory) for handling networking. Increase TX queue length Spread services to other nodes/pods. Replace physical cables, change ports. Look into introducting Quality of Service or other TCP congestion avoidance algorithms "},{"id":71,"href":"/runbooks/node/nodenetworktransmiterrs/","title":"Node Network Transmit Errors","section":"node","content":"\rNodeNetworkTransmitErrs\r#\rMeaning\r#\rNetwork interface is reporting many transmit errors.\nImpact\r#\rApplications on the node may no longer be able to operate with other services. Network attached storage performance issues or even data loss.\nDiagnosis\r#\rInvestigate networking issues on the node and to connected hardware. Check network interface saturation. Check CPU usage saturation. Check physical cables, check networking firewall rules and so on.\nMitigation\r#\rIn general mitigation landscape is quite vast, some suggestions:\nEnsure some node capacity is left unallocated (cpu/memory) for handling networking. Increase TX queue length Spread services to other nodes/pods. Replace physical cables, change ports. Look into introducting Quality of Service or other TCP congestion avoidance algorithms "},{"id":72,"href":"/runbooks/node/noderaiddegraded/","title":"Node RAID Degraded","section":"node","content":"\rNodeRAIDDegraded\r#\rMeaning\r#\rRAID Array is degraded.\nThis alert is triggered when a node has a storage configuration with RAID array, and the array is reporting as being in a degraded state due to one or more disk failures.\nImpact\r#\rThe affected node could go offline at any moment if the RAID array fully fails due to further issues with disks.\nDiagnosis\r#\rYou can open a shell on the node and use the standard Linux utilities to diagnose the issue, but you may need to install additional software in the debug container:\n$ NODE_NAME=\u0026#39;\u0026lt;value of instance label from alert\u0026gt;\u0026#39; $ oc debug \u0026#34;node/$NODE_NAME\u0026#34; $ cat /proc/mdstat Mitigation\r#\rCordon and drain node if possible, proceed to RAID recovery.\nSee the Red Hat Enterprise Linux [documentation][1] for potential steps.\n1 "},{"id":73,"href":"/runbooks/node/noderaiddiskfailure/","title":"Node RAID Disk Failure","section":"node","content":"\rNodeRAIDDiskFailure\r#\rSee Node RAID Degraded\n"},{"id":74,"href":"/runbooks/node/nodetextfilecollectorscrapeerror/","title":"Node Text File Collector Scrape Error","section":"node","content":"\rNodeTextFileCollectorScrapeError\r#\rMeaning\r#\rNode Exporter text file collector failed to scrape.\nImpact\r#\rMissing metrics from additional scripts.\nDiagnosis\r#\rCheck node_exporter logs Check script supervisor (like systemd or cron) for more information about failed script execution Mitigation\r#\rCheck if provided configuration is valid, if files were not renamed during upgrades.\n"},{"id":75,"href":"/runbooks/prometheus/prometheusbadconfig/","title":"Prometheus Bad Config","section":"prometheus","content":"\rPrometheusBadConfig\r#\rMeaning\r#\rAlert fires when Prometheus cannot successfully reload the configuration file due to the file having incorrect content.\nImpact\r#\rConfiguration cannot be reloaded and prometheus operates with last known good configuration. Configuration changes in any of Prometheus, Probe, PodMonitor, or ServiceMonitor objects may not be picked up by prometheus server.\nDiagnosis\r#\rCheck prometheus container logs for an explanation of which part of the configuration is problematic.\nUsually this can occur when ServiceMonitors or PodMonitors share the same job label.\nMitigation\r#\rRemove conflicting configuration option.\n"},{"id":76,"href":"/runbooks/prometheus/prometheusduplicatetimestamps/","title":"Prometheus Duplicate Timestamps","section":"prometheus","content":"\rPrometheusDuplicateTimestamps\r#\rFind the Prometheus Pod that concerns this.\n$ kubectl -n \u0026lt;namespace\u0026gt; get pod prometheus-k8s-0 2/2 Running 1 122m prometheus-k8s-1 2/2 Running 1 122m Look at the logs of each of them, there should be a log line such as:\n$ kubectl -n \u0026lt;namespace\u0026gt; logs prometheus-k8s-0 level=warn ts=2021-01-04T15:08:55.613Z caller=scrape.go:1372 component=\u0026#34;scrape manager\u0026#34; scrape_pool=default/main-ingress-nginx-controller/0 target=http://10.0.7.3:10254/metrics msg=\u0026#34;Error on ingesting samples with different value but same timestamp\u0026#34; num_dropped=16 Now there is a judgement call to make, this could be the result of:\nFaulty configuration, which could be resolved by removing the offending ServiceMonitor or PodMonitor object, which can be identified through the scrape_pool label in the log line, which is in the format of \u0026lt;namespace\u0026gt;/\u0026lt;service-monitor-name\u0026gt;/\u0026lt;endpoint-id\u0026gt;.\nThe target is reporting faulty data, sometimes this can be resolved by restarting the target, or it might need to be fixed in code of the offending application.\nFurther reading blog\n"},{"id":77,"href":"/runbooks/prometheus/prometheuserrorsendingalertstoanyalertmanager/","title":"Prometheus Error Sending Alerts To Any Alertmanager","section":"prometheus","content":"\rPrometheusErrorSendingAlertsToAnyAlertmanager\r#\rMeaning\r#\rPrometheus has encountered errors sending alerts to a any Alertmanager.\nImpact\r#\rAll alerts may be lost.\nDiagnosis\r#\rCheck connectivity issues between Prometheus and AlertManager cluster. Check NetworkPolicies, network saturation. Check if AlertManager is not overloaded or has not enough resources.\nMitigation\r#\rSet multiple AlertManager instances, spread them across nodes.\n"},{"id":78,"href":"/runbooks/prometheus/prometheuserrorsendingalertstosomealertmanagers/","title":"Prometheus Error Sending Alerts To Some Alertmanagers","section":"prometheus","content":"\rPrometheusErrorSendingAlertsToSomeAlertmanagers\r#\rMeaning\r#\rPrometheus has encountered more than 1% errors sending alerts to a specific Alertmanager.\nImpact\r#\rSome alerts may be lost.\nDiagnosis\r#\rCheck connectivity issues between Prometheus and AlertManager. Check NetworkPolicies, network saturation. Check if AlertManager is not overloaded or has not enough resources.\nMitigation\r#\rSet multiple AlertManager instances, spread them across nodes.\n"},{"id":79,"href":"/runbooks/prometheus/prometheuslabellimithit/","title":"Prometheus Label LimitHit","section":"prometheus","content":"\rPrometheusLabelLimitHit\r#\rMeaning\r#\rPrometheus has dropped targets because some scrape configs have exceeded the labels limit.\nImpact\r#\rMetrics and alerts may be missing or inaccurate.\nDiagnosis\r#\rMitigation\r#\rStart thinking about sharding prometheus. Increase scrape times to perform it less frequently.\n"},{"id":80,"href":"/runbooks/prometheus/prometheusmissingruleevaluations/","title":"Prometheus Missing Rule Evaluations","section":"prometheus","content":"\rPrometheusMissingRuleEvaluations\r#\rMeaning\r#\rPrometheus is missing rule evaluations due to slow rule group evaluation.\nImpact\r#\rMetrics and alerts may be missing or inaccurate.\nDiagnosis\r#\rCheck which rules fail, try to calcuate them differently.\nMitigation\r#\rSometimes giving more CPU is the only way to fix it.\n"},{"id":81,"href":"/runbooks/prometheus/prometheusnotconnectedtoalertmanagers/","title":"Prometheus Not Connected To Alertmanagers","section":"prometheus","content":"\rPrometheusNotConnectedToAlertmanagers\r#\rMeaning\r#\rPrometheus is not connected to any Alertmanagers.\nImpact\r#\rSending alerts is not possible.\nDiagnosis\r#\rCheck connectivity issues between Prometheus and AlertManager. Check NetworkPolicies, network saturation. Check if AlertManager is not overloaded or has not enough resources.\nMitigation\r#\rSet multiple AlertManager instances, spread them across nodes.\n"},{"id":82,"href":"/runbooks/prometheus/prometheusnotingestingsamples/","title":"Prometheus Not Ingesting Samples","section":"prometheus","content":"\rPrometheusNotIngestingSamples\r#\rMeaning\r#\rPrometheus is not ingesting samples.\nImpact\r#\rMissing metrics.\nDiagnosis\r#\rTODO\nMitigation\r#\rTODO\n"},{"id":83,"href":"/runbooks/prometheus/prometheusnotificationqueuerunningfull/","title":"Prometheus Notification Queue Running Full","section":"prometheus","content":"\rPrometheusNotificationQueueRunningFull\r#\rMeaning\r#\rPrometheus alert notification queue predicted to run full in less than 30m.\nImpact\r#\rFail to send alerts.\nDiagnosis\r#\rCheck prometheus container logs for an explanation of which part of the configuration is problematic.\nMitigation\r#\rRemove conflicting configuration option.\nCheck if there is an option to decrease number of alerts firing, for example by sharding prometheus.\n"},{"id":84,"href":"/runbooks/prometheus-operator/prometheusoperatorlisterrors/","title":"Prometheus Operator List Errors","section":"prometheus-operator","content":"\rPrometheusOperatorListErrors\r#\rMeaning\r#\rErrors while performing list operations in controller.\nImpact\r#\rPrometheus Operator has troubles in managing its operands and Custom Resources.\nDiagnosis\r#\rCheck logs of Prometheus Operator pod. Check service account tokens. Check Prometheus Operator RBAC configuration. Mitigation\r#\r"},{"id":85,"href":"/runbooks/prometheus-operator/prometheusoperatornodelookuperrors/","title":"Prometheus Operator Node Lookup Errors","section":"prometheus-operator","content":"\rPrometheusOperatorNodeLookupErrors\r#\rMeaning\r#\rErrors while reconciling information about kubernetes nodes.\nImpact\r#\rPrometheus Operator is not able to configure Prometheus scrape configuration.\nDiagnosis\r#\rCheck logs of Prometheus Operator pod. Check kubelet Service managed by Prometheus Operator $ kubelet describe Service -n kube-system -l app.kubernetes.io/managed-by=prometheus-operator ## Mitigation TODO "},{"id":86,"href":"/runbooks/prometheus-operator/prometheusoperatornotready/","title":"Prometheus Operator NotReady","section":"prometheus-operator","content":"\rPrometheusOperatorNotReady\r#\rMeaning\r#\rPrometheus operator is not ready.\nImpact\r#\rPrometheus Operator is not able to perform any operation.\nDiagnosis\r#\rCheck Prometheus Operator Deployment configuration. Check logs of Prometheus Operator pod. Check service account tokens. Mitigation\r#\rTODO\n"},{"id":87,"href":"/runbooks/prometheus-operator/prometheusoperatorreconcileerrors/","title":"Prometheus Operator Reconcile Errors","section":"prometheus-operator","content":"\rPrometheusOperatorReconcileErrors\r#\rMeaning\r#\rErrors while reconciling controller.\nImpact\r#\rPrometheus Operator will not be able to manage Prometheuses/Alertmanagers.\nDiagnosis\r#\rCheck logs of Prometheus Operator pod. Check service account tokens.\nMitigation\r#\r"},{"id":88,"href":"/runbooks/prometheus-operator/prometheusoperatorrejectedresources/","title":"Prometheus Operator Rejected Resources","section":"prometheus-operator","content":"\rPrometheusOperatorRejectedResources\r#\rMeaning\r#\rCustom Resources managed by Prometheus Operator were rejected and not propagated to operands (prometheus, alertmanager).\nImpact\r#\rCustom Resource won\u0026rsquo;t be used by prometheus-operator and thus configuration it carries won\u0026rsquo;t be translated to prometheus or alertmanager configuration.\nDiagnosis\r#\rCheck newly created Custom Resources like Prometheus, Alertmanager, Rules, Probes, ServiceMonitors, and others that have a CRD used by Prometheus Operator. Check logs of Prometheus Operator pod. Mitigation\r#\rFix newly created Custom Resource to conform to the schema defined in a CRD and reapply it to the cluster.\nConsider using a tool like kubeconform to validate newly created resources. You can check kube-prometheus integration of such a tool in the CI pipeline.\n"},{"id":89,"href":"/runbooks/prometheus-operator/prometheusoperatorsyncfailed/","title":"Prometheus Operator Sync Failed","section":"prometheus-operator","content":"\rPrometheusOperatorSyncFailed\r#\rMeaning\r#\rLast controller reconciliation failed\nImpact\r#\rPrometheus Operator will not be able to manage Prometheuses/Alertmanagers.\nDiagnosis\r#\rCheck logs of Prometheus Operator pod. Check service account tokens.\nMitigation\r#\r"},{"id":90,"href":"/runbooks/prometheus-operator/prometheusoperatorwatcherrors/","title":"Prometheus Operator Watch Errors","section":"prometheus-operator","content":"\rPrometheusOperatorWatchErrors\r#\rMeaning\r#\rErrors while performing watch operations in controller.\nImpact\r#\rPrometheus Operator will not be able to manage Prometheuses/Alertmanagers.\nDiagnosis\r#\rCheck logs of Prometheus Operator pod. Check service account tokens.\nMitigation\r#\r"},{"id":91,"href":"/runbooks/prometheus/prometheusoutofordertimestamps/","title":"Prometheus Out Of Order Timestamps","section":"prometheus","content":"\rPrometheusOutOfOrderTimestamps\r#\rMore information in blog\n"},{"id":92,"href":"/runbooks/prometheus/prometheusremotestoragefailures/","title":"Prometheus Remote Storage Failures","section":"prometheus","content":"\rPrometheusRemoteStorageFailures\r#\rMeaning\r#\rPrometheus fails to send samples to remote storage.\nImpact\r#\rMetrics and alerts may be missing or inaccurate.\nDiagnosis\r#\rCheck prometheus logs and remote storage logs. Investigate network issues. Check configs and credentials.\nMitigation\r#\rTODO\n"},{"id":93,"href":"/runbooks/prometheus/prometheusremotewritebehind/","title":"Prometheus Remote Write Behind","section":"prometheus","content":"\rPrometheusRemoteStorageFailures\r#\rMeaning\r#\rPrometheus remote write is behind.\nImpact\r#\rMetrics and alerts may be missing or inaccurate. Increased data lag between locations.\nDiagnosis\r#\rCheck prometheus logs and remote storage logs. Investigate network issues. Check configs and credentials.\nMitigation\r#\rProbbaly amout of data sent to remote system is too high for given network connectivity speed. You may need to limit which metrics to send to minimize transfers.\nSee Prometheus Remote Storage Failures\n"},{"id":94,"href":"/runbooks/prometheus/prometheusrulefailures/","title":"Prometheus Rule Failures","section":"prometheus","content":"\rPrometheusRuleFailures\r#\rMeaning\r#\rPrometheus is failing rule evaluations. Prometheus rules are incorrect or failed to calculate.\nImpact\r#\rMetrics and alerts may be missing or inaccurate.\nDiagnosis\r#\rYour best starting point is the rules page of the Prometheus UI (:9090/rules). It will show the error.\nYou can also evaluate the rule expression yourself, using the UI, or maybe using PromLens to help debug expression issues.\nMitigation\r#\rFix rules.\n"},{"id":95,"href":"/runbooks/prometheus/prometheustargetlimithit/","title":"Prometheus Target Limit Hit","section":"prometheus","content":"\rPrometheusTargetLimitHit\r#\rMeaning\r#\rPrometheus has dropped targets because some scrape configs have exceeded the targets limit.\nImpact\r#\rMetrics and alerts may be missing or inaccurate.\nDiagnosis\r#\rMitigation\r#\rStart thinking about sharding prometheus. Increase scrape times to perform it less frequently.\n"},{"id":96,"href":"/runbooks/prometheus/prometheustargetsyncfailure/","title":"Prometheus Target Sync Failure","section":"prometheus","content":"\rPrometheusTargetSyncFailure\r#\rMeaning\r#\rThis alert is triggered when at least one of the Prometheus instances has consistently failed to sync its configuration.\nImpact\r#\rMetrics and alerts may be missing or inaccurate.\nDiagnosis\r#\rDetermine whether the alert is for the cluster or user workload Prometheus by inspecting the alert\u0026rsquo;s namespace label.\nCheck the logs for the appropriate Prometheus instance:\n$ NAMESPACE=\u0026#39;\u0026lt;value of namespace label from alert\u0026gt;\u0026#39; $ oc -n $NAMESPACE logs -l \u0026#39;app=prometheus\u0026#39; level=error ... msg=\u0026#34;Creating target failed\u0026#34; ... Mitigation\r#\rIf the logs indicate a syntax or other configuration error, correct the corresponding ServiceMonitor, PodMonitor, or other configuration resource. In most all cases, the operator should prevent this from happening.\n"},{"id":97,"href":"/runbooks/prometheus/prometheustsdbcompactionsfailing/","title":"Prometheus TSDB Compactions Failing","section":"prometheus","content":"\rPrometheusTSDBCompactionsFailing\r#\rMeaning\r#\rPrometheus has issues compacting blocks.\nImpact\r#\rMetrics and alerts may be missing or inaccurate.\nDiagnosis\r#\rCheck storage used by the pod. This can happen if there is a lot of going on in the cluster and prometheus did not manage to compact data.\nMitigation\r#\rAt first just wait, it may fix itself after some time.\nIncrease Prometheus pod memory so that it caches more from disk. Try expanding volumes if they are too small or too slow. Change PVC storageClass to a more performant one.\n"},{"id":98,"href":"/runbooks/prometheus/prometheustsdbreloadsfailing/","title":"Prometheus TSDB Reloads Failing","section":"prometheus","content":"\rPrometheusTSDBReloadsFailing\r#\rMeaning\r#\rPrometheus has issues reloading blocks from disk.\nImpact\r#\rMetrics and alerts may be missing or inaccurate.\nDiagnosis\r#\rCheck storage used by the pod.\nMitigation\r#\rIncrease Prometheus pod memory so that it caches more from disk. Try expanding volumes if they are too small or too slow. Change PVC storageClass to a more performant one.\n"},{"id":99,"href":"/runbooks/prometheus/prometheusremotewritedesiredshards/","title":"PrometheusRemoteWriteDesiredShards","section":"prometheus","content":"\rPrometheusRemoteWriteDesiredShards\r#\rMeaning\r#\rPrometheus remote write desired shards calculation wants to run more than configured max shards.\nImpact\r#\rMetrics and alerts may be missing or inaccurate.\nDiagnosis\r#\rCheck metrics cardinality.\nCheck prometheus logs and remote storage logs. Investigate network issues. Check configs and credentials.\nMitigation\r#\rProbbaly amout of data sent to remote system is too high for given network connectivity speed. You may need to limit which metrics to send to minimize transfers.\nSee Prometheus Remote Storage Failures\n"},{"id":100,"href":"/runbooks/general/watchdog/","title":"Watchdog","section":"general","content":"\rWatchdog\r#\rMeaning\r#\rThis is an alert meant to ensure that the entire alerting pipeline is functional. This alert is always firing, therefore it should always be firing in Alertmanager and always fire against a receiver.\nImpact\r#\rIf not firing then it should alert external systems that this alerting system is no longer working.\nDiagnosis\r#\rMisconfigured alertmanager, bad credentials, bad endpoint, firewalls.. Check alertmanager logs.\nMitigation\r#\rThere are integrations with various notification mechanisms that send a notification when this alert is not firing. For example the DeadMansSnitch integration in PagerDuty.\n"},{"id":101,"href":"/runbooks/deployprod/prod-deploy-checklist-docker-compose-mysql/","title":"Prod Deploy Checklist Docker Compose Mysql","section":"Runbooks","content":"\rProduction Deployment Readiness Checklist\r#\rStack: Docker Compose (User FE/BE, Admin FE/BE, Batch), MySQL, Jenkins CI/CD, Nginx (domain/LB/IP TBD)\nPurpose: Highlevel checklist to prep and execute a production deployment. Fill all TBD fields before golive.\n0) Quick Header  fill these first\r#\rEnvironment name: \u0026lt;TBD\u0026gt; (e.g., prod-ap-southeast-1) Change window (UTC+7): \u0026lt;TBD date/time\u0026gt; Release tag / commit: \u0026lt;TBD\u0026gt; Owners / Oncall: \u0026lt;TBD\u0026gt; RTO / RPO targets: \u0026lt;TBD\u0026gt; (e.g., RTO 30m / RPO 5m) 1) Scope \u0026amp; Ownership\r#\rServices confirmed in scope: User FE, User BE, Admin FE, Admin BE, Batch jobs. Service owners + escalation contacts listed. Release scope frozen; artifacts tagged and immutable. Change ticket(s) created and approved (if required). 2) Infrastructure (WBS  details TBD)\r#\rCompute hosts sized and reserved (CPU/RAM/Storage headroom). OS hardened (updates, NTP, timezone, locales); baseline image pinned. Networking: VLAN/VPC, subnets, firewall rules (ingress/egress) TBD. Storage volumes for data, logs, backups planned and provisioned. Time synchronization verified across all hosts and containers. 3) Access \u0026amp; Governance\r#\rIAM: leastprivilege accounts for Jenkins, deployers, and ops. Breakglass procedure documented and tested. Secrets management defined (e.g., env files + Vault/KMS); no secrets in Git. Audit logging and access logs retention policy documented. 4) CI/CD  Jenkins\r#\rPipelines per component: User FE, User BE, Admin FE, Admin BE, Batch. Stages: build  unit tests  SAST/deps scan  package  push image/artifact  deploy to staging  E2E  manual/auto promote to prod. Artifact/Image registry configured (Nexus/Artifactory/ECR/GCR) with retention. Versioning strategy: semver or date+build; tags are immutable. Rollback job ready (redeploy previous tag). Jenkins credentials stored securely (no plaintext in Jenkinsfiles). Build reproducibility verified (locked base images, package locks). 5) Docker Compose  Structure \u0026amp; Conventions\r#\rFiles organized: docker-compose.yml (base), docker-compose.prod.yml (prod overrides) .env, .env.prod (nonsecret vars), secrets via external store or docker secrets .dockerignore for each build context Image sources decided: build locally or pull from registry (TBD). Networks separated: frontend, backend, db (isolate MySQL). Volumes declared for persistent data: MySQL data, app logs/cache as needed. Healthchecks for each service; depends_on uses healthcheck condition, not just start order. Resource limits set (cpus, mem_limit, ulimits) for each service. Restart policies set (unless-stopped or on-failure for jobs). Log drivers configured (json-file/syslog/fluentd) with rotation to avoid disk fill. Config per environment externalized; no hardcoded prod values in images. 6) Application Readiness (Java BE/FE)\r#\rJava: JDK version pinned; JVM flags (heap/GC) documented. Health endpoints implemented (/healthz, /readyz); graceful shutdown on SIGTERM. FE: production build (minified, hashed assets); env injection at runtime if needed. Config: DB URLs, API base URLs, feature flags set via env/secret. Idempotent DB migrations (e.g., Flyway/Liquibase) wired into deploy step. Static assets served efficiently (Nginx or dedicated FE container). 7) MySQL (Production)\r#\rVersion pinned (e.g., 8.0.x); image digest locked. Charset/collation set (utf8mb4 / utf8mb4_0900_ai_ci) and timezone configured. Users/roles and leastprivilege grants created. Connection pool sizing reviewed; max connections tuned. Backups: Schedule defined (full + incremental if applicable) Test restore performed and documented Backup encryption and offhost storage verified Slow query log enabled; performance schema sizing reviewed. Optional HA/replication decision documented (TBD). Initial data import / migration plan validated. 8) Nginx / Edge (Domain/LB/IP TBD)\r#\rPublic entry decided: direct Nginx on host vs behind LB (L4/L7) TBD. DNS records planned (A/AAAA/CNAME); cert strategy (ACME/issued) TBD. TLS configured (redirect HTTPHTTPS, HSTS, modern ciphers). Upstreams defined for User BE / Admin BE; timeouts, buffers, gzip/brotli tuned. Path routing \u0026amp; headers: CORS/CSRF rules, security headers, body size limits. Access/error logs rotated and shipped to central logging. Optional WAF/CDN/DDoS protection decision TBD. 9) Security\r#\rSAST/Dependency scan gates in CI; criticals addressed or waived with approval. Container hardening: nonroot users, readonly FS when possible, drop capabilities. Secrets not logged; PII masking; sample logs validated. SBOM generated and stored with artifacts (optional but recommended). 10) Observability\r#\rMetrics for BE/FE/MySQL/Nginx collected (RPS, latency, errors, JVM, DB QPS). Logs are structured (JSON) with correlation IDs across FEBE; retention set. Tracing headers propagated (W3C traceparent or similar). Dashboards per service + Nginx created. Alerts: availability/SLO, error spikes, saturation; oncall paging tested. 11) Performance \u0026amp; Capacity\r#\rLoad test targets defined (p95/p99 latency, throughput) and executed on staging. Bottlenecks identified; scaling plan or headroom documented. Caching strategy (client/server/CDN) documented. 12) Release Strategy\r#\rStrategy chosen: bluegreen or canary (document how with Compose). Preprod parity confirmed (configs, feature flags, secrets). Manual approval gates + signoffs complete. 13) PreGoLive Tests\r#\rSmoke tests pass in staging with prodlike config. DR/rollback drill: redeploy previous tag via Compose (verified). Failure simulation for one dependency (optional). 14) GoLive Day Runbook\r#\rChange freeze window confirmed; stakeholders notified. Fresh DB backup/snapshot taken and verified. Deploy order: DB migrations  Backends  Frontends  Nginx reload. Postdeploy smoke tests (synthetic + manual) all green. Monitor golden signals for \u0026lt;X\u0026gt; minutes; hold change before closing. 15) PostDeploy\r#\rAnnounce success; unfreeze. Compare metrics vs baseline; note regressions. Tag release artifacts; write release notes. Schedule retro if any incidents. 16) Batch Jobs (JenkinsDriven)\r#\rJobs list with owners and SLAs (start/end windows). Idempotent design; retries with backoff; concurrency controls. Configs/env externalized; secrets in credentials store. Metrics/logs/alerts per job (success/fail, duration, processed counts). Failure policies defined (autoretry, notify, rollback/data fix). Schedules versioncontrolled (Jenkinsfile/crontab in repo). Prerun health checks for dependencies (DB/APIs/storage). 17) Inventory (fill before golive)\r#\rService URLs: User FE: \u0026lt;TBD\u0026gt;  User BE: \u0026lt;TBD\u0026gt;  Admin FE: \u0026lt;TBD\u0026gt;  Admin BE: \u0026lt;TBD\u0026gt; Nginx public endpoint(s): \u0026lt;TBD\u0026gt; DNS records (hostnames): \u0026lt;TBD\u0026gt; Certificates (issuer, expiry): \u0026lt;TBD\u0026gt; MySQL host/port/database(s): \u0026lt;TBD\u0026gt; Registry image tags (FE/BE/Batch): \u0026lt;TBD\u0026gt; Notes\r#\rKeep this checklist in the repo (e.g., /docs/prod-deploy-checklist.md) and update per release. Treat TBD items as blockers until filled.\n"},{"id":102,"href":"/runbooks/etcd/etcdbackendquotalowspace/","title":"Etcd Backend Quota Low Space","section":"etcd","content":"\retcdBackendQuotaLowSpace\r#\rMeaning\r#\rThis alert fires when the total existing DB size exceeds 95% of the maximum DB quota. The consumed space is in Prometheus represented by the metric etcd_mvcc_db_total_size_in_bytes, and the DB quota size is defined by etcd_server_quota_backend_bytes.\nImpact\r#\rIn case the DB size exceeds the DB quota, no writes can be performed anymore on the etcd cluster. This further prevents any updates in the cluster, such as the creation of pods.\nDiagnosis\r#\rThe following two approaches can be used for the diagnosis.\nCLI Checks\r#\rTo run etcdctl commands, we need to rsh into the etcdctl container of any etcd pod.\n$ NAMESPACE=\u0026#34;kube-etcd\u0026#34; $ kubectl rsh -c etcdctl -n $NAMESPACE $(kubectl get po -l app=etcd -oname -n $NAMESPACE | awk -F\u0026#34;/\u0026#34; \u0026#39;NR==1{ print $2 }\u0026#39;) Validate that the etcdctl command is available:\n$ etcdctl version etcdctl can be used to fetch the DB size of the etcd endpoints.\n$ etcdctl endpoint status -w table PromQL queries\r#\rCheck the percentage consumption of etcd DB with the following query in the metrics console:\n(etcd_mvcc_db_total_size_in_bytes / etcd_server_quota_backend_bytes) * 100 Check the DB size in MB that can be reduced after defragmentation:\n(etcd_mvcc_db_total_size_in_bytes - etcd_mvcc_db_total_size_in_use_in_bytes)/1024/1024 Mitigation\r#\rCapacity planning\r#\rIf the etcd_mvcc_db_total_size_in_bytes shows that you are growing close to the etcd_server_quota_backend_bytes, etcd almost reached max capacity and it\u0026rsquo;s start planning for new cluster.\nIn the meantime before migration happens, you can use defrag to gain some time.\nDefrag\r#\rWhen the etcd DB size increases, we can defragment existing etcd DB to optimize DB consumption as described in etcdDefragmentation. Run the following command in all etcd pods.\n$ etcdctl defrag As validation, check the endpoint status of etcd members to know the reduced size of etcd DB. Use for this purpose the same diagnostic approaches as listed above. More space should be available now.\n"},{"id":103,"href":"/runbooks/etcd/etcdgrpcrequestsslow/","title":"Etcd Grpcrequests Slow","section":"etcd","content":"\retcdGRPCRequestsSlow\r#\rMeaning\r#\rThis alert fires when the 99th percentile of etcd gRPC requests are too slow.\nImpact\r#\rWhen requests are too slow, they can lead to various scenarios like leader election failure, slow reads and writes.\nDiagnosis\r#\rThis could be result of slow disk (due to fragmented state) or CPU contention.\nSlow disk\r#\rOne of the most common reasons for slow gRPC requests is disk. Checking disk related metrics and dashboards should provide a more clear picture.\nPromQL queries used to troubleshoot\r#\rVerify the value of how slow the etcd gRPC requests are by using the following query in the metrics console:\nhistogram_quantile(0.99, sum(rate(grpc_server_handling_seconds_bucket{job=~\u0026#34;.*etcd.*\u0026#34;, grpc_type=\u0026#34;unary\u0026#34;}[5m])) without(grpc_type)) That result should give a rough timeline of when the issue started.\netcd_disk_wal_fsync_duration_seconds_bucket reports the etcd disk fsync duration, etcd_server_leader_changes_seen_total reports the leader changes. To rule out a slow disk and confirm that the disk is reasonably fast, 99th percentile of the etcd_disk_wal_fsync_duration_seconds_bucket should be less than 10ms. Query in metrics UI:\nhistogram_quantile(0.99, sum by (instance, le) (irate(etcd_disk_wal_fsync_duration_seconds_bucket{job=\u0026#34;etcd\u0026#34;}[5m]))) Console dashboards\r#\rIn the OpenShift dashboard console under Observe section, select the etcd dashboard. There are both RPC rate as well as Disk Sync Duration dashboards which will assist with further issues.\nResource exhaustion\r#\rIt can happen that etcd responds slower due to CPU resource exhaustion. This was seen in some cases when one application was requesting too much CPU which led to this alert firing for multiple methods.\nOften if this is the case, we also see etcd_disk_wal_fsync_duration_seconds_bucket slower as well.\nTo confirm this is the cause of the slow requests either:\nIn OpenShift console on primary page under \u0026ldquo;Cluster utilization\u0026rdquo; view the requested CPU vs available.\nPromQL query is the following to see top consumers of CPU:\ntopk(25, sort_desc( sum by (namespace) ( ( sum(avg_over_time(pod:container_cpu_usage:sum{container=\u0026#34;\u0026#34;,pod!=\u0026#34;\u0026#34;}[5m])) BY (namespace, pod) * on(pod,namespace) group_left(node) (node_namespace_pod:kube_pod_info:) ) * on(node) group_left(role) (max by (node) (kube_node_role{role=~\u0026#34;.+\u0026#34;})) ) )) Mitigation\r#\rFragmented state\r#\rIn the case of slow fisk or when the etcd DB size increases, we can defragment existing etcd DB to optimize DB consumption as described in etcdDefragmentation. Run the following command in all etcd pods.\n$ etcdctl defrag As validation, check the endpoint status of etcd members to know the reduced size of etcd DB. Use for this purpose the same diagnostic approaches as listed above. More space should be available now.\nFurther info on etcd best practices can be found in the etcdPractices.\n"},{"id":104,"href":"/runbooks/etcd/etcdhighfsyncdurations/","title":"Etcd High Fsync Durations","section":"etcd","content":"\retcdHighFsyncDurations\r#\rMeaning\r#\rThis alert fires when the 99th percentile of etcd disk fsync duration is too high for 10 minutes.\nFull context\rEvery write request sent to etcd has to be [fsync\u0026rsquo;d][fsync] to disk by the leader node, transmitted to its peers, and fsync\u0026rsquo;d to those disks as well before etcd can tell the client that the write request succeeded (as part of the [Raft consensus algorithm][raft]). As a result of all those fsync\u0026rsquo;s, etcd cares a LOT about disk latency, which this alert picks up on.\nEtcd instances perform poorly on network-attached storage. Directly-attached spinning disks may work, but solid-state disks or better [are recommended][etcd-disks] for larger clusters. For very large clusters, you may even consider a [separate etcd cluster just for events][etcd-events] to reduce the write load.\nImpact\r#\rWhen this happens it can lead to various scenarios like leader election failure, frequent leader elections, slow reads and writes.\nDiagnosis\r#\rThis could be result of slow disk possibly due to fragmented state in etcd or simply due to slow disk.\nSlow disk\r#\rChecking disk related metrics and dashboards should provide a more clear picture.\nPromQL queries used to troubleshoot\r#\retcd_disk_wal_fsync_duration_seconds_bucket reports the etcd disk fsync duration, etcd_server_leader_changes_seen_total reports the leader changes. To rule out a slow disk and confirm that the disk is reasonably fast, 99th percentile of the etcd_disk_wal_fsync_duration_seconds_bucket should be less than 10ms. Query in metrics UI:\nhistogram_quantile(0.99, sum by (instance, le) (irate(etcd_disk_wal_fsync_duration_seconds_bucket{job=\u0026#34;etcd\u0026#34;}[5m]))) Mitigation\r#\rFragmented state\r#\rIn the case of slow fisk or when the etcd DB size increases, we can defragment existing etcd DB to optimize DB consumption as described in [here][etcdDefragmentation]. Run the following command in all etcd pods.\n$ etcdctl defrag As validation, check the endpoint status of etcd members to know the reduced size of etcd DB. Use for this purpose the same diagnostic approaches as listed above. More space should be available now.\nFurther info on etcd best practices can be found in the [OpenShift docs here][etcdPractices].\nfsync raft etcd-disks etcd-events etcdDefragmentation etcdPractices "},{"id":105,"href":"/runbooks/etcd/etcdhighnumberoffailedgrpcrequests/","title":"Etcd High Number of Failed Grpcrequests","section":"etcd","content":"\retcdHighNumberOfFailedGRPCRequests\r#\rMeaning\r#\rThis alert fires when at least 5% of etcd gRPC requests failed in the past 10 minutes.\nImpact\r#\rFirst establish which gRPC method is failing, this will be visible in the alert. If it\u0026rsquo;s not part of the alert, the following query will display method and etcd instance that has failing requests:\n100 * sum without(grpc_type, grpc_code) (rate(grpc_server_handled_total{grpc_code=~\u0026#34;Unknown|FailedPrecondition|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded\u0026#34;,job=\u0026#34;etcd\u0026#34;}[5m])) / sum without(grpc_type, grpc_code) (rate(grpc_server_handled_total{job=\u0026#34;etcd\u0026#34;}[5m])) \u0026gt; 5 and on() (sum(cluster_infrastructure_provider{type!~\u0026#34;ipi|BareMetal\u0026#34;} == bool 1)) Diagnosis\r#\rAll the gRPC errors should also be logged in each respective etcd instance logs. You can get the instance name from the alert that is firing or by running the query detailed above. Those etcd instance logs should serve as further insight into what is wrong.\nTo get logs of etcd containers either check the instance from the alert and check logs directly or run the following:\nNAMESPACE=\u0026#34;kube-etcd\u0026#34; kubectl logs -n $NAMESPACE -lapp=etcd etcd Mitigation\r#\rDepending on the above diagnosis, the issue will most likely be described in the error log line of either etcd or openshift-etcd-operator. Most likely causes tend to be networking issues.\n"},{"id":106,"href":"/runbooks/etcd/etcdinsufficientmembers/","title":"Etcd Insufficient Members","section":"etcd","content":"\retcdInsufficientMembers\r#\rMeaning\r#\rThis alert fires when there are fewer instances available than are needed by etcd to be healthy. This means that etcd cluster has not enough members in the cluster to create quorum.\nImpact\r#\rWhen etcd does not have a majority of instances available the Kubernetes and OpenShift APIs will reject read and write requests and operations that preserve the health of workloads cannot be performed.\nIn general loosing quorum will switch etcd to read only, which effectively renders k8s api read only. It is possible to read the current state, but not possible to update it.\nDiagnosis\r#\rThis can occur when multiple control plane nodes are powered off or are unable to connect to each other via the network. Check that all control plane nodes are powered on and that network connections between each machine are functional.\nCheck any other critical, warning or info alerts firing that can assist with the diagnosis.\nLogin to the cluster. Check health of master nodes if any of them is in NotReady state or not.\n$ kubectl get nodes -l node-role.kubernetes.io/master= General etcd health\r#\rTo run etcdctl commands, we need to exec into the etcdctl container of any etcd pod.\n$ kubectl exec -c etcdctl -n openshift-etcd $(kubectl get po -l app=etcd -oname -n openshift-etcd | awk -F\u0026#34;/\u0026#34; \u0026#39;NR==1{ print $2 }\u0026#39;) Validate that the etcdctl command is available:\n$ etcdctl version Run the following command to get the health of etcd:\n$ etcdctl endpoint health -w table Mitigation\r#\rDisaster and recovery\r#\rIf an upgrade is in progress, the alert may automatically resolve in some time when the master node comes up again. If MCO is not working on the master node, check the cloud provider to verify if the master node instances are running or not.\nIn the case when you are running on AWS, the AWS instance retirement might need a manual reboot of the master node.\nAs a last resort if none of the above fix the issue and the alert is still firing, for etcd specific issues follow the steps described in the disaster-recovery.\n"},{"id":107,"href":"/runbooks/etcd/etcdmembersdown/","title":"Etcd Members Down","section":"etcd","content":"\retcdMembersDown\r#\rMeaning\r#\rThis alert fires when one or more etcd member goes down and evaluates the number of etcd members that are currently down. Often, this alert was observed as part of a cluster upgrade when a master node is being upgraded and requires a reboot.\nImpact\r#\rIn etcd a majority of (n/2)+1 has to agree on membership changes or key-value upgrade proposals. With this approach, a split-brain inconsistency can be avoided. In the case that only one member is down in a 3-member cluster, it still can make forward progress. Due to the fact that the quorum is 2 and 2 members are still alive. However, when more members are down, the cluster becomes unrecoverable.\nDiagnosis\r#\rLogin to the cluster. Check health of master nodes if any of them is in NotReady state or not.\n$ kubectl get nodes -l node-role.kubernetes.io/master= In case there is no upgrade going on, but there is a change in the machineconfig for the master pool causing a rolling reboot of each master node, this alert can be triggered as well. We can check if the machineconfiguration.openshift.io/state : Working annotation is set for any of the master nodes. This is the case when the machine-config-operator (MCO) is working on it.\n$ kubectl get nodes -l node-role.kubernetes.io/master= -o template --template=\u0026#39;{{range .items}}{{\u0026#34;===\u0026gt; node:\u0026gt; \u0026#34;}}{{.metadata.name}}{{\u0026#34;\\n\u0026#34;}}{{range $k, $v := .metadata.annotations}}{{println $k \u0026#34;:\u0026#34; $v}}{{end}}{{\u0026#34;\\n\u0026#34;}}{{end}}\u0026#39; General etcd health\r#\rTo run etcdctl commands, we need to exec into the etcdctl container of any etcd pod.\n$ kubectl exec -c etcdctl -n openshift-etcd $(kubectl get po -l app=etcd -oname -n openshift-etcd | awk -F\u0026#34;/\u0026#34; \u0026#39;NR==1{ print $2 }\u0026#39;) Validate that the etcdctl command is available:\n$ etcdctl version Run the following command to get the health of etcd:\n$ etcdctl endpoint health -w table Mitigation\r#\rIf an upgrade is in progress, the alert may automatically resolve in some time when the master node comes up again. If MCO is not working on the master node, check the cloud provider to verify if the master node instances are running or not.\nIn the case when you are running on AWS, the AWS instance retirement might need a manual reboot of the master node.\n"},{"id":108,"href":"/runbooks/etcd/etcdnoleader/","title":"Etcd No Leader","section":"etcd","content":"\retcdNoLeader\r#\rMeaning\r#\rThis alert is triggered when etcd cluster does not have a leader for more than 1 minute. This can happen if nodes from the cluster are orphaned - they were part of the cluster but now they are in minority and thus can not form a cluster, for example due to network partition.\nImpact\r#\rWhen there is no leader, Kubernetes API will not be able to work as expected and cluster cannot process any writes or reads, and any write requests are queued for processing until a new leader is elected. Operations that preserve the health of the workloads cannot be performed.\nIn general loosing quorum will switch etcd to read only, which effectively renders k8s api read only. It is possible to read the current state, but not possible to update it.\nDiagnosis\r#\rControl plane nodes issue\r#\rThis can occur multiple control plane nodes are powered off or are unable to connect each other via the network. Check that all control plane nodes are powered and that network connections between each machine are functional.\nSlow disk issue\r#\rAnother potential cause could be slow disk, inspect the Disk Sync\rDurationdashboard, as well as the Total Leader Elections Per Day to get more insight and help with diagnosis.\nOther\r#\rCheck the logs of etcd containers to see any further information and to verify that etcd does not have leader. Logs should contain something like etcdserver: no leader.\nMitigation\r#\rDisaster and recovery\r#\rFollow the steps described in the disaster-recovery\n"},{"id":109,"href":"/runbooks/general/targetdown/","title":"Target Down","section":"general","content":"\rTargetDown\r#\rMeaning\r#\rThe alert means that one or more prometheus scrape targets are down. It fires when at least 10% of scrape targets in a Service are unreachable.\nFull context\rPrometheus works by sending an HTTP GET request to all of its \u0026ldquo;targets\u0026rdquo; every few seconds. So TargetDown really means that Prometheus just can\u0026rsquo;t access your service, which may or may not mean it\u0026rsquo;s actually down. If your service appears to be running fine, a common cause could be a misconfigured ServiceMonitor (maybe the port or path is incorrect), a misconfigured NetworkPolicy, or Service with incorrect labelSelectors that isn\u0026rsquo;t selecting any Pods.\nImpact\r#\rMetrics from a particular target cannot be scraped as such there is no data for this target in Prometheus and alerting can be hindered.\nDiagnosis\r#\r/targets page in Prometheus UI can be used to check the scrape error for the particular target.\nup == 0 query can be used to check the trend over time.\nMitigation\r#\rMitigation depends on the error reported by prometheus and there is no generic one.\n"}]