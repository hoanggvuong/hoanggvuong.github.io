<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Introduction on kube-prometheus runbooks</title><link>https://runbooks.prometheus-operator.dev/</link><description>Recent content in Introduction on kube-prometheus runbooks</description><generator>Hugo</generator><language>en-us</language><atom:link href="https://runbooks.prometheus-operator.dev/index.xml" rel="self" type="application/rss+xml"/><item><title>Alertmanager Cluster Crashlooping</title><link>https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerclustercrashlooping/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerclustercrashlooping/</guid><description>&lt;h1 id="alertmanagerclustercrashlooping">
 AlertmanagerClusterCrashlooping
 
 &lt;a class="anchor" href="#alertmanagerclustercrashlooping">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Half or more of the Alertmanager instances within the same cluster are crashlooping.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Alerts could be notified multiple time unless pods are crashing to fast and no alerts can be sent.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl get pod -l app&lt;span style="color:#f92672">=&lt;/span>alertmanager
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAMESPACE NAME READY STATUS RESTARTS AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>default alertmanager-main-0 1/2 CrashLoopBackOff &lt;span style="color:#ae81ff">37107&lt;/span> 2d
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>default alertmanager-main-1 2/2 Running &lt;span style="color:#ae81ff">0&lt;/span> 43d
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>default alertmanager-main-2 2/2 Running &lt;span style="color:#ae81ff">0&lt;/span> 43d 
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Find the root cause by looking to events for a given pod/deployement&lt;/p></description></item><item><title>Alertmanager Cluster Down</title><link>https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerclusterdown/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerclusterdown/</guid><description>&lt;h1 id="alertmanagerclusterdown">
 AlertmanagerClusterDown
 
 &lt;a class="anchor" href="#alertmanagerclusterdown">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Half or more of the Alertmanager instances within the same cluster are down.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>You have an unstable cluster, if everything goes wrong you will lose the whole cluster.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Verify why pods are not running.
You can get a big picture with &lt;code>events&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>$ kubectl get events --field-selector involvedObject.kind&lt;span style="color:#f92672">=&lt;/span>Pod | grep alertmanager
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>There are no cheap options to mitigate this risk.
Verifying any new changes in preprod before production environment should improve stability.&lt;/p></description></item><item><title>Alertmanager Cluster Failed To Send Alerts</title><link>https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerclusterfailedtosendalerts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerclusterfailedtosendalerts/</guid><description>&lt;h1 id="alertmanagerclusterfailedtosendalerts">
 AlertmanagerClusterFailedToSendAlerts
 
 &lt;a class="anchor" href="#alertmanagerclusterfailedtosendalerts">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>All instances failed to send notification to an integration.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>You will not receive a notification when an alert is raised.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>No alerts are received at the integration level from the cluster.&lt;/p>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>Depending on the integration, correct the integration with the faulty instance (network, authorization token, firewall&amp;hellip;)&lt;/p></description></item><item><title>Alertmanager ConfigInconsistent</title><link>https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerconfiginconsistent/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerconfiginconsistent/</guid><description>&lt;h1 id="alertmanagerconfiginconsistent">
 AlertmanagerConfigInconsistent
 
 &lt;a class="anchor" href="#alertmanagerconfiginconsistent">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>The configuration between instances inside a cluster is inconsistent.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Configuration inconsistency can be multiple and impact is hard to predict.
Nevertheless, in most cases the alert might be lost or routed to the incorrect integration.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Run a &lt;code>diff&lt;/code> tool between all &lt;code>alertmanager.yml&lt;/code> that are deployed to find what is wrong.
You could run a job within your CI to avoid this issue in the future.&lt;/p></description></item><item><title>Alertmanager Failed Reload</title><link>https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerfailedreload/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerfailedreload/</guid><description>&lt;h1 id="alertmanagerfailedreload">
 AlertmanagerFailedReload
 
 &lt;a class="anchor" href="#alertmanagerfailedreload">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>The alert &lt;code>AlertmanagerFailedReload&lt;/code> is triggered when the Alertmanager instance
for the cluster monitoring stack has consistently failed to reload its
configuration for a certain period.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>The impact depends on the type of the error you will find in the logs.
Most of the time, previous configuration is still working, thanks to multiple
instances, so avoid deleting existing pods.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Verify if there is an error in &lt;code>config-reloader&lt;/code> container logs.
Here an example with network issues.&lt;/p></description></item><item><title>Alertmanager Failed To Send Alerts</title><link>https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerfailedtosendalerts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerfailedtosendalerts/</guid><description>&lt;h1 id="alertmanagerfailedtosendalerts">
 AlertmanagerFailedToSendAlerts
 
 &lt;a class="anchor" href="#alertmanagerfailedtosendalerts">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>At least one instance is unable to routed alert to the corresponding integration.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>No impact since another instance should be able to send the notification,
unless &lt;code>AlertmanagerClusterFailedToSendAlerts&lt;/code> is also triggerd for the same integration.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Verify the amount of failed notification per alert-manager-[instance] for
a specific integration.&lt;/p>
&lt;p>You can look metrics exposed in prometheus console using promQL.
For exemple the following query will display the number of failed
notifications per instance for pager duty integration.
We have 3 instances involved in the example bellow.&lt;/p></description></item><item><title>Alertmanager Members Inconsistent</title><link>https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagermembersinconsistent/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagermembersinconsistent/</guid><description>&lt;h1 id="alertmanagermembersinconsistent">
 AlertmanagerMembersInconsistent
 
 &lt;a class="anchor" href="#alertmanagermembersinconsistent">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>At least one of alertmanager cluster members cannot be found.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Check if IP addresses discovered by alertmanager cluster are the same ones as in alertmanager Service. Following example show possible inconsistency in Endpoint IP addresses:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>$ kubectl describe svc alertmanager-main
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Name: alertmanager-main
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Namespace: monitoring
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Endpoints: 10.128.2.3:9095,10.129.2.5:9095,10.131.0.44:9095
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ kubectl get pod -o wide | grep alertmanager-main
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>alertmanager-main-0 5/5 Running &lt;span style="color:#ae81ff">0&lt;/span> 11d 10.129.2.6
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>alertmanager-main-1 5/5 Running &lt;span style="color:#ae81ff">0&lt;/span> 2d16h 10.131.0.44 
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>alertmanager-main-2 5/5 Running &lt;span style="color:#ae81ff">0&lt;/span> 6d 10.128.2.3 
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>Deleting an incorrect Endpoint should trigger its recreation with a correct IP address.&lt;/p></description></item><item><title>Config Reloader Sidecar Errors</title><link>https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/configreloadersidecarerrors/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/configreloadersidecarerrors/</guid><description>&lt;h1 id="configreloadersidecarerrors">
 ConfigReloaderSidecarErrors
 
 &lt;a class="anchor" href="#configreloadersidecarerrors">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Errors encountered while the config-reloader sidecar attempts to sync
configuration in a given namespace.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>As a result, configuration for services such as prometheus or alertmanager maybe
stale and cannot be automatically updated.'&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Check config-reloader logs and the configuration which it tries to reload.&lt;/p>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>Usually means new config was rejected by the controlled app because it contains
errors such as unknown configuration sections or bad resource definitions.&lt;/p></description></item><item><title>CPU Throttling High</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/cputhrottlinghigh/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/cputhrottlinghigh/</guid><description>&lt;h1 id="cpu-throttling-high">
 CPU Throttling High
 
 &lt;a class="anchor" href="#cpu-throttling-high">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Processes experience elevated CPU throttling.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>The alert is purely informative and unless there is some other issue with
the application, it can be skipped.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;ul>
&lt;li>Check if application is performing normally&lt;/li>
&lt;li>Check if CPU resource requests are adjusted accordingly to the app usage&lt;/li>
&lt;li>Check kernel version in the node&lt;/li>
&lt;/ul>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>&lt;strong>Notice&lt;/strong>:
User shouldn&amp;rsquo;t increase CPU limits unless the application is behaving
erratically (another alert firing).&lt;/p></description></item><item><title>Info Inhibitor</title><link>https://runbooks.prometheus-operator.dev/runbooks/general/infoinhibitor/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/general/infoinhibitor/</guid><description>&lt;h1 id="infoinhibitor">
 InfoInhibitor
 
 &lt;a class="anchor" href="#infoinhibitor">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>This is an alert that is used to inhibit info alerts.&lt;/p>
&lt;p>By themselves, the info-level alerts are sometimes very noisy,
but they are relevant when combined with other alerts.&lt;/p>
&lt;details>
&lt;summary>Full context&lt;/summary>
&lt;p>More information about the alert and design considerations can be found in a &lt;a href="https://github.com/prometheus-operator/kube-prometheus/issues/861">kube-prometheus issue&lt;/a>&lt;/p>
&lt;/details>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Alert does not have any impact and it is used only as a workaround to a missing feature in alertmanager.&lt;/p></description></item><item><title>Kube Aggregated API Down</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeaggregatedapidown/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeaggregatedapidown/</guid><description>&lt;h1 id="kubeaggregatedapidown">
 KubeAggregatedAPIDown
 
 &lt;a class="anchor" href="#kubeaggregatedapidown">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Kubernetes aggregated API has reported errors.
It has appeared unavailable X times averaged over the past 10m.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>From minor such as inability to see cluster metrics to more severe such as
unable to use custom metrics to scale or even unable to use cluster.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;ul>
&lt;li>Check networking on the node.&lt;/li>
&lt;li>Check firewall on the node.&lt;/li>
&lt;li>Investigate additional API logs.&lt;/li>
&lt;li>Investigate NetworkPolicies if kubeApi - additional API was not filtered out.&lt;/li>
&lt;li>Investigate NetworkPolicies if prometheus/additional api was not filtered out.&lt;/li>
&lt;/ul>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>TODO&lt;/p></description></item><item><title>Kube Aggregated API Errors</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeaggregatedapierrors/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeaggregatedapierrors/</guid><description>&lt;h1 id="kubeaggregatedapierrors">
 KubeAggregatedAPIErrors
 
 &lt;a class="anchor" href="#kubeaggregatedapierrors">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Kubernetes aggregated API has reported errors.
It has appeared unavailable over 4 times averaged over the past 10m.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>From minor such as inability to see cluster metrics to more severe such as
unable to use custom metrics to scale or even unable to use cluster.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;ul>
&lt;li>Check networking on the node.&lt;/li>
&lt;li>Check firewall on the node.&lt;/li>
&lt;li>Investigate additional API logs.&lt;/li>
&lt;li>Investigate NetworkPolicies if kubeApi - additional API was not filtered out.&lt;/li>
&lt;li>Investigate NetworkPolicies if prometheus/additional API was not filtered out.&lt;/li>
&lt;/ul>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>TODO&lt;/p></description></item><item><title>Kube API Down</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapidown/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapidown/</guid><description>&lt;h1 id="kubeapidown">
 KubeAPIDown
 
 &lt;a class="anchor" href="#kubeapidown">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>The &lt;code>KubeAPIDown&lt;/code> alert is triggered when all Kubernetes API servers have not
been reachable by the monitoring system for more than 15 minutes.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>This is a critical alert. The Kubernetes API is not responding. The
cluster may partially or fully non-functional.&lt;/p>
&lt;p>Applications, which do not use kubernetes API directly, will continue to work. Changing kubernetes resources is not possible.
in the cluster.&lt;/p></description></item><item><title>Kube API Error Budget Burn</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapierrorbudgetburn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapierrorbudgetburn/</guid><description>&lt;h1 id="kubeapierrorbudgetburn">
 KubeAPIErrorBudgetBurn
 
 &lt;a class="anchor" href="#kubeapierrorbudgetburn">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>The overall availability of your Kubernetes cluster isn&amp;rsquo;t guaranteed any more.
There may be &lt;strong>too many errors&lt;/strong> returned by the APIServer and/or &lt;strong>responses take too long&lt;/strong> for guarantee proper reconciliation.&lt;/p>
&lt;p>&lt;strong>This is always important; the only deciding factor is how urgent it is at the current rate&lt;/strong>&lt;/p>
&lt;details>
&lt;summary>Full context&lt;/summary>
&lt;p>This alert essentially means that a higher-than-expected percentage of the operations kube-apiserver is performing are erroring. Since random errors are inevitable, kube-apiserver has a &amp;ldquo;budget&amp;rdquo; of errors that it is allowed to make before triggering this alert.&lt;/p></description></item><item><title>Kube API Terminated Requests</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapiterminatedrequests/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapiterminatedrequests/</guid><description>&lt;h1 id="kubeapiterminatedrequests">
 KubeAPITerminatedRequests
 
 &lt;a class="anchor" href="#kubeapiterminatedrequests">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>The apiserver has terminated over 20% of its incoming requests.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Client will not be able to interact with the cluster.
Some in-cluster services this may degrade or make service unavailable.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Use the &lt;code>apiserver_flowcontrol_rejected_requests_total&lt;/code> metric to determine
which flow schema is throttling the traffic to the API Server.
The flow schema also provides information on the affected resources and subjects.&lt;/p></description></item><item><title>Kube Client Certificate Expiration</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeclientcertificateexpiration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeclientcertificateexpiration/</guid><description>&lt;h1 id="kubeclientcertificateexpiration">
 KubeClientCertificateExpiration
 
 &lt;a class="anchor" href="#kubeclientcertificateexpiration">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>A client certificate used to authenticate to the apiserver is expiring in less than 7 days (warning alert) or 24 hours (critical alert).&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Client will not be able to interact with the cluster.
In cluster services communicating with Kubernetes API may degrade or become unavailable.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Check when certificate was issued and when it expires.
Check serviceAccounts and service account tokens.&lt;/p></description></item><item><title>Kube Client Errors</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeclienterrors/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeclienterrors/</guid><description>&lt;h1 id="kubeclienterrors">
 KubeClientErrors
 
 &lt;a class="anchor" href="#kubeclienterrors">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Kubernetes API server client is experiencing over 1% error rate in the last 15 minutes.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Specific kubernetes client may malfunction. Service degradation.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Usual issues:&lt;/p>
&lt;ul>
&lt;li>networking errors&lt;/li>
&lt;li>too low resources to perform given API calls (usually too low CPU/memory requests)&lt;/li>
&lt;li>wrong api client (old libraries)&lt;/li>
&lt;li>investigate if the app does not request more data than it really requires
from kubernetes API, for example it has too wide permissions and scans for
resources in all namespaces.&lt;/li>
&lt;/ul>
&lt;p>Check logs from client side (sometimes app logs).&lt;/p></description></item><item><title>Kube Container Waiting</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecontainerwaiting/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecontainerwaiting/</guid><description>&lt;h1 id="kubecontainerwaiting">
 KubeContainerWaiting
 
 &lt;a class="anchor" href="#kubecontainerwaiting">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Container in pod is in Waiting state for too long.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Service degradation or unavailability.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;ul>
&lt;li>Check pod events via &lt;code>kubectl -n $NAMESPACE describe pod $POD&lt;/code>.&lt;/li>
&lt;li>Check pod logs via &lt;code>kubectl -n $NAMESPACE logs $POD -c $CONTAINER&lt;/code>&lt;/li>
&lt;li>Check for missing files such as configmaps/secrets/volumes&lt;/li>
&lt;li>Check for pod requests, especially special ones such as GPU.&lt;/li>
&lt;li>Check for node taints and capabilities.&lt;/li>
&lt;/ul>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>See &lt;a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/#my-pod-stays-waiting">Container waiting&lt;/a>&lt;/p></description></item><item><title>Kube Controller Manager Down</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecontrollermanagerdown/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecontrollermanagerdown/</guid><description>&lt;h1 id="kubecontrollermanagerdown">
 KubeControllerManagerDown
 
 &lt;a class="anchor" href="#kubecontrollermanagerdown">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>KubeControllerManager has disappeared from Prometheus target discovery.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>The cluster is not functional and Kubernetes resources cannot be reconciled.&lt;/p>
&lt;details>
&lt;summary>Full context&lt;/summary>
&lt;p>More about kube-controller-manager function can be found at &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/">https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/&lt;/a>&lt;/p>
&lt;/details>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>TODO&lt;/p>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>See old CoreOS docs in &lt;a href="http://web.archive.org/web/20201026205154/https://coreos.com/tectonic/docs/latest/troubleshooting/controller-recovery.html">Web Archive&lt;/a>&lt;/p></description></item><item><title>Kube CPU Overcommit</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecpuovercommit/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecpuovercommit/</guid><description>&lt;h1 id="kubecpuovercommit">
 KubeCPUOvercommit
 
 &lt;a class="anchor" href="#kubecpuovercommit">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Cluster has overcommitted CPU resource requests for Pods
and cannot tolerate node failure.&lt;/p>
&lt;details>
&lt;summary>Full context&lt;/summary>
&lt;p>Total number of CPU requests for pods exceeds cluster capacity.
In case of node failure some pods will not fit in the remaining nodes.&lt;/p>
&lt;/details>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>The cluster cannot tolerate node failure. In the event of a node failure, some Pods will be in &lt;code>Pending&lt;/code> state.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;ul>
&lt;li>Check if CPU resource requests are adjusted to the app usage&lt;/li>
&lt;li>Check if some nodes are available and not cordoned&lt;/li>
&lt;li>Check if cluster-autoscaler has issues with adding new nodes&lt;/li>
&lt;/ul>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>Add more nodes to the cluster - usually it is better to have more smaller
nodes, than few bigger.&lt;/p></description></item><item><title>Kube CPU Quota Overcommit</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecpuquotaovercommit/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecpuquotaovercommit/</guid><description>&lt;h1 id="kubecpuquotaovercommit">
 KubeCPUQuotaOvercommit
 
 &lt;a class="anchor" href="#kubecpuquotaovercommit">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Cluster has overcommitted CPU resource requests for Namespaces and cannot tolerate node failure.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>In the event of a node failure, some Pods will be in &lt;code>Pending&lt;/code> state due to a lack of available CPU resources.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;ul>
&lt;li>Check if CPU resource requests are adjusted to the app usage&lt;/li>
&lt;li>Check if some nodes are available and not cordoned&lt;/li>
&lt;li>Check if cluster-autoscaler has issues with adding new nodes&lt;/li>
&lt;li>Check if the given namespace usage grows in time more than expected&lt;/li>
&lt;/ul>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>Review existing quota for given namespace and adjust it accordingly.&lt;/p></description></item><item><title>Kube DaemonSet MisScheduled</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedaemonsetmisscheduled/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedaemonsetmisscheduled/</guid><description>&lt;h1 id="kubedaemonsetmisscheduled">
 KubeDaemonSetMisScheduled
 
 &lt;a class="anchor" href="#kubedaemonsetmisscheduled">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>A number of pods of daemonset are running where they are not supposed to run.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Service degradation or unavailability.
Excessive resource usage where they could be used by other apps.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Usually happens when specifying wrong pod nodeSelector/taints/affinities or
node (node pools) were tainted and existing pods were not scheduled for eviction.&lt;/p>
&lt;ul>
&lt;li>Check daemonset status via &lt;code>kubectl -n $NAMESPACE describe daemonset $NAME&lt;/code>.&lt;/li>
&lt;li>Check &lt;a href="https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/">DaemonSet update strategy&lt;/a>&lt;/li>
&lt;li>Check the status of the pods which belong to the replica sets under the deployment.&lt;/li>
&lt;li>Check pod template parameters such as:
&lt;ul>
&lt;li>pod priority - maybe it was evicted by other more important pods&lt;/li>
&lt;li>affinity rules - maybe due to affinities and not enough nodes it is not
possible to schedule pods&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Check node taints and labels&lt;/li>
&lt;li>Check logs for &lt;a href="https://kubernetes-sigs.github.io/node-feature-discovery/master/get-started/index.html">node-feature-discovery&lt;/a>
and other supporting tools such as gpu-feature-discovery&lt;/li>
&lt;/ul>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>Update DaemonSet and apply change, delete pods manually.&lt;/p></description></item><item><title>Kube DaemonSet Not Scheduled</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedaemonsetnotscheduled/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedaemonsetnotscheduled/</guid><description>&lt;h1 id="kubedaemonsetnotscheduled">
 KubeDaemonSetNotScheduled
 
 &lt;a class="anchor" href="#kubedaemonsetnotscheduled">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>A number of pods of daemonset are not scheduled.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Service degradation or unavailability.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Usually happens when specifying wrong pod taints/affinities or lack of
resources on the nodes.&lt;/p>
&lt;ul>
&lt;li>Check daemonset status via &lt;code>kubectl -n $NAMESPACE describe daemonset $NAME&lt;/code>.&lt;/li>
&lt;li>Check &lt;a href="https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/">DaemonSet update strategy&lt;/a>&lt;/li>
&lt;li>Check the status of the pods which belong to the replica sets under the deployment.&lt;/li>
&lt;li>Check pod template parameters such as:
&lt;ul>
&lt;li>pod priority - maybe it was evicted by other more important pods&lt;/li>
&lt;li>resources - maybe it tries to use unavailable resource, such as GPU but
there is limited number of nodes with GPU&lt;/li>
&lt;li>affinity rules - maybe due to affinities and not enough nodes it is not
possible to schedule pods&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Check if Horizontal Pod Autoscaler (HPA) is not triggered due to untested
values (requests values).&lt;/li>
&lt;li>Check if cluster-autoscaler is able to create new nodes - see its logs or
cluster-autoscaler status configmap.&lt;/li>
&lt;/ul>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>Set proper priority class for important dameonsets to system-node-critical.&lt;/p></description></item><item><title>Kube DaemonSet Rollout Stuck</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedaemonsetrolloutstuck/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedaemonsetrolloutstuck/</guid><description>&lt;h1 id="kubedaemonsetrolloutstuck">
 KubeDaemonSetRolloutStuck
 
 &lt;a class="anchor" href="#kubedaemonsetrolloutstuck">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>DaemonSet update is stuck waiting for replaced pod.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Service degradation or unavailability.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;ul>
&lt;li>Check daemonset status via &lt;code>kubectl -n $NAMESPACE describe daemonset $NAME&lt;/code>.&lt;/li>
&lt;li>Check &lt;a href="https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/">DaemonSet update strategy&lt;/a>&lt;/li>
&lt;li>Check the status of the pods which belong to the replica sets under the deployment.&lt;/li>
&lt;li>Check pod template parameters such as:
&lt;ul>
&lt;li>pod priority - maybe it was evicted by other more important pods&lt;/li>
&lt;li>resources - maybe it tries to use unavailable resource, such as GPU but
there is limited number of nodes with GPU&lt;/li>
&lt;li>affinity rules - maybe due to affinities and not enough nodes it is not
possible to schedule pods&lt;/li>
&lt;li>pod termination grace period - if too long then pods may be for too long
in terminating state&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Check if Horizontal Pod Autoscaler (HPA) is not triggered due to untested
values (requests values).&lt;/li>
&lt;li>Check if cluster-autoscaler is able to create new nodes - see its logs or
cluster-autoscaler status configmap.&lt;/li>
&lt;/ul>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>See &lt;a href="https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/#daemonset-rolling-update-is-stuck">DaemonSet rolling update is stuck&lt;/a>&lt;/p></description></item><item><title>Kube Deployment Generation Mismatch</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedeploymentgenerationmismatch/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedeploymentgenerationmismatch/</guid><description>&lt;h1 id="kubedeploymentgenerationmismatch">
 KubeDeploymentGenerationMismatch
 
 &lt;a class="anchor" href="#kubedeploymentgenerationmismatch">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Deployment generation mismatch due to possible roll-back.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Service degradation or unavailability.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>See &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#failed-deployment">Kubernetes Docs - Failed Deployment&lt;/a>&lt;/p>
&lt;ul>
&lt;li>Check out rollout history &lt;code>kubectl -n $NAMESPACE rollout history deployment $NAME&lt;/code>&lt;/li>
&lt;li>Check rollout status if it is not paused&lt;/li>
&lt;li>Check deployment status via &lt;code>kubectl -n $NAMESPACE describe deployment $NAME&lt;/code>.&lt;/li>
&lt;li>Check how many replicas are there declared.&lt;/li>
&lt;li>Investigate if new pods are not crashing.&lt;/li>
&lt;li>Check the status of the pods which belong to the replica sets under the deployment.&lt;/li>
&lt;li>Check pod template parameters such as:
&lt;ul>
&lt;li>pod priority - maybe it was evicted by other more important pods&lt;/li>
&lt;li>resources - maybe it tries to use unavailable resource, such as GPU
but there is limited number of nodes with GPU&lt;/li>
&lt;li>affinity rules - maybe due to affinities and not enough nodes it is
not possible to schedule pods&lt;/li>
&lt;li>pod termination grace period - if too long then pods may be for too long
in terminating state&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Check if Horizontal Pod Autoscaler (HPA) is not triggered due to untested
values (requests values).&lt;/li>
&lt;li>Check if cluster-autoscaler is able to create new nodes - see its logs or
cluster-autoscaler status configmap.&lt;/li>
&lt;/ul>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>Depending on the conditions usually adding new nodes solves the issue.&lt;/p></description></item><item><title>Kube Deployment Replicas Mismatch</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedeploymentreplicasmismatch/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedeploymentreplicasmismatch/</guid><description>&lt;h1 id="kubedeploymentreplicasmismatch">
 KubeDeploymentReplicasMismatch
 
 &lt;a class="anchor" href="#kubedeploymentreplicasmismatch">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Deployment has not matched the expected number of replicas.&lt;/p>
&lt;details>
&lt;summary>Full context&lt;/summary>
&lt;p>Kubernetes Deployment resource does not have number of replicas which were
declared to be in operation.
For example deployment is expected to have 3 replicas, but it has less than
that for a noticeable period of time.&lt;/p>
&lt;p>In rare occasions there may be more replicas than it should and system did
not clean it up.&lt;/p></description></item><item><title>Kube HPA Replicas Mismatch</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubehpareplicasmismatch/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubehpareplicasmismatch/</guid><description>&lt;h1 id="kubehpareplicasmismatch">
 KubeHpaReplicasMismatch
 
 &lt;a class="anchor" href="#kubehpareplicasmismatch">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Horizontal Pod Autoscaler has not matched the desired number of replicas for
longer than 15 minutes.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>HPA was unable to schedule desired number of pods.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Check why HPA was unable to scale:&lt;/p>
&lt;ul>
&lt;li>not enough nodes in the cluster&lt;/li>
&lt;li>hitting resource quotas in the cluster&lt;/li>
&lt;li>pods evicted due to pod priority&lt;/li>
&lt;/ul>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>In case of cluster-autoscaler you may need to set up preemtive pod pools to
ensure nodes are created on time.&lt;/p></description></item><item><title>Kube HPA Maxed Out</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubehpamaxedout/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubehpamaxedout/</guid><description>&lt;h1 id="kubehpamaxedout">
 KubeHpaMaxedOut
 
 &lt;a class="anchor" href="#kubehpamaxedout">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Horizontal Pod Autoscaler has been running at max replicas for longer
than 15 minutes.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Horizontal Pod Autoscaler won&amp;rsquo;t be able to add new pods and thus scale application.
&lt;strong>Notice&lt;/strong> for some services maximizing HPA is in fact desired.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Check why HPA was unable to scale:&lt;/p>
&lt;ul>
&lt;li>max replicas too low&lt;/li>
&lt;li>too low value for requests such as CPU?&lt;/li>
&lt;/ul>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>If using basic metrics like CPU/Memory then ensure to set proper values for
&lt;code>requests&lt;/code>.
For memory based scaling ensure there are no memory leaks.
If using custom metrics then fine-tune how app scales accordingly to it.&lt;/p></description></item><item><title>Kube Job Completion</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubejobcompletion/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubejobcompletion/</guid><description>&lt;h1 id="kubejobcompletion">
 KubeJobCompletion
 
 &lt;a class="anchor" href="#kubejobcompletion">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Job is taking more than 1h to complete.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;ul>
&lt;li>Long processing of batch jobs.&lt;/li>
&lt;li>Possible issues with scheduling next Job&lt;/li>
&lt;/ul>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;ul>
&lt;li>Check job via &lt;code>kubectl -n $NAMESPACE describe jobs $JOB&lt;/code>.&lt;/li>
&lt;li>Check pod events via &lt;code>kubectl -n $NAMESPACE describe job $JOB&lt;/code>.&lt;/li>
&lt;/ul>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;ul>
&lt;li>Give it more resources so it finishes faster, if applicable.&lt;/li>
&lt;li>See &lt;a href="https://kubernetes.io/docs/tasks/job/">Job patterns&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Kube Job Failed</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubejobfailed/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubejobfailed/</guid><description>&lt;h1 id="kubejobfailed">
 KubeJobFailed
 
 &lt;a class="anchor" href="#kubejobfailed">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Job failed complete.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Failure of processing of a scheduled task.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;ul>
&lt;li>Check job via &lt;code>kubectl -n $NAMESPACE describe jobs $JOB&lt;/code>.&lt;/li>
&lt;li>Check pod events via &lt;code>kubectl -n $NAMESPACE describe pod $POD_FROM_JOB&lt;/code>.&lt;/li>
&lt;li>Check pod logs via &lt;code>kubectl -n $NAMESPACE log pod $POD_FROM_JOB&lt;/code>.&lt;/li>
&lt;/ul>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;ul>
&lt;li>See &lt;a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/#debugging-pods">Debugging Pods&lt;/a>&lt;/li>
&lt;li>See &lt;a href="https://kubernetes.io/docs/tasks/job/">Job patterns&lt;/a>&lt;/li>
&lt;li>redesign job so that it is idempotent (can be re-run many times which will
always produce the same output even if input differs)&lt;/li>
&lt;/ul></description></item><item><title>Kube Memory Overcommit</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubememoryovercommit/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubememoryovercommit/</guid><description>&lt;h1 id="kubememoryovercommit">
 KubeMemoryOvercommit
 
 &lt;a class="anchor" href="#kubememoryovercommit">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Cluster has overcommitted Memory resource requests for Pods
and cannot tolerate node failure.&lt;/p>
&lt;details>
&lt;summary>Full context&lt;/summary>
&lt;p>Total number of Memory requests for pods exceeds cluster capacity.
In case of node failure some pods will not fit in the remaining nodes.&lt;/p>
&lt;/details>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>The cluster cannot tolerate node failure. In the event of a node failure,
some Pods will be in &lt;code>Pending&lt;/code> state.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;ul>
&lt;li>Check if Memory resource requests are adjusted to the app usage&lt;/li>
&lt;li>Check if some nodes are available and not cordoned&lt;/li>
&lt;li>Check if cluster-autoscaler has issues with adding new nodes&lt;/li>
&lt;/ul>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>Add more nodes to the cluster - usually it is better to have more smaller
nodes, than few bigger.&lt;/p></description></item><item><title>Kube Memory Quota Overcommit</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubememoryquotaovercommit/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubememoryquotaovercommit/</guid><description>&lt;h1 id="kubememoryquotaovercommit">
 KubeMemoryQuotaOvercommit
 
 &lt;a class="anchor" href="#kubememoryquotaovercommit">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Cluster has overcommitted memory resource requests for Namespaces.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Various services degradation or unavailability in case of single node failure.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;ul>
&lt;li>Check if Memory resource requests are adjusted to the app usage&lt;/li>
&lt;li>Check if some nodes are available and not cordoned&lt;/li>
&lt;li>Check if cluster-autoscaler has issues with adding new nodes&lt;/li>
&lt;li>Check if the given namespace usage grows in time more than expected&lt;/li>
&lt;/ul>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>Review existing quota for given namespace and adjust it accordingly.&lt;/p></description></item><item><title>Kube Node Not Ready</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubenodenotready/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubenodenotready/</guid><description>&lt;h1 id="kubenodenotready">
 KubeNodeNotReady
 
 &lt;a class="anchor" href="#kubenodenotready">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>KubeNodeNotReady alert is fired when a Kubernetes node is not in &lt;code>Ready&lt;/code>
state for a certain period. In this case, the node is not able to host any new
pods as described [here][KubeNode].&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>The performance of the cluster deployments is affected, depending on the overall
workload and the type of the node.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>The notification details should list the node that&amp;rsquo;s not ready. For Example:&lt;/p></description></item><item><title>Kube Node Readiness Flapping</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubenodereadinessflapping/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubenodereadinessflapping/</guid><description>&lt;h1 id="kubenodereadinessflapping">
 KubeNodeReadinessFlapping
 
 &lt;a class="anchor" href="#kubenodereadinessflapping">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>The readiness status of node has changed few times in the last 15 minutes.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>The performance of the cluster deployments is affected, depending on the overall
workload and the type of the node.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>The notification details should list the node that&amp;rsquo;s not reachable. For Example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-txt" data-lang="txt">&lt;span style="display:flex;">&lt;span> - alertname = KubeNodeUnreachable
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - node = node1.example.com
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Login to the cluster. Check the status of that node:&lt;/p></description></item><item><title>Kube Node Unreachable</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubenodeunreachable/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubenodeunreachable/</guid><description>&lt;h1 id="kubenodeunreachable">
 KubeNodeUnreachable
 
 &lt;a class="anchor" href="#kubenodeunreachable">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Kubernetes node is unreachable and some workloads may be rescheduled.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>The performance of the cluster deployments is affected, depending on the overall
workload and the type of the node.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>The notification details should list the node that&amp;rsquo;s not reachable. For Example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-txt" data-lang="txt">&lt;span style="display:flex;">&lt;span> - alertname = KubeNodeUnreachable
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - node = node1.example.com
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Login to the cluster. Check the status of that node:&lt;/p></description></item><item><title>Kube Persistent Volume Errors</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumeerrors/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumeerrors/</guid><description>&lt;h1 id="kubepersistentvolumeerrors">
 KubePersistentVolumeErrors
 
 &lt;a class="anchor" href="#kubepersistentvolumeerrors">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>PersistentVolume is having issues with provisioning.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Volue may be unavailable or have data erors (corrupted storage).&lt;/p>
&lt;p>Service degradation, data loss.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;ul>
&lt;li>Check PV events via &lt;code>kubectl describe pv $PV&lt;/code>.&lt;/li>
&lt;li>Check storage provider for logs.&lt;/li>
&lt;li>Check storage quotas in the cloud.&lt;/li>
&lt;/ul>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>In happy scenario storage is just not provisioned as fast as expected.
In worst scenario there is data corruption or data loss. Restore from backup.&lt;/p></description></item><item><title>Kube Persistent Volume Filling Up</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumefillingup/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumefillingup/</guid><description>&lt;h1 id="kubepersistentvolumefillingup">
 KubePersistentVolumeFillingUp
 
 &lt;a class="anchor" href="#kubepersistentvolumefillingup">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>There can be various reasons why a volume is filling up.
This runbook does not cover application specific reasons, only mitigations
for volumes that are legitimately filling.&lt;/p>
&lt;p>As always refer to recommended scenarios for given service.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Service degradation, switching to read only mode.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Check app usage in time.
Check if there are any configurations such as snapshotting, automatic data retention.&lt;/p></description></item><item><title>Kube Pod Crash Looping</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepodcrashlooping/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepodcrashlooping/</guid><description>&lt;h1 id="kubepodcrashlooping">
 KubePodCrashLooping
 
 &lt;a class="anchor" href="#kubepodcrashlooping">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Pod is in CrashLoop which means the app dies or is unresponsive and
kubernetes tries to restart it automatically.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Service degradation or unavailability.
Inability to do rolling upgrades.
Certain apps will not perform required tasks such as data migrations.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;ul>
&lt;li>Check template via &lt;code>kubectl -n $NAMESPACE get pod $POD&lt;/code>.&lt;/li>
&lt;li>Check pod events via &lt;code>kubectl -n $NAMESPACE describe pod $POD&lt;/code>.&lt;/li>
&lt;li>Check pod logs via &lt;code>kubectl -n $NAMESPACE logs $POD -c $CONTAINER&lt;/code>&lt;/li>
&lt;li>Check pod template parameters such as:
&lt;ul>
&lt;li>pod priority&lt;/li>
&lt;li>resources - maybe it tries to use unavailable resource, such as GPU but
there is limited number of nodes with GPU&lt;/li>
&lt;li>readiness and liveness probes may be incorrect - wrong port or command,
check is failing too fast due to short timeout for response&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>Other things to check:&lt;/p></description></item><item><title>Kube Pod Not Ready</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepodnotready/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepodnotready/</guid><description>&lt;h1 id="kubepodnotready">
 KubePodNotReady
 
 &lt;a class="anchor" href="#kubepodnotready">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Pod has been in a non-ready state for more than 15 minutes.&lt;/p>
&lt;p>State Running but not ready means readiness probe fails.
State Pending means pod can not be created for specific namespace and node.&lt;/p>
&lt;details>
&lt;summary>Full context&lt;/summary>
&lt;p>Pod failed to reach ready state, depending on the readiness/liveness probes.
See &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/">pod-lifecycle&lt;/a>&lt;/p>
&lt;/details>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Service degradation or unavailability.
Pod not attached to service, thus not getting any traffic.&lt;/p></description></item><item><title>Kube Quota Almost Full</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubequotaalmostfull/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubequotaalmostfull/</guid><description>&lt;h1 id="kubequotaalmostfull">
 KubeQuotaAlmostFull
 
 &lt;a class="anchor" href="#kubequotaalmostfull">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Cluster reaches to the allowed limits for given namespace.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>In the future deployments may not be possbile.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;ul>
&lt;li>Check resource usage for the namespace in given time span&lt;/li>
&lt;/ul>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;ul>
&lt;li>Review existing quota for given namespace and adjust it accordingly.&lt;/li>
&lt;li>Review resources used by the quota and fine tune them.&lt;/li>
&lt;li>Continue with standard capacity planning procedures.&lt;/li>
&lt;li>See &lt;a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/">Quotas&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Kube Quota Exceeded</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubequotaexceeded/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubequotaexceeded/</guid><description>&lt;h1 id="kubequotaexceeded">
 KubeQuotaExceeded
 
 &lt;a class="anchor" href="#kubequotaexceeded">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Cluster reaches to the allowed hard limits for given namespace.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Inability to create resources in kubernetes.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;ul>
&lt;li>Check resource usage for the namespace in given time span&lt;/li>
&lt;/ul>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;ul>
&lt;li>Review existing quota for given namespace and adjust it accordingly.&lt;/li>
&lt;li>Review resources used by the quota and fine tune them.&lt;/li>
&lt;li>Continue with standard capacity planning procedures.&lt;/li>
&lt;li>See &lt;a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/">Quotas&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Kube Quota Fully Used</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubequotafullyused/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubequotafullyused/</guid><description>&lt;h1 id="kubequotafullyused">
 KubeQuotaFullyUsed
 
 &lt;a class="anchor" href="#kubequotafullyused">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Cluster reached allowed limits for given namespace.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>New app installations may not be possible.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;ul>
&lt;li>Check resource usage for the namespace in given time span&lt;/li>
&lt;/ul>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;ul>
&lt;li>Review existing quota for given namespace and adjust it accordingly.&lt;/li>
&lt;li>Review resources used by the quota and fine tune them.&lt;/li>
&lt;li>Continue with standard capacity planning procedures.&lt;/li>
&lt;li>See &lt;a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/">Quotas&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Kube Scheduler Down</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeschedulerdown/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeschedulerdown/</guid><description>&lt;h1 id="kubeschedulerdown">
 KubeSchedulerDown
 
 &lt;a class="anchor" href="#kubeschedulerdown">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Kube Scheduler has disappeared from Prometheus target discovery.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>This is a critical alert. The cluster may partially or fully non-functional.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>To be added.&lt;/p>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>See old CoreOS docs in &lt;a href="http://web.archive.org/web/20201026205154/https://coreos.com/tectonic/docs/latest/troubleshooting/controller-recovery.html">Web Archive&lt;/a>&lt;/p></description></item><item><title>Kube State Metric sWatch Errors</title><link>https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricswatcherrors/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricswatcherrors/</guid><description>&lt;h1 id="kubestatemetricswatcherrors">
 KubeStateMetricsWatchErrors
 
 &lt;a class="anchor" href="#kubestatemetricswatcherrors">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>kube-state-metrics is experiencing errors in watch operations.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Unable to get metrics for certain resources&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Check kube-state-metric container logs.
Check service account token.
Check networking rules and network policies.&lt;/p>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>TODO&lt;/p></description></item><item><title>Kube State Metrics List Errors</title><link>https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricslisterrors/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricslisterrors/</guid><description>&lt;h1 id="kubestatemetricslisterrors">
 KubeStateMetricsListErrors
 
 &lt;a class="anchor" href="#kubestatemetricslisterrors">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>kube-state-metrics is experiencing errors in list operations.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Unable to get metrics for certain resources&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Check kube-state-metric container logs.
Check service account token.
Check networking rules and network policies.&lt;/p>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>TODO&lt;/p></description></item><item><title>Kube State Metrics Sharding Mismatch</title><link>https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricsshardingmismatch/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricsshardingmismatch/</guid><description>&lt;h1 id="kubestatemetricsshardingmismatch">
 KubeStateMetricsShardingMismatch
 
 &lt;a class="anchor" href="#kubestatemetricsshardingmismatch">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>kube-state-metrics sharding is misconfigured.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Unable to get metrics for certain resources.
Some metrics can be unavailable.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Check kube-state-metric container logs for each shard.
Check service account token.
Check networking rules and network policies.&lt;/p>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>TODO&lt;/p></description></item><item><title>Kube State Metrics Shards Missing</title><link>https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricsshardsmissing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricsshardsmissing/</guid><description>&lt;h1 id="kubestatemetricsshardsmissing">
 KubeStateMetricsShardsMissing
 
 &lt;a class="anchor" href="#kubestatemetricsshardsmissing">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>kube-state-metrics shards are missing.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Unable to get metrics for certain resources.
Some metrics can be unavailable.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Check kube-state-metric container logs for each shard.
Check if certain pods were forcefully evicted.
Check service account token.
Check networking rules and network policies.&lt;/p>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>TODO&lt;/p></description></item><item><title>Kube StatefulSet Generation Mismatch</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubestatefulsetgenerationmismatch/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubestatefulsetgenerationmismatch/</guid><description>&lt;h1 id="kubestatefulsetgenerationmismatch">
 KubeStatefulSetGenerationMismatch
 
 &lt;a class="anchor" href="#kubestatefulsetgenerationmismatch">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>StatefulSet generation mismatch due to possible roll-back.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Service degradation or unavailability.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>See &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#failed-deployment">Kubernetes Docs - Failed Deployment&lt;/a>
which can be also applied to StatefulSets to some extent&lt;/p>
&lt;ul>
&lt;li>Check out rollout history &lt;code>kubectl -n $NAMESPACE rollout history statefulset $NAME&lt;/code>&lt;/li>
&lt;li>Check rollout status if it is not paused&lt;/li>
&lt;li>Check deployment status via &lt;code>kubectl -n $NAMESPACE describe statefulset $NAME&lt;/code>.&lt;/li>
&lt;li>Check how many replicas are there declared.&lt;/li>
&lt;li>Investigate if new pods are not crashing.&lt;/li>
&lt;li>Look at the issues with PersistentVolumes attached to StatefulSets.&lt;/li>
&lt;li>Check the status of the pods which belong to the replica sets under the deployment.&lt;/li>
&lt;li>Check pod template parameters such as:
&lt;ul>
&lt;li>pod priority - maybe it was evicted by other more important pods&lt;/li>
&lt;li>resources - maybe it tries to use unavailable resource, such as GPU
but there is limited number of nodes with GPU&lt;/li>
&lt;li>affinity rules - maybe due to affinities and not enough nodes it is
not possible to schedule pods&lt;/li>
&lt;li>pod termination grace period - if too long then pods may be for too long
in terminating state&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Check if Horizontal Pod Autoscaler (HPA) is not triggered due to untested
values (requests values).&lt;/li>
&lt;li>Check if cluster-autoscaler is able to create new nodes - see its logs or
cluster-autoscaler status configmap.&lt;/li>
&lt;/ul>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>Statefulsets are quite specific, and usually have special scripts on pod termination.
See if there are special commands executed such as data migration, which may significantly slow down the progress.&lt;/p></description></item><item><title>Kube StatefulSet Replicas Mismatch</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubestatefulsetreplicasmismatch/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubestatefulsetreplicasmismatch/</guid><description>&lt;h1 id="kubestatefulsetreplicasmismatch">
 KubeStatefulSetReplicasMismatch
 
 &lt;a class="anchor" href="#kubestatefulsetreplicasmismatch">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>StatefulSet has not matched the expected number of replicas.&lt;/p>
&lt;details>
&lt;summary>Full context&lt;/summary>
&lt;p>Kubernetes StatefulSet resource does not have number of replicas which were
declared to be in operation.
For example statefulset is expected to have 3 replicas, but it has less than
that for a noticeable period of time.&lt;/p>
&lt;p>In rare occasions there may be more replicas than it should and system did not
clean it up.&lt;/p></description></item><item><title>Kube StatefulSet Update Not RolledOut</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubestatefulsetupdatenotrolledout/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubestatefulsetupdatenotrolledout/</guid><description>&lt;h1 id="kubestatefulsetupdatenotrolledout">
 KubeStatefulSetUpdateNotRolledOut
 
 &lt;a class="anchor" href="#kubestatefulsetupdatenotrolledout">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>StatefulSet update has not been rolled out.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Service degradation or unavailability.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;ul>
&lt;li>Check statefulset via &lt;code>kubectl -n $NAMESPACE describe statefulset $NAME&lt;/code>.&lt;/li>
&lt;li>Check if statefuls update was not paused manually (see status)&lt;/li>
&lt;li>Check how many replicas are there declared.&lt;/li>
&lt;li>Check the status of the pods which belong to the replica sets under the
statefulset.&lt;/li>
&lt;li>Check pod template parameters such as:
&lt;ul>
&lt;li>pod priority - maybe it was evicted by other more importand pods&lt;/li>
&lt;li>resources - maybe it tries to use unavailabe resource, such as GPU but
there is limited number of nodes with GPU&lt;/li>
&lt;li>affinity rules - maybe due to affinities and not enough nodes it is
not possible to schedule pods&lt;/li>
&lt;li>pod termination grace period - if too long then pods may be for too long
in terminating state&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Check if there are issues with attaching disks to statefulset - for example
disk was in Zone A, but pod is scheduled in Zone B.&lt;/li>
&lt;li>Check if Horizontal Pod Autoscaler (HPA) is not triggered due to untested
values (requests values).&lt;/li>
&lt;li>Check if cluster-autoscaler is able to create new nodes - see its logs or
cluster-autoscaler status configmap.&lt;/li>
&lt;/ul>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>TODO&lt;/p></description></item><item><title>Kube Version Mismatch</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeversionmismatch/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeversionmismatch/</guid><description>&lt;h1 id="kubeversionmismatch">
 KubeVersionMismatch
 
 &lt;a class="anchor" href="#kubeversionmismatch">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Different semantic versions of Kubernetes components running.
Usually happens during kubernetes cluster upgrade process.&lt;/p>
&lt;details>
&lt;summary>Full context&lt;/summary>
&lt;p>Kubernetes control plane nodes or worker nodes use different versions.
This usually happens when kubernetes cluster is upgraded between minor and
major version.&lt;/p>
&lt;/details>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Incompatible API versions between kubernetes components may have very
broad range of issues, influencing single containers, through app stability,
ending at whole cluster stability.&lt;/p></description></item><item><title>Kubelet Client Certificate Expiration</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletclientcertificateexpiration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletclientcertificateexpiration/</guid><description>&lt;h1 id="kubeletclientcertificateexpiration">
 KubeletClientCertificateExpiration
 
 &lt;a class="anchor" href="#kubeletclientcertificateexpiration">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Client certificate for Kubelet on node expires soon or already expired.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Node will not be able to be used within the cluster.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Check when certificate was issued and when it expires.&lt;/p>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>Update certificates in the cluster control nodes and the worker nodes.
Refer to the documentation of the tool used to create cluster.&lt;/p>
&lt;p>Another option is to delete node if it affects only one,&lt;/p></description></item><item><title>Kubelet Client Certificate Renewal Errors</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletclientcertificaterenewalerrors/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletclientcertificaterenewalerrors/</guid><description>&lt;h1 id="kubeletclientcertificaterenewalerrors">
 KubeletClientCertificateRenewalErrors
 
 &lt;a class="anchor" href="#kubeletclientcertificaterenewalerrors">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Kubelet on node has failed to renew its client certificate
(XX errors in the last 15 minutes)&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Node will not be able to be used within the cluster.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Check when certificate was issued and when it expires.&lt;/p>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>Update certificates in the cluster control nodes and the worker nodes.
Refer to the documentation of the tool used to create cluster.&lt;/p></description></item><item><title>Kubelet Down</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletdown/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletdown/</guid><description>&lt;h1 id="kubeletdown">
 KubeletDown
 
 &lt;a class="anchor" href="#kubeletdown">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>This alert is triggered when the monitoring system has not been able to reach
any of the cluster&amp;rsquo;s Kubelets for more than 15 minutes.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>This alert represents a critical threat to the cluster&amp;rsquo;s stability. Excluding
the possibility of a network issue preventing the monitoring system from
scraping Kubelet metrics, multiple nodes in the cluster are likely unable to
respond to configuration changes for pods and other resources, and some
debugging tools are likely not functional, e.g. &lt;code>kubectl exec&lt;/code> and &lt;code>kubectl logs&lt;/code>.&lt;/p></description></item><item><title>Kubelet Pod Lifecycle Event Generator Duration High</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletplegdurationhigh/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletplegdurationhigh/</guid><description>&lt;h1 id="kubeletplegdurationhigh">
 KubeletPlegDurationHigh
 
 &lt;a class="anchor" href="#kubeletplegdurationhigh">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>The Kubelet Pod Lifecycle Event Generator has a 99th percentile duration of
XX seconds on node.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>TODO&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>TODO&lt;/p>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>TODO&lt;/p></description></item><item><title>Kubelet Pod Start Up Latency High</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletpodstartuplatencyhigh/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletpodstartuplatencyhigh/</guid><description>&lt;h1 id="kubeletpodstartuplatencyhigh">
 KubeletPodStartUpLatencyHigh
 
 &lt;a class="anchor" href="#kubeletpodstartuplatencyhigh">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Kubelet Pod startup 99th percentile latency is XX seconds on node.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Slow pod starts.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Usually exhaused IOPS for node storage.&lt;/p>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/">Cordon and drain node&lt;/a> and delete it.
If issue persists look into the node logs.&lt;/p></description></item><item><title>Kubelet Server Certificate Expiration</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletservercertificateexpiration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletservercertificateexpiration/</guid><description>&lt;h1 id="kubeletservercertificateexpiration">
 KubeletServerCertificateExpiration
 
 &lt;a class="anchor" href="#kubeletservercertificateexpiration">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Server certificate for Kubelet on node expires soon or already expired.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>&lt;strong>Critical&lt;/strong> - Cluster will be in inoperable state.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Check when certificate was issued and when it expires.&lt;/p>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>Update certificates in the cluster control nodes and the worker nodes.
Refer to the documentation of the tool used to create cluster.&lt;/p>
&lt;p>Another option is to delete node if it affects only one,&lt;/p></description></item><item><title>Kubelet Server Certificate Renewal Errors</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletservercertificaterenewalerrors/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletservercertificaterenewalerrors/</guid><description>&lt;h1 id="kubeletservercertificaterenewalerrors">
 KubeletServerCertificateRenewalErrors
 
 &lt;a class="anchor" href="#kubeletservercertificaterenewalerrors">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Kubelet on node has failed to renew its server certificate
(XX errors in the last 5 minutes)&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>&lt;strong>Critical&lt;/strong> - Cluster will be in inoperable state.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Check when certificate was issued and when it expires.&lt;/p>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>Update certificates in the cluster control nodes and the worker nodes.
Refer to the documentation of the tool used to create cluster.&lt;/p></description></item><item><title>Kubelet Too Many Pods</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubelettoomanypods/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubelettoomanypods/</guid><description>&lt;h1 id="kubelettoomanypods">
 KubeletTooManyPods
 
 &lt;a class="anchor" href="#kubelettoomanypods">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>The alert fires when a specific node is running &amp;gt;95% of its capacity of pods
(110 by default).&lt;/p>
&lt;details>
&lt;summary>Full context&lt;/summary>
&lt;p>Kubelets have a configuration that limits how many Pods they can run.
The default value of this is 110 Pods per Kubelet, but it is configurable
(and this alert takes that configuration into account with the
&lt;code>kube_node_status_capacity_pods&lt;/code> metric).&lt;/p>
&lt;/details>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Running many pods (more than 110) on a single node places a strain on the
Container Runtime Interface (CRI), Container Network Interface (CNI),
and the operating system itself. Approaching that limit may affect performance
and availability of that node.&lt;/p></description></item><item><title>KubeProxy Down</title><link>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeproxydown/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeproxydown/</guid><description>&lt;h1 id="kubeproxydown">
 KubeProxyDown
 
 &lt;a class="anchor" href="#kubeproxydown">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>The &lt;code>KubeProxyDown&lt;/code> alert is triggered when all Kubernetes Proxy instances have not
been reachable by the monitoring system for more than 15 minutes.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>kube-proxy is a network proxy that runs on each node in your cluster,
implementing part of the Kubernetes Service concept.&lt;/p>
&lt;p>kube-proxy maintains network rules on nodes.
These network rules allow network communication to your Pods
from network sessions inside or outside of your cluster.&lt;/p></description></item><item><title>Node Clock Not Synchronising</title><link>https://runbooks.prometheus-operator.dev/runbooks/node/nodeclocknotsynchronising/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/node/nodeclocknotsynchronising/</guid><description>&lt;h1 id="nodeclocknotsynchronising">
 NodeClockNotSynchronising
 
 &lt;a class="anchor" href="#nodeclocknotsynchronising">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Clock not synchronising.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Time is not automatically synchronizing on the node. This can cause issues with handling TLS as well as problems with other time-sensitive applications.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>TODO&lt;/p>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>See &lt;a href="https://runbooks.prometheus-operator.dev/runbooks/node/nodeclockskewdetected/">Node Clok Skew Detected&lt;/a> for mitigation steps.&lt;/p></description></item><item><title>Node Clock Skew Detected</title><link>https://runbooks.prometheus-operator.dev/runbooks/node/nodeclockskewdetected/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/node/nodeclockskewdetected/</guid><description>&lt;h1 id="nodeclockskewdetected">
 NodeClockSkewDetected
 
 &lt;a class="anchor" href="#nodeclockskewdetected">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Clock skew detected.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Time is skewed on the node. This can cause issues with handling TLS as well as problems with other time-sensitive applications.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>TODO&lt;/p>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>Ensure time synchronization service is running.
Set proper time servers.
Esure to sync time on server start, especially when using
low power mode or hibernation.&lt;/p>
&lt;p>Some resource consuming process can cause issues on given hardware,
so move it to different servers.&lt;/p></description></item><item><title>Node File Descriptor Limit</title><link>https://runbooks.prometheus-operator.dev/runbooks/node/nodefiledescriptorlimit/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/node/nodefiledescriptorlimit/</guid><description>&lt;h1 id="nodefiledescriptorlimit">
 NodeFileDescriptorLimit
 
 &lt;a class="anchor" href="#nodefiledescriptorlimit">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>This alert is triggered when a node&amp;rsquo;s kernel is found to be running out of
available file descriptors &amp;ndash; a &lt;code>warning&lt;/code> level alert at greater than 70% usage
and a &lt;code>critical&lt;/code> level alert at greater than 90% usage.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Applications on the node may no longer be able to open and operate on
files. This is likely to have severe consequences for anything scheduled on this
node.&lt;/p></description></item><item><title>Node Filesystem Almost Out Of Files</title><link>https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutoffiles/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutoffiles/</guid><description>&lt;h1 id="nodefilesystemalmostoutoffiles">
 NodeFilesystemAlmostOutOfFiles
 
 &lt;a class="anchor" href="#nodefilesystemalmostoutoffiles">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>This alert is similar to the NodeFilesystemSpaceFillingUp alert, but rather
than being based on a prediction that a filesystem will run out of inodes in a
certain amount of time, it uses simple static thresholds. The alert will fire as
at a &lt;code>warning&lt;/code> level at 5% of available inodes left, and at a &lt;code>critical&lt;/code> level
with 3% of available inodes left.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>A node&amp;rsquo;s filesystem becoming full can have a far reaching impact, as it may
cause any or all of the applications scheduled to that node to experience
anything from performance degradation to full inoperability. Depending on the
node and filesystem involved, this could pose a critical threat to the stability
of the cluster.&lt;/p></description></item><item><title>Node Filesystem Almost Out Of Space</title><link>https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutofspace/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutofspace/</guid><description>&lt;h1 id="nodefilesystemalmostoutofspace">
 NodeFilesystemAlmostOutOfSpace
 
 &lt;a class="anchor" href="#nodefilesystemalmostoutofspace">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>This alert is similar to the NodeFilesystemSpaceFillingUp alert, but rather
than being based on a prediction that a filesystem will become full in a certain
amount of time, it uses simple static thresholds. The alert will fire as at a
&lt;code>warning&lt;/code> level at 5% space left, and at a &lt;code>critical&lt;/code> level with 3% space left.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>A node&amp;rsquo;s filesystem becoming full can have a far reaching impact, as it may
cause any or all of the applications scheduled to that node to experience
anything from performance degradation to full inoperability. Depending on the
node and filesystem involved, this could pose a critical threat to the stability
of the cluster.&lt;/p></description></item><item><title>Node Filesystem Files Filling Up</title><link>https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemfilesfillingup/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemfilesfillingup/</guid><description>&lt;h1 id="nodefilesystemfilesfillingup">
 NodeFilesystemFilesFillingUp
 
 &lt;a class="anchor" href="#nodefilesystemfilesfillingup">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>This alert is similar to the NodeFilesystemSpaceFillingUp alert, but
predicts the filesystem will run out of inodes rather than bytes of storage
space. The alert fires at a &lt;code>critical&lt;/code> level when the filesystem is predicted to
run out of available inodes within four hours.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>A node&amp;rsquo;s filesystem becoming full can have a far reaching impact, as it may
cause any or all of the applications scheduled to that node to experience
anything from performance degradation to full inoperability. Depending on the
node and filesystem involved, this could pose a critical threat to the stability
of the cluster.&lt;/p></description></item><item><title>Node Filesystem Space Filling Up</title><link>https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemspacefillingup/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemspacefillingup/</guid><description>&lt;h1 id="nodefilesystemspacefillingup">
 NodeFilesystemSpaceFillingUp
 
 &lt;a class="anchor" href="#nodefilesystemspacefillingup">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>This alert is based on an extrapolation of the space used in a file system. It
fires if both the current usage is above a certain threshold &lt;em>and&lt;/em> the
extrapolation predicts to run out of space in a certain time. This is a
warning-level alert if that time is less than 24h. It&amp;rsquo;s a critical alert if that
time is less than 4h.&lt;/p>
&lt;details>
&lt;summary>Full context&lt;/summary>
&lt;p>The filesystem on Kubernetes nodes mainly consists of the operating system,
[container ephemeral storage][1], container images, and container logs.
Since Kubelet automatically handles [cleaning up old logs][2] and
[deleting unused images][3], container ephemeral storage is a common cause of
this alert. Although this alert may be triggered before Kubelet&amp;rsquo;s garbage
collection kicks in.&lt;/p></description></item><item><title>Node High Number Conntrack Entries Used</title><link>https://runbooks.prometheus-operator.dev/runbooks/node/nodehighnumberconntrackentriesused/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/node/nodehighnumberconntrackentriesused/</guid><description>&lt;h1 id="nodehighnumberconntrackentriesused">
 NodeHighNumberConntrackEntriesUsed
 
 &lt;a class="anchor" href="#nodehighnumberconntrackentriesused">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Number of conntrack are getting close to the limit.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>When reached the limit then some connections will be dropped, degrading service quality.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Check current conntrack value on the node.
Check which apps are generating a lot of connections.&lt;/p>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>Migrate some pods to another nodes.
Bump conntrack limit directly on the node, remembering to make it persistent across node reboots.&lt;/p></description></item><item><title>Node Network Interface Flapping</title><link>https://runbooks.prometheus-operator.dev/runbooks/general/nodenetworkinterfaceflapping/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/general/nodenetworkinterfaceflapping/</guid><description>&lt;h1 id="nodenetworkinterfaceflapping">
 NodeNetworkInterfaceFlapping
 
 &lt;a class="anchor" href="#nodenetworkinterfaceflapping">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Network interface is often changing its status&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Applications on the node may no longer be able to operate with other services.
Network attached storage performance issues or even data loss.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Investigate networking issues on the node and to connected hardware.
Check physical cables, check networking firewall rules and so on.&lt;/p>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>Cordon and drain node to migrate apps from it.&lt;/p></description></item><item><title>Node Network Receive Errors</title><link>https://runbooks.prometheus-operator.dev/runbooks/node/nodenetworkreceiveerrs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/node/nodenetworkreceiveerrs/</guid><description>&lt;h1 id="nodenetworkreceiveerrs">
 NodeNetworkReceiveErrs
 
 &lt;a class="anchor" href="#nodenetworkreceiveerrs">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Network interface is reporting many receive errors.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Applications on the node may no longer be able to operate with other services.
Network attached storage performance issues or even data loss.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Investigate networking issues on the node and to connected hardware.
Check physical cables, check networking firewall rules and so on.&lt;/p>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>In general mitigation landscape is quite vast, some suggestions:&lt;/p></description></item><item><title>Node Network Transmit Errors</title><link>https://runbooks.prometheus-operator.dev/runbooks/node/nodenetworktransmiterrs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/node/nodenetworktransmiterrs/</guid><description>&lt;h1 id="nodenetworktransmiterrs">
 NodeNetworkTransmitErrs
 
 &lt;a class="anchor" href="#nodenetworktransmiterrs">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Network interface is reporting many transmit errors.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Applications on the node may no longer be able to operate with other services.
Network attached storage performance issues or even data loss.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Investigate networking issues on the node and to connected hardware.
Check network interface saturation.
Check CPU usage saturation.
Check physical cables, check networking firewall rules and so on.&lt;/p></description></item><item><title>Node RAID Degraded</title><link>https://runbooks.prometheus-operator.dev/runbooks/node/noderaiddegraded/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/node/noderaiddegraded/</guid><description>&lt;h1 id="noderaiddegraded">
 NodeRAIDDegraded
 
 &lt;a class="anchor" href="#noderaiddegraded">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>RAID Array is degraded.&lt;/p>
&lt;p>This alert is triggered when a node has a storage configuration with RAID array,
and the array is reporting as being in a degraded state due to one or more disk
failures.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>The affected node could go offline at any moment if the RAID array fully fails
due to further issues with disks.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>You can open a shell on the node and use the standard Linux utilities to
diagnose the issue, but you may need to install additional software in the debug
container:&lt;/p></description></item><item><title>Node RAID Disk Failure</title><link>https://runbooks.prometheus-operator.dev/runbooks/node/noderaiddiskfailure/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/node/noderaiddiskfailure/</guid><description>&lt;h1 id="noderaiddiskfailure">
 NodeRAIDDiskFailure
 
 &lt;a class="anchor" href="#noderaiddiskfailure">#&lt;/a>
 
&lt;/h1>
&lt;p>See &lt;a href="https://runbooks.prometheus-operator.dev/runbooks/node/noderaiddegraded/">Node RAID Degraded&lt;/a>&lt;/p></description></item><item><title>Node Text File Collector Scrape Error</title><link>https://runbooks.prometheus-operator.dev/runbooks/node/nodetextfilecollectorscrapeerror/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/node/nodetextfilecollectorscrapeerror/</guid><description>&lt;h1 id="nodetextfilecollectorscrapeerror">
 NodeTextFileCollectorScrapeError
 
 &lt;a class="anchor" href="#nodetextfilecollectorscrapeerror">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Node Exporter text file collector failed to scrape.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Missing metrics from additional scripts.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;ul>
&lt;li>Check node_exporter logs&lt;/li>
&lt;li>Check script supervisor (like systemd or cron) for more information about failed script execution&lt;/li>
&lt;/ul>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>Check if provided configuration is valid, if files were not renamed during upgrades.&lt;/p></description></item><item><title>Prometheus Bad Config</title><link>https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusbadconfig/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusbadconfig/</guid><description>&lt;h1 id="prometheusbadconfig">
 PrometheusBadConfig
 
 &lt;a class="anchor" href="#prometheusbadconfig">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Alert fires when Prometheus cannot successfully reload the configuration file
due to the file having incorrect content.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Configuration cannot be reloaded and prometheus operates with last known good
configuration.
Configuration changes in any of Prometheus, Probe, PodMonitor,
or ServiceMonitor objects may not be picked up by prometheus server.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Check prometheus container logs for an explanation of which part of the
configuration is problematic.&lt;/p></description></item><item><title>Prometheus Duplicate Timestamps</title><link>https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusduplicatetimestamps/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusduplicatetimestamps/</guid><description>&lt;h1 id="prometheusduplicatetimestamps">
 PrometheusDuplicateTimestamps
 
 &lt;a class="anchor" href="#prometheusduplicatetimestamps">#&lt;/a>
 
&lt;/h1>
&lt;p>Find the Prometheus Pod that concerns this.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>$ kubectl -n &amp;lt;namespace&amp;gt; get pod
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>prometheus-k8s-0 2/2 Running &lt;span style="color:#ae81ff">1&lt;/span> 122m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>prometheus-k8s-1 2/2 Running &lt;span style="color:#ae81ff">1&lt;/span> 122m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Look at the logs of each of them, there should be a log line such as:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>$ kubectl -n &amp;lt;namespace&amp;gt; logs prometheus-k8s-0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>level&lt;span style="color:#f92672">=&lt;/span>warn ts&lt;span style="color:#f92672">=&lt;/span>2021-01-04T15:08:55.613Z caller&lt;span style="color:#f92672">=&lt;/span>scrape.go:1372 component&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;scrape manager&amp;#34;&lt;/span> scrape_pool&lt;span style="color:#f92672">=&lt;/span>default/main-ingress-nginx-controller/0 target&lt;span style="color:#f92672">=&lt;/span>http://10.0.7.3:10254/metrics msg&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Error on ingesting samples with different value but same timestamp&amp;#34;&lt;/span> num_dropped&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#ae81ff">16&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now there is a judgement call to make, this could be the result of:&lt;/p></description></item><item><title>Prometheus Error Sending Alerts To Any Alertmanager</title><link>https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheuserrorsendingalertstoanyalertmanager/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheuserrorsendingalertstoanyalertmanager/</guid><description>&lt;h1 id="prometheuserrorsendingalertstoanyalertmanager">
 PrometheusErrorSendingAlertsToAnyAlertmanager
 
 &lt;a class="anchor" href="#prometheuserrorsendingalertstoanyalertmanager">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Prometheus has encountered errors sending alerts to a any Alertmanager.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>All alerts may be lost.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Check connectivity issues between Prometheus and AlertManager cluster.
Check NetworkPolicies, network saturation.
Check if AlertManager is not overloaded or has not enough resources.&lt;/p>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>Set multiple AlertManager instances, spread them across nodes.&lt;/p></description></item><item><title>Prometheus Error Sending Alerts To Some Alertmanagers</title><link>https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheuserrorsendingalertstosomealertmanagers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheuserrorsendingalertstosomealertmanagers/</guid><description>&lt;h1 id="prometheuserrorsendingalertstosomealertmanagers">
 PrometheusErrorSendingAlertsToSomeAlertmanagers
 
 &lt;a class="anchor" href="#prometheuserrorsendingalertstosomealertmanagers">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Prometheus has encountered more than 1% errors sending alerts to a specific Alertmanager.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Some alerts may be lost.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Check connectivity issues between Prometheus and AlertManager.
Check NetworkPolicies, network saturation.
Check if AlertManager is not overloaded or has not enough resources.&lt;/p>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>Set multiple AlertManager instances, spread them across nodes.&lt;/p></description></item><item><title>Prometheus Federation - Detailed Implementation Guide</title><link>https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusfederation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusfederation/</guid><description>&lt;h1 id="prometheus-federation---detailed-implementation-guide">
 Prometheus Federation - Detailed Implementation Guide
 
 &lt;a class="anchor" href="#prometheus-federation---detailed-implementation-guide">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="1-introduction">
 1. Introduction
 
 &lt;a class="anchor" href="#1-introduction">#&lt;/a>
 
&lt;/h2>
&lt;p>&lt;strong>Prometheus Federation&lt;/strong> is a technique that allows one Prometheus server (&lt;strong>global/federated Prometheus&lt;/strong>) to scrape metrics from one or more other Prometheus instances (&lt;strong>child Prometheus&lt;/strong>) via their &lt;code>/federate&lt;/code> endpoint.&lt;/p>
&lt;h3 id="key-goals">
 Key Goals
 
 &lt;a class="anchor" href="#key-goals">#&lt;/a>
 
&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Centralize metrics&lt;/strong> from multiple sources (multi-cluster, multi-datacenter).&lt;/li>
&lt;li>&lt;strong>Aggregate precomputed metrics&lt;/strong> (recording rules) to reduce query load.&lt;/li>
&lt;li>Maintain &lt;strong>local alerting&lt;/strong> at each cluster to avoid dependency on network connectivity to the central server.&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="2-architecture-overview">
 2. Architecture Overview
 
 &lt;a class="anchor" href="#2-architecture-overview">#&lt;/a>
 
&lt;/h2>
&lt;figure>&lt;img src="https://runbooks.prometheus-operator.dev/promfederation.png" width="500" height="800">&lt;figcaption>
 &lt;h4>Architecture Overview&lt;/h4>
 &lt;/figcaption>
&lt;/figure>

&lt;hr>
&lt;h2 id="3-federation-concepts">
 3. Federation Concepts
 
 &lt;a class="anchor" href="#3-federation-concepts">#&lt;/a>
 
&lt;/h2>
&lt;h3 id="31-child-prometheus">
 3.1. Child Prometheus
 
 &lt;a class="anchor" href="#31-child-prometheus">#&lt;/a>
 
&lt;/h3>
&lt;ul>
&lt;li>Scrapes metrics locally from exporters, Kubernetes services, etc.&lt;/li>
&lt;li>Can run &lt;strong>alerting rules&lt;/strong> locally to ensure cluster-level alerts are not dependent on the central instance.&lt;/li>
&lt;li>Publishes selected metrics through the &lt;code>/federate&lt;/code> endpoint.&lt;/li>
&lt;/ul>
&lt;h3 id="32-globalfederated-prometheus">
 3.2. Global/Federated Prometheus
 
 &lt;a class="anchor" href="#32-globalfederated-prometheus">#&lt;/a>
 
&lt;/h3>
&lt;ul>
&lt;li>Does &lt;strong>not&lt;/strong> scrape individual exporters directly.&lt;/li>
&lt;li>Pulls &lt;strong>aggregated or selected metrics&lt;/strong> from child Prometheus instances.&lt;/li>
&lt;li>Ideal for &lt;strong>global dashboards&lt;/strong> and &lt;strong>cross-cluster SLO/SLA monitoring&lt;/strong>.&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="4-federation-use-cases">
 4. Federation Use Cases
 
 &lt;a class="anchor" href="#4-federation-use-cases">#&lt;/a>
 
&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Multi-Cluster Kubernetes Monitoring&lt;/strong>&lt;/p></description></item><item><title>Prometheus Label LimitHit</title><link>https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheuslabellimithit/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheuslabellimithit/</guid><description>&lt;h1 id="prometheuslabellimithit">
 PrometheusLabelLimitHit
 
 &lt;a class="anchor" href="#prometheuslabellimithit">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Prometheus has dropped targets because some scrape configs have exceeded the labels limit.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Metrics and alerts may be missing or inaccurate.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>Start thinking about sharding prometheus.
Increase scrape times to perform it less frequently.&lt;/p></description></item><item><title>Prometheus Missing Rule Evaluations</title><link>https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusmissingruleevaluations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusmissingruleevaluations/</guid><description>&lt;h1 id="prometheusmissingruleevaluations">
 PrometheusMissingRuleEvaluations
 
 &lt;a class="anchor" href="#prometheusmissingruleevaluations">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Prometheus is missing rule evaluations due to slow rule group evaluation.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Metrics and alerts may be missing or inaccurate.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Check which rules fail, try to calcuate them differently.&lt;/p>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>Sometimes giving more CPU is the only way to fix it.&lt;/p></description></item><item><title>Prometheus Not Connected To Alertmanagers</title><link>https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusnotconnectedtoalertmanagers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusnotconnectedtoalertmanagers/</guid><description>&lt;h1 id="prometheusnotconnectedtoalertmanagers">
 PrometheusNotConnectedToAlertmanagers
 
 &lt;a class="anchor" href="#prometheusnotconnectedtoalertmanagers">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Prometheus is not connected to any Alertmanagers.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Sending alerts is not possible.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Check connectivity issues between Prometheus and AlertManager.
Check NetworkPolicies, network saturation.
Check if AlertManager is not overloaded or has not enough resources.&lt;/p>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>Set multiple AlertManager instances, spread them across nodes.&lt;/p></description></item><item><title>Prometheus Not Ingesting Samples</title><link>https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusnotingestingsamples/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusnotingestingsamples/</guid><description>&lt;h1 id="prometheusnotingestingsamples">
 PrometheusNotIngestingSamples
 
 &lt;a class="anchor" href="#prometheusnotingestingsamples">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Prometheus is not ingesting samples.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Missing metrics.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>TODO&lt;/p>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>TODO&lt;/p></description></item><item><title>Prometheus Notification Queue Running Full</title><link>https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusnotificationqueuerunningfull/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusnotificationqueuerunningfull/</guid><description>&lt;h1 id="prometheusnotificationqueuerunningfull">
 PrometheusNotificationQueueRunningFull
 
 &lt;a class="anchor" href="#prometheusnotificationqueuerunningfull">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Prometheus alert notification queue predicted to run full in less than 30m.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Fail to send alerts.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Check prometheus container logs for an explanation of which part of the
configuration is problematic.&lt;/p>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>Remove conflicting configuration option.&lt;/p>
&lt;p>Check if there is an option to decrease number of alerts firing,
for example by sharding prometheus.&lt;/p></description></item><item><title>Prometheus Operator List Errors</title><link>https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorlisterrors/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorlisterrors/</guid><description>&lt;h1 id="prometheusoperatorlisterrors">
 PrometheusOperatorListErrors
 
 &lt;a class="anchor" href="#prometheusoperatorlisterrors">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Errors while performing list operations in controller.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Prometheus Operator has troubles in managing its operands and Custom Resources.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;ul>
&lt;li>Check logs of Prometheus Operator pod.&lt;/li>
&lt;li>Check service account tokens.&lt;/li>
&lt;li>Check Prometheus Operator RBAC configuration.&lt;/li>
&lt;/ul>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2></description></item><item><title>Prometheus Operator Node Lookup Errors</title><link>https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatornodelookuperrors/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatornodelookuperrors/</guid><description>&lt;h1 id="prometheusoperatornodelookuperrors">
 PrometheusOperatorNodeLookupErrors
 
 &lt;a class="anchor" href="#prometheusoperatornodelookuperrors">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Errors while reconciling information about kubernetes nodes.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Prometheus Operator is not able to configure Prometheus scrape configuration.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;ul>
&lt;li>Check logs of Prometheus Operator pod.&lt;/li>
&lt;li>Check kubelet Service managed by Prometheus Operator&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>$ kubelet describe Service -n kube-system -l app.kubernetes.io/managed-by&lt;span style="color:#f92672">=&lt;/span>prometheus-operator
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e">## Mitigation&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>TODO
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Prometheus Operator NotReady</title><link>https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatornotready/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatornotready/</guid><description>&lt;h1 id="prometheusoperatornotready">
 PrometheusOperatorNotReady
 
 &lt;a class="anchor" href="#prometheusoperatornotready">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Prometheus operator is not ready.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Prometheus Operator is not able to perform any operation.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;ul>
&lt;li>Check Prometheus Operator Deployment configuration.&lt;/li>
&lt;li>Check logs of Prometheus Operator pod.&lt;/li>
&lt;li>Check service account tokens.&lt;/li>
&lt;/ul>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>TODO&lt;/p></description></item><item><title>Prometheus Operator Reconcile Errors</title><link>https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorreconcileerrors/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorreconcileerrors/</guid><description>&lt;h1 id="prometheusoperatorreconcileerrors">
 PrometheusOperatorReconcileErrors
 
 &lt;a class="anchor" href="#prometheusoperatorreconcileerrors">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Errors while reconciling controller.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Prometheus Operator will not be able to manage Prometheuses/Alertmanagers.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Check logs of Prometheus Operator pod.
Check service account tokens.&lt;/p>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2></description></item><item><title>Prometheus Operator Rejected Resources</title><link>https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorrejectedresources/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorrejectedresources/</guid><description>&lt;h1 id="prometheusoperatorrejectedresources">
 PrometheusOperatorRejectedResources
 
 &lt;a class="anchor" href="#prometheusoperatorrejectedresources">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Custom Resources managed by Prometheus Operator were rejected and not propagated to operands (prometheus, alertmanager).&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Custom Resource won&amp;rsquo;t be used by prometheus-operator and thus configuration it carries won&amp;rsquo;t be translated to prometheus or alertmanager configuration.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;ul>
&lt;li>Check newly created Custom Resources like Prometheus, Alertmanager, Rules, Probes, ServiceMonitors, and others that have a CRD used by Prometheus Operator.&lt;/li>
&lt;li>Check logs of Prometheus Operator pod.&lt;/li>
&lt;/ul>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>Fix newly created Custom Resource to conform to the schema defined in a CRD and reapply it to the cluster.&lt;/p></description></item><item><title>Prometheus Operator Sync Failed</title><link>https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorsyncfailed/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorsyncfailed/</guid><description>&lt;h1 id="prometheusoperatorsyncfailed">
 PrometheusOperatorSyncFailed
 
 &lt;a class="anchor" href="#prometheusoperatorsyncfailed">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Last controller reconciliation failed&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Prometheus Operator will not be able to manage Prometheuses/Alertmanagers.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Check logs of Prometheus Operator pod.
Check service account tokens.&lt;/p>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2></description></item><item><title>Prometheus Operator Watch Errors</title><link>https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorwatcherrors/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorwatcherrors/</guid><description>&lt;h1 id="prometheusoperatorwatcherrors">
 PrometheusOperatorWatchErrors
 
 &lt;a class="anchor" href="#prometheusoperatorwatcherrors">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Errors while performing watch operations in controller.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Prometheus Operator will not be able to manage Prometheuses/Alertmanagers.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Check logs of Prometheus Operator pod.
Check service account tokens.&lt;/p>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2></description></item><item><title>Prometheus Out Of Order Timestamps</title><link>https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusoutofordertimestamps/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusoutofordertimestamps/</guid><description>&lt;h1 id="prometheusoutofordertimestamps">
 PrometheusOutOfOrderTimestamps
 
 &lt;a class="anchor" href="#prometheusoutofordertimestamps">#&lt;/a>
 
&lt;/h1>
&lt;p>More information in &lt;a href="https://www.robustperception.io/debugging-out-of-order-samples">blog&lt;/a>&lt;/p></description></item><item><title>Prometheus Remote Storage Failures</title><link>https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusremotestoragefailures/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusremotestoragefailures/</guid><description>&lt;h1 id="prometheusremotestoragefailures">
 PrometheusRemoteStorageFailures
 
 &lt;a class="anchor" href="#prometheusremotestoragefailures">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Prometheus fails to send samples to remote storage.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Metrics and alerts may be missing or inaccurate.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Check prometheus logs and remote storage logs.
Investigate network issues.
Check configs and credentials.&lt;/p>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>TODO&lt;/p></description></item><item><title>Prometheus Remote Write Behind</title><link>https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusremotewritebehind/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusremotewritebehind/</guid><description>&lt;h1 id="prometheusremotestoragefailures">
 PrometheusRemoteStorageFailures
 
 &lt;a class="anchor" href="#prometheusremotestoragefailures">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Prometheus remote write is behind.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Metrics and alerts may be missing or inaccurate.
Increased data lag between locations.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Check prometheus logs and remote storage logs.
Investigate network issues.
Check configs and credentials.&lt;/p>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>Probbaly amout of data sent to remote system is too high
for given network connectivity speed.
You may need to limit which metrics to send to minimize transfers.&lt;/p></description></item><item><title>Prometheus Rule Failures</title><link>https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusrulefailures/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusrulefailures/</guid><description>&lt;h1 id="prometheusrulefailures">
 PrometheusRuleFailures
 
 &lt;a class="anchor" href="#prometheusrulefailures">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Prometheus is failing rule evaluations.
Prometheus rules are incorrect or failed to calculate.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Metrics and alerts may be missing or inaccurate.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Your best starting point is the rules page of the Prometheus UI (:9090/rules).
It will show the error.&lt;/p>
&lt;p>You can also evaluate the rule expression yourself, using the UI, or maybe
using PromLens to help debug expression issues.&lt;/p></description></item><item><title>Prometheus Target Limit Hit</title><link>https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustargetlimithit/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustargetlimithit/</guid><description>&lt;h1 id="prometheustargetlimithit">
 PrometheusTargetLimitHit
 
 &lt;a class="anchor" href="#prometheustargetlimithit">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Prometheus has dropped targets because some scrape configs have exceeded the targets limit.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Metrics and alerts may be missing or inaccurate.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>Start thinking about sharding prometheus.
Increase scrape times to perform it less frequently.&lt;/p></description></item><item><title>Prometheus Target Sync Failure</title><link>https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustargetsyncfailure/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustargetsyncfailure/</guid><description>&lt;h1 id="prometheustargetsyncfailure">
 PrometheusTargetSyncFailure
 
 &lt;a class="anchor" href="#prometheustargetsyncfailure">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>This alert is triggered when at least one of the Prometheus instances has
consistently failed to sync its configuration.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Metrics and alerts may be missing or inaccurate.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Determine whether the alert is for the cluster or user workload Prometheus by
inspecting the alert&amp;rsquo;s &lt;code>namespace&lt;/code> label.&lt;/p>
&lt;p>Check the logs for the appropriate Prometheus instance:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>$ NAMESPACE&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#39;&amp;lt;value of namespace label from alert&amp;gt;&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ oc -n $NAMESPACE logs -l &lt;span style="color:#e6db74">&amp;#39;app=prometheus&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>level&lt;span style="color:#f92672">=&lt;/span>error ... msg&lt;span style="color:#f92672">=&lt;/span>&lt;span style="color:#e6db74">&amp;#34;Creating target failed&amp;#34;&lt;/span> ...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>If the logs indicate a syntax or other configuration error, correct the
corresponding &lt;code>ServiceMonitor&lt;/code>, &lt;code>PodMonitor&lt;/code>, or other configuration
resource. In most all cases, the operator should prevent this from happening.&lt;/p></description></item><item><title>Prometheus TSDB Compactions Failing</title><link>https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustsdbcompactionsfailing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustsdbcompactionsfailing/</guid><description>&lt;h1 id="prometheustsdbcompactionsfailing">
 PrometheusTSDBCompactionsFailing
 
 &lt;a class="anchor" href="#prometheustsdbcompactionsfailing">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Prometheus has issues compacting blocks.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Metrics and alerts may be missing or inaccurate.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Check storage used by the pod.
This can happen if there is a lot of going on in the cluster and
prometheus did not manage to compact data.&lt;/p>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>At first just wait, it may fix itself after some time.&lt;/p>
&lt;p>Increase Prometheus pod memory so that it caches more from disk.
Try expanding volumes if they are too small or too slow.
Change PVC storageClass to a more performant one.&lt;/p></description></item><item><title>Prometheus TSDB Reloads Failing</title><link>https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustsdbreloadsfailing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustsdbreloadsfailing/</guid><description>&lt;h1 id="prometheustsdbreloadsfailing">
 PrometheusTSDBReloadsFailing
 
 &lt;a class="anchor" href="#prometheustsdbreloadsfailing">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Prometheus has issues reloading blocks from disk.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Metrics and alerts may be missing or inaccurate.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Check storage used by the pod.&lt;/p>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>Increase Prometheus pod memory so that it caches more from disk.
Try expanding volumes if they are too small or too slow.
Change PVC storageClass to a more performant one.&lt;/p></description></item><item><title>PrometheusRemoteWriteDesiredShards</title><link>https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusremotewritedesiredshards/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusremotewritedesiredshards/</guid><description>&lt;h1 id="prometheusremotewritedesiredshards">
 PrometheusRemoteWriteDesiredShards
 
 &lt;a class="anchor" href="#prometheusremotewritedesiredshards">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Prometheus remote write desired shards calculation wants to run
more than configured max shards.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Metrics and alerts may be missing or inaccurate.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Check metrics cardinality.&lt;/p>
&lt;p>Check prometheus logs and remote storage logs.
Investigate network issues.
Check configs and credentials.&lt;/p>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>Probbaly amout of data sent to remote system is too high
for given network connectivity speed.
You may need to limit which metrics to send to minimize transfers.&lt;/p></description></item><item><title>Watchdog</title><link>https://runbooks.prometheus-operator.dev/runbooks/general/watchdog/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/general/watchdog/</guid><description>&lt;h1 id="watchdog">
 Watchdog
 
 &lt;a class="anchor" href="#watchdog">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>This is an alert meant to ensure that the entire alerting pipeline is functional.
This alert is always firing, therefore it should always be firing in Alertmanager
and always fire against a receiver.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>If not firing then it should alert external systems that this alerting system
is no longer working.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Misconfigured alertmanager, bad credentials, bad endpoint, firewalls..
Check alertmanager logs.&lt;/p></description></item><item><title/><link>https://runbooks.prometheus-operator.dev/docs/runbooks/devops/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/docs/runbooks/devops/</guid><description>&lt;h1 id="devops-runbooks">
 DevOps Runbooks
 
 &lt;a class="anchor" href="#devops-runbooks">#&lt;/a>
 
&lt;/h1>
&lt;p>Practical guides and automation for CI/CD, monitoring, and cloud operations.&lt;/p></description></item><item><title/><link>https://runbooks.prometheus-operator.dev/docs/runbooks/gitops/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/docs/runbooks/gitops/</guid><description>&lt;h1 id="gitops-runbooks">
 GitOps Runbooks
 
 &lt;a class="anchor" href="#gitops-runbooks">#&lt;/a>
 
&lt;/h1>
&lt;p>Workflows and best practices for infrastructure as code and declarative deployments.&lt;/p></description></item><item><title/><link>https://runbooks.prometheus-operator.dev/docs/runbooks/systemadmin/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/docs/runbooks/systemadmin/</guid><description>&lt;h1 id="system-admin-runbooks">
 System Admin Runbooks
 
 &lt;a class="anchor" href="#system-admin-runbooks">#&lt;/a>
 
&lt;/h1>
&lt;p>Procedures for server management, backups, user administration, and troubleshooting.&lt;/p></description></item><item><title/><link>https://runbooks.prometheus-operator.dev/runbooks/devops/ansible_playbook_handbook_vn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/devops/ansible_playbook_handbook_vn/</guid><description>&lt;h1 id="ansible-playbook-handbook-vn">
 Ansible Playbook Handbook (VN)
 
 &lt;a class="anchor" href="#ansible-playbook-handbook-vn">#&lt;/a>
 
&lt;/h1>

&lt;blockquote class='book-hint '>
 &lt;p>S tay thc chin t AZ: cu trc project, inventory, roles, tags, limit, vars, vault, rolling update, testing, Windows hosts&lt;/p>
&lt;/blockquote>&lt;hr>
&lt;h2 id="0-chun-b-mi-trng-windows--wsl2--xut">
 0) Chun b mi trng (Windows  WSL2  xut)
 
 &lt;a class="anchor" href="#0-chu%e1%ba%a9n-b%e1%bb%8b-m%c3%b4i-tr%c6%b0%e1%bb%9dng-windows--wsl2-%c4%91%e1%bb%81-xu%e1%ba%a5t">#&lt;/a>
 
&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Ci WSL2 + Ubuntu 22.04&lt;/strong>:
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-powershell" data-lang="powershell">&lt;span style="display:flex;">&lt;span>wsl --install -d Ubuntu-&lt;span style="color:#ae81ff">22.04&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>&lt;strong>Ci Ansible qua pipx (bn mi)&lt;/strong>:
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>sudo apt update &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> sudo apt install -y python3-pip python3-venv pipx git ssh
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pipx ensurepath
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>pipx install --include-deps ansible
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ansible --version
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>(Nu qun Windows hosts) ci thm winrm libs:
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>pipx inject ansible pywinrm pywinrm&lt;span style="color:#f92672">[&lt;/span>credssp&lt;span style="color:#f92672">]&lt;/span> kerberos requests-kerberos
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="1-t-chc-th-mc-project-khuyn-ngh">
 1) T chc th mc project (khuyn ngh)
 
 &lt;a class="anchor" href="#1-t%e1%bb%95-ch%e1%bb%a9c-th%c6%b0-m%e1%bb%a5c-project-khuy%e1%ba%bfn-ngh%e1%bb%8b">#&lt;/a>
 
&lt;/h2>
&lt;pre tabindex="0">&lt;code>ansible/
 inventories/
  dev/
   hosts.ini
   group_vars/
    all.yml
    web.yml
    db.yml
   host_vars/
   web01.yml
   db01.yml
  uat/ ...
  prod/ ...
 roles/
  common/
   tasks/main.yml
   handlers/main.yml
   templates/
   files/
   vars/main.yml
   defaults/main.yml
  nginx/ ...
  app/ ...
 group_vars/ (tu chn dng chung mi mi trng)
 host_vars/ (tu chn)
 playbooks/
  site.yml
  play_nginx.yml
  play_app.yml
 requirements.yml (galaxy)
 ansible.cfg
 README.md
&lt;/code>&lt;/pre>&lt;p>&lt;strong>ansible.cfg (mu):&lt;/strong>&lt;/p></description></item><item><title/><link>https://runbooks.prometheus-operator.dev/runbooks/devops/ci-cd-failure-resolution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/devops/ci-cd-failure-resolution/</guid><description>&lt;h3 id="objective">
 Objective
 
 &lt;a class="anchor" href="#objective">#&lt;/a>
 
&lt;/h3>
&lt;p>To provide a systematic approach for diagnosing, resolving, and documenting failures within the Continuous Integration/Continuous Delivery (CI/CD) pipeline. This runbook ensures that pipeline blockages are resolved quickly and efficiently, minimizing disruption to development workflows.&lt;/p>
&lt;h3 id="prerequisites">
 Prerequisites
 
 &lt;a class="anchor" href="#prerequisites">#&lt;/a>
 
&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Access:&lt;/strong> Read/write access to the CI/CD platform (e.g., GitLab CI, GitHub Actions, Jenkins).&lt;/li>
&lt;li>&lt;strong>Permissions:&lt;/strong> Ability to view pipeline execution logs, re-run jobs, and access the source code repository.&lt;/li>
&lt;li>&lt;strong>Knowledge:&lt;/strong> Familiarity with the application&amp;rsquo;s architecture, build process, and deployment targets.&lt;/li>
&lt;/ul>
&lt;h3 id="steps-for-resolution">
 Steps for Resolution
 
 &lt;a class="anchor" href="#steps-for-resolution">#&lt;/a>
 
&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Identify the Failure Point:&lt;/strong>&lt;/p></description></item><item><title/><link>https://runbooks.prometheus-operator.dev/runbooks/devops/trivy-offline-scan-guide/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/devops/trivy-offline-scan-guide/</guid><description>&lt;h1 id="-trivy-offline-scan-guide">
  Trivy Offline Scan Guide
 
 &lt;a class="anchor" href="#-trivy-offline-scan-guide">#&lt;/a>
 
&lt;/h1>
&lt;p>This guide explains how to run &lt;strong>Trivy scans offline&lt;/strong> using a previously downloaded vulnerability database (DB).&lt;/p>
&lt;hr>
&lt;h2 id="-prerequisites">
  Prerequisites
 
 &lt;a class="anchor" href="#-prerequisites">#&lt;/a>
 
&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Trivy Docker image&lt;/strong> pulled:
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>docker pull aquasec/trivy:latest
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>&lt;strong>Offline DB downloaded&lt;/strong> into &lt;code>./db&lt;/code> (from another machine with internet access):
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>docker run --rm -v ./db:/root/.cache/trivy/db aquasec/trivy:latest image --download-db-only
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="-config-scan-iac-dockerfile-k8s-manifests">
  Config Scan (IaC, Dockerfile, K8s Manifests)
 
 &lt;a class="anchor" href="#-config-scan-iac-dockerfile-k8s-manifests">#&lt;/a>
 
&lt;/h2>
&lt;p>Scan Infrastructure-as-Code configs in a project folder:&lt;/p></description></item><item><title/><link>https://runbooks.prometheus-operator.dev/runbooks/etcd/etcdbackendquotalowspace/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/etcd/etcdbackendquotalowspace/</guid><description>&lt;h1 id="etcdbackendquotalowspace">
 etcdBackendQuotaLowSpace
 
 &lt;a class="anchor" href="#etcdbackendquotalowspace">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>This alert fires when the total existing DB size exceeds 95% of the maximum
DB quota. The consumed space is in Prometheus represented by the metric
&lt;code>etcd_mvcc_db_total_size_in_bytes&lt;/code>, and the DB quota size is defined by
&lt;code>etcd_server_quota_backend_bytes&lt;/code>.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>In case the DB size exceeds the DB quota, no writes can be performed anymore on
the etcd cluster. This further prevents any updates in the cluster, such as the
creation of pods.&lt;/p></description></item><item><title/><link>https://runbooks.prometheus-operator.dev/runbooks/etcd/etcdgrpcrequestsslow/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/etcd/etcdgrpcrequestsslow/</guid><description>&lt;h1 id="etcdgrpcrequestsslow">
 etcdGRPCRequestsSlow
 
 &lt;a class="anchor" href="#etcdgrpcrequestsslow">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>This alert fires when the 99th percentile of etcd gRPC requests are too slow.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>When requests are too slow, they can lead to various scenarios like leader
election failure, slow reads and writes.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>This could be result of slow disk (due to fragmented state) or CPU contention.&lt;/p>
&lt;h3 id="slow-disk">
 Slow disk
 
 &lt;a class="anchor" href="#slow-disk">#&lt;/a>
 
&lt;/h3>
&lt;p>One of the most common reasons for slow gRPC requests is disk. Checking disk
related metrics and dashboards should provide a more clear picture.&lt;/p></description></item><item><title/><link>https://runbooks.prometheus-operator.dev/runbooks/etcd/etcdhighfsyncdurations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/etcd/etcdhighfsyncdurations/</guid><description>&lt;h1 id="etcdhighfsyncdurations">
 etcdHighFsyncDurations
 
 &lt;a class="anchor" href="#etcdhighfsyncdurations">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>This alert fires when the 99th percentile of etcd disk fsync duration is too
high for 10 minutes.&lt;/p>
&lt;details>
&lt;summary>Full context&lt;/summary>
&lt;p>Every write request sent to etcd has to be [fsync&amp;rsquo;d][fsync] to disk by the leader node, transmitted to its peers, and fsync&amp;rsquo;d to those disks as well before etcd can tell the client that the write request succeeded (as part of the [Raft consensus algorithm][raft]). As a result of all those fsync&amp;rsquo;s, etcd cares a LOT about disk latency, which this alert picks up on.&lt;/p></description></item><item><title/><link>https://runbooks.prometheus-operator.dev/runbooks/etcd/etcdhighnumberoffailedgrpcrequests/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/etcd/etcdhighnumberoffailedgrpcrequests/</guid><description>&lt;h1 id="etcdhighnumberoffailedgrpcrequests">
 etcdHighNumberOfFailedGRPCRequests
 
 &lt;a class="anchor" href="#etcdhighnumberoffailedgrpcrequests">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>This alert fires when at least 5% of etcd gRPC requests failed in the past 10
minutes.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>First establish which gRPC method is failing, this will be visible in the alert.
If it&amp;rsquo;s not part of the alert, the following query will display method and etcd
instance that has failing requests:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-promql" data-lang="promql">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#ae81ff">100&lt;/span> &lt;span style="color:#f92672">*&lt;/span> &lt;span style="color:#66d9ef">sum&lt;/span> &lt;span style="color:#66d9ef">without&lt;/span>&lt;span style="color:#f92672">(&lt;/span>grpc_type, grpc_code&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">(&lt;/span>&lt;span style="color:#66d9ef">rate&lt;/span>&lt;span style="color:#f92672">(&lt;/span>grpc_server_handled_total{grpc_code&lt;span style="color:#f92672">=~&lt;/span>&amp;#34;&lt;span style="color:#e6db74">Unknown|FailedPrecondition|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded&lt;/span>&amp;#34;,job&lt;span style="color:#f92672">=&lt;/span>&amp;#34;&lt;span style="color:#e6db74">etcd&lt;/span>&amp;#34;}[&lt;span style="color:#e6db74">5m&lt;/span>]&lt;span style="color:#f92672">))&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">/&lt;/span> &lt;span style="color:#66d9ef">sum&lt;/span> &lt;span style="color:#66d9ef">without&lt;/span>&lt;span style="color:#f92672">(&lt;/span>grpc_type, grpc_code&lt;span style="color:#f92672">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">(&lt;/span>&lt;span style="color:#66d9ef">rate&lt;/span>&lt;span style="color:#f92672">(&lt;/span>grpc_server_handled_total{job&lt;span style="color:#f92672">=&lt;/span>&amp;#34;&lt;span style="color:#e6db74">etcd&lt;/span>&amp;#34;}[&lt;span style="color:#e6db74">5m&lt;/span>]&lt;span style="color:#f92672">))&lt;/span> &lt;span style="color:#f92672">&amp;gt;&lt;/span> &lt;span style="color:#ae81ff">5&lt;/span> &lt;span style="color:#f92672">and&lt;/span> &lt;span style="color:#66d9ef">on&lt;/span>&lt;span style="color:#f92672">()&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f92672">(&lt;/span>&lt;span style="color:#66d9ef">sum&lt;/span>&lt;span style="color:#f92672">(&lt;/span>cluster_infrastructure_provider{type&lt;span style="color:#f92672">!~&lt;/span>&amp;#34;&lt;span style="color:#e6db74">ipi|BareMetal&lt;/span>&amp;#34;} &lt;span style="color:#f92672">==&lt;/span> &lt;span style="color:#66d9ef">bool&lt;/span> &lt;span style="color:#ae81ff">1&lt;/span>&lt;span style="color:#f92672">))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>All the gRPC errors should also be logged in each respective etcd instance logs.
You can get the instance name from the alert that is firing or by running the
query detailed above. Those etcd instance logs should serve as further insight
into what is wrong.&lt;/p></description></item><item><title/><link>https://runbooks.prometheus-operator.dev/runbooks/etcd/etcdinsufficientmembers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/etcd/etcdinsufficientmembers/</guid><description>&lt;h1 id="etcdinsufficientmembers">
 etcdInsufficientMembers
 
 &lt;a class="anchor" href="#etcdinsufficientmembers">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>This alert fires when there are fewer instances available than are needed by
etcd to be healthy.
This means that etcd cluster has not enough members in the cluster to create quorum.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>When etcd does not have a majority of instances available the Kubernetes and
OpenShift APIs will reject read and write requests and operations that preserve
the health of workloads cannot be performed.&lt;/p></description></item><item><title/><link>https://runbooks.prometheus-operator.dev/runbooks/etcd/etcdmembersdown/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/etcd/etcdmembersdown/</guid><description>&lt;h1 id="etcdmembersdown">
 etcdMembersDown
 
 &lt;a class="anchor" href="#etcdmembersdown">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>This alert fires when one or more etcd member goes down and evaluates the
number of etcd members that are currently down. Often, this alert was observed
as part of a cluster upgrade when a master node is being upgraded and requires a
reboot.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>In etcd a majority of (n/2)+1 has to agree on membership changes or key-value
upgrade proposals. With this approach, a split-brain inconsistency can be
avoided. In the case that only one member is down in a 3-member cluster, it
still can make forward progress. Due to the fact that the quorum is 2 and 2
members are still alive. However, when more members are down, the cluster
becomes unrecoverable.&lt;/p></description></item><item><title/><link>https://runbooks.prometheus-operator.dev/runbooks/etcd/etcdnoleader/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/etcd/etcdnoleader/</guid><description>&lt;h1 id="etcdnoleader">
 etcdNoLeader
 
 &lt;a class="anchor" href="#etcdnoleader">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>This alert is triggered when etcd cluster does not have a leader for more than 1
minute.
This can happen if nodes from the cluster are orphaned - they were part of the cluster
but now they are in minority and thus can not form a cluster,
for example due to network partition.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>When there is no leader, Kubernetes API will not be able to work
as expected and cluster cannot process any writes or reads, and any write
requests are queued for processing until a new leader is elected. Operations
that preserve the health of the workloads cannot be performed.&lt;/p></description></item><item><title/><link>https://runbooks.prometheus-operator.dev/runbooks/general/targetdown/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/general/targetdown/</guid><description>&lt;h1 id="targetdown">
 TargetDown
 
 &lt;a class="anchor" href="#targetdown">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>The alert means that one or more prometheus scrape targets are down. It fires when at least 10% of scrape targets in a Service are unreachable.&lt;/p>
&lt;details>
&lt;summary>Full context&lt;/summary>
&lt;p>Prometheus works by sending an HTTP GET request to all of its &amp;ldquo;targets&amp;rdquo; every few seconds. So TargetDown really means that Prometheus just can&amp;rsquo;t access your service, which may or may not mean it&amp;rsquo;s actually down. If your service appears to be running fine, a common cause could be a misconfigured ServiceMonitor (maybe the port or path is incorrect), a misconfigured NetworkPolicy, or Service with incorrect labelSelectors that isn&amp;rsquo;t selecting any Pods.&lt;/p></description></item><item><title/><link>https://runbooks.prometheus-operator.dev/runbooks/gitops/argocd-application-bootstrap/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/gitops/argocd-application-bootstrap/</guid><description>&lt;h3 id="objective">
 Objective
 
 &lt;a class="anchor" href="#objective">#&lt;/a>
 
&lt;/h3>
&lt;p>To provide a standardized procedure for deploying a new application into a Kubernetes cluster using Argo CD, following GitOps principles. This ensures that all application deployments are version-controlled, auditable, and consistently managed.&lt;/p>
&lt;h3 id="prerequisites">
 Prerequisites
 
 &lt;a class="anchor" href="#prerequisites">#&lt;/a>
 
&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Access:&lt;/strong>
&lt;ul>
&lt;li>Read/write access to the Git repository that will store the application&amp;rsquo;s Kubernetes manifests (the &amp;ldquo;config repo&amp;rdquo;).&lt;/li>
&lt;li>&lt;code>kubectl&lt;/code> access to the target Kubernetes cluster with permissions to manage Argo CD resources.&lt;/li>
&lt;li>Access to the Argo CD UI/CLI.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Tools:&lt;/strong> &lt;code>git&lt;/code>, &lt;code>kubectl&lt;/code>, and the &lt;code>argocd&lt;/code> CLI installed and configured.&lt;/li>
&lt;li>&lt;strong>Knowledge:&lt;/strong> Understanding of Kubernetes manifests (Deployments, Services, Ingress, etc.) and basic Git operations.&lt;/li>
&lt;/ul>
&lt;h3 id="steps-for-bootstrapping">
 Steps for Bootstrapping
 
 &lt;a class="anchor" href="#steps-for-bootstrapping">#&lt;/a>
 
&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Prepare Application Manifests:&lt;/strong>&lt;/p></description></item><item><title/><link>https://runbooks.prometheus-operator.dev/runbooks/linux/server-hardening/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/linux/server-hardening/</guid><description>&lt;h3 id="objective">
 Objective
 
 &lt;a class="anchor" href="#objective">#&lt;/a>
 
&lt;/h3>
&lt;p>To provide a comprehensive checklist and set of procedures for securing a newly provisioned Linux server. This runbook ensures a consistent and strong security posture for all servers, minimizing the attack surface.&lt;/p>
&lt;h3 id="prerequisites">
 Prerequisites
 
 &lt;a class="anchor" href="#prerequisites">#&lt;/a>
 
&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Access:&lt;/strong> Root or &lt;code>sudo&lt;/code> access to the target Linux server via SSH.&lt;/li>
&lt;li>&lt;strong>Knowledge:&lt;/strong> Basic Linux command-line skills and an understanding of networking and security concepts.&lt;/li>
&lt;li>&lt;strong>Policy:&lt;/strong> Familiarity with your organization&amp;rsquo;s security policies.&lt;/li>
&lt;/ul>
&lt;h3 id="hardening-checklist-and-procedures">
 Hardening Checklist and Procedures
 
 &lt;a class="anchor" href="#hardening-checklist-and-procedures">#&lt;/a>
 
&lt;/h3>
&lt;h4 id="1-update-the-system">
 1. Update the System
 
 &lt;a class="anchor" href="#1-update-the-system">#&lt;/a>
 
&lt;/h4>
&lt;ul>
&lt;li>&lt;strong>Action:&lt;/strong> Immediately apply all available security patches and updates.&lt;/li>
&lt;li>&lt;strong>Command (Ubuntu/Debian):&lt;/strong>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>sudo apt update &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> sudo apt upgrade -y
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>&lt;strong>Command (CentOS/RHEL):&lt;/strong>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>sudo dnf update -y
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ul>
&lt;h4 id="2-configure-user-access">
 2. Configure User Access
 
 &lt;a class="anchor" href="#2-configure-user-access">#&lt;/a>
 
&lt;/h4>
&lt;ul>
&lt;li>&lt;strong>Action:&lt;/strong> Disable direct root login and create a dedicated administrative user.&lt;/li>
&lt;li>&lt;strong>Steps:&lt;/strong>
&lt;ol>
&lt;li>Create a new user: &lt;code>sudo adduser &amp;lt;adminuser&amp;gt;&lt;/code>&lt;/li>
&lt;li>Add the user to the &lt;code>sudo&lt;/code> group: &lt;code>sudo usermod -aG sudo &amp;lt;adminuser&amp;gt;&lt;/code>&lt;/li>
&lt;li>Disable root login via SSH. Edit &lt;code>/etc/ssh/sshd_config&lt;/code>:
&lt;pre tabindex="0">&lt;code>PermitRootLogin no
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>Restart the SSH service: &lt;code>sudo systemctl restart sshd&lt;/code>&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;h4 id="3-secure-ssh">
 3. Secure SSH
 
 &lt;a class="anchor" href="#3-secure-ssh">#&lt;/a>
 
&lt;/h4>
&lt;ul>
&lt;li>&lt;strong>Action:&lt;/strong> Harden the SSH configuration to prevent unauthorized access.&lt;/li>
&lt;li>&lt;strong>Steps:&lt;/strong> Edit &lt;code>/etc/ssh/sshd_config&lt;/code> and set the following (at a minimum):
&lt;pre tabindex="0">&lt;code>PasswordAuthentication no # Enforce key-based authentication only
X11Forwarding no # Disable X11 forwarding unless needed
AllowUsers &amp;lt;adminuser&amp;gt; &amp;lt;otheruser&amp;gt; # Whitelist allowed SSH users
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>&lt;strong>Verification:&lt;/strong> After restarting &lt;code>sshd&lt;/code>, ensure you can still log in with your admin user via an SSH key.&lt;/li>
&lt;/ul>
&lt;h4 id="4-configure-a-firewall">
 4. Configure a Firewall
 
 &lt;a class="anchor" href="#4-configure-a-firewall">#&lt;/a>
 
&lt;/h4>
&lt;ul>
&lt;li>&lt;strong>Action:&lt;/strong> Enable and configure a host-based firewall to block all traffic except for necessary services.&lt;/li>
&lt;li>&lt;strong>Tool:&lt;/strong> &lt;code>ufw&lt;/code> (Uncomplicated Firewall) is common on Ubuntu.&lt;/li>
&lt;li>&lt;strong>Commands:&lt;/strong>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>sudo ufw default deny incoming &lt;span style="color:#75715e"># Deny all incoming traffic by default&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sudo ufw default allow outgoing &lt;span style="color:#75715e"># Allow all outgoing traffic&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sudo ufw allow ssh &lt;span style="color:#75715e"># Allow SSH (port 22)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sudo ufw allow http &lt;span style="color:#75715e"># Allow HTTP (port 80) if it&amp;#39;s a web server&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sudo ufw allow https &lt;span style="color:#75715e"># Allow HTTPS (port 443) if it&amp;#39;s a web server&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sudo ufw enable &lt;span style="color:#75715e"># Enable the firewall&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>&lt;strong>Verification:&lt;/strong> &lt;code>sudo ufw status verbose&lt;/code>&lt;/li>
&lt;/ul>
&lt;h4 id="5-remove-unnecessary-services">
 5. Remove Unnecessary Services
 
 &lt;a class="anchor" href="#5-remove-unnecessary-services">#&lt;/a>
 
&lt;/h4>
&lt;ul>
&lt;li>&lt;strong>Action:&lt;/strong> Reduce the attack surface by uninstalling or disabling any software and services that are not required.&lt;/li>
&lt;li>&lt;strong>Steps:&lt;/strong>
&lt;ol>
&lt;li>List installed packages to identify candidates for removal.&lt;/li>
&lt;li>List running services: &lt;code>sudo systemctl list-units --type=service --state=running&lt;/code>&lt;/li>
&lt;li>Uninstall a package (e.g., &lt;code>telnet&lt;/code> server): &lt;code>sudo apt remove telnetd -y&lt;/code>&lt;/li>
&lt;li>Disable a service from starting on boot: &lt;code>sudo systemctl disable &amp;lt;service-name&amp;gt;&lt;/code>&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;h4 id="6-install-security-tools">
 6. Install Security Tools
 
 &lt;a class="anchor" href="#6-install-security-tools">#&lt;/a>
 
&lt;/h4>
&lt;ul>
&lt;li>&lt;strong>Action:&lt;/strong> Install tools for intrusion detection and monitoring.&lt;/li>
&lt;li>&lt;strong>Recommendations:&lt;/strong>
&lt;ul>
&lt;li>&lt;strong>Fail2Ban:&lt;/strong> Monitors logs for malicious activity (like brute-force login attempts) and temporarily bans offending IPs.
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>sudo apt install fail2ban -y
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#75715e"># Default configuration is usually sufficient to start.&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>&lt;strong>Auditd:&lt;/strong> The Linux Auditing System provides detailed logging of security-relevant events.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="verification">
 Verification
 
 &lt;a class="anchor" href="#verification">#&lt;/a>
 
&lt;/h3>
&lt;ul>
&lt;li>Perform a port scan of the server from an external machine (e.g., using &lt;code>nmap&lt;/code>) to confirm that only expected ports are open.
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>nmap -sT -p- &amp;lt;server-ip&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>Attempt to log in as &lt;code>root&lt;/code> via SSH and confirm it is denied.&lt;/li>
&lt;li>Attempt to log in with a password for your admin user and confirm it is denied (if key-only auth was enforced).&lt;/li>
&lt;/ul>
&lt;h3 id="rollback-plan">
 Rollback Plan
 
 &lt;a class="anchor" href="#rollback-plan">#&lt;/a>
 
&lt;/h3>
&lt;ul>
&lt;li>If a hardening step locks you out (e.g., incorrect firewall rule), you will need console access (physical or via your cloud provider&amp;rsquo;s console) to log in and revert the change.&lt;/li>
&lt;li>Always apply firewall and SSH changes with an active console session open as a backup.&lt;/li>
&lt;/ul>
&lt;h3 id="escalation">
 Escalation
 
 &lt;a class="anchor" href="#escalation">#&lt;/a>
 
&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Who:&lt;/strong> Senior System Administrator or Security Team.&lt;/li>
&lt;li>&lt;strong>When:&lt;/strong> If you are unsure about the impact of a specific hardening setting or if you suspect a security compromise during the provisioning process.&lt;/li>
&lt;/ul></description></item></channel></rss>