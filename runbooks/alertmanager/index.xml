<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>alertmanager on kube-prometheus runbooks</title><link>https://hoanggvuong.github.io/runbooks/alertmanager/</link><description>Recent content in alertmanager on kube-prometheus runbooks</description><generator>Hugo</generator><language>en-us</language><atom:link href="https://hoanggvuong.github.io/runbooks/alertmanager/index.xml" rel="self" type="application/rss+xml"/><item><title>Alertmanager Cluster Crashlooping</title><link>https://hoanggvuong.github.io/runbooks/alertmanager/alertmanagerclustercrashlooping/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://hoanggvuong.github.io/runbooks/alertmanager/alertmanagerclustercrashlooping/</guid><description>&lt;h1 id="alertmanagerclustercrashlooping">
 AlertmanagerClusterCrashlooping
 
 &lt;a class="anchor" href="#alertmanagerclustercrashlooping">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Half or more of the Alertmanager instances within the same cluster are crashlooping.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Alerts could be notified multiple time unless pods are crashing to fast and no alerts can be sent.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl get pod -l app&lt;span style="color:#f92672">=&lt;/span>alertmanager
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAMESPACE NAME READY STATUS RESTARTS AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>default alertmanager-main-0 1/2 CrashLoopBackOff &lt;span style="color:#ae81ff">37107&lt;/span> 2d
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>default alertmanager-main-1 2/2 Running &lt;span style="color:#ae81ff">0&lt;/span> 43d
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>default alertmanager-main-2 2/2 Running &lt;span style="color:#ae81ff">0&lt;/span> 43d 
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Find the root cause by looking to events for a given pod/deployement&lt;/p></description></item><item><title>Alertmanager Cluster Down</title><link>https://hoanggvuong.github.io/runbooks/alertmanager/alertmanagerclusterdown/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://hoanggvuong.github.io/runbooks/alertmanager/alertmanagerclusterdown/</guid><description>&lt;h1 id="alertmanagerclusterdown">
 AlertmanagerClusterDown
 
 &lt;a class="anchor" href="#alertmanagerclusterdown">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Half or more of the Alertmanager instances within the same cluster are down.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>You have an unstable cluster, if everything goes wrong you will lose the whole cluster.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Verify why pods are not running.
You can get a big picture with &lt;code>events&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>$ kubectl get events --field-selector involvedObject.kind&lt;span style="color:#f92672">=&lt;/span>Pod | grep alertmanager
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>There are no cheap options to mitigate this risk.
Verifying any new changes in preprod before production environment should improve stability.&lt;/p></description></item><item><title>Alertmanager Cluster Failed To Send Alerts</title><link>https://hoanggvuong.github.io/runbooks/alertmanager/alertmanagerclusterfailedtosendalerts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://hoanggvuong.github.io/runbooks/alertmanager/alertmanagerclusterfailedtosendalerts/</guid><description>&lt;h1 id="alertmanagerclusterfailedtosendalerts">
 AlertmanagerClusterFailedToSendAlerts
 
 &lt;a class="anchor" href="#alertmanagerclusterfailedtosendalerts">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>All instances failed to send notification to an integration.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>You will not receive a notification when an alert is raised.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>No alerts are received at the integration level from the cluster.&lt;/p>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>Depending on the integration, correct the integration with the faulty instance (network, authorization token, firewall&amp;hellip;)&lt;/p></description></item><item><title>Alertmanager ConfigInconsistent</title><link>https://hoanggvuong.github.io/runbooks/alertmanager/alertmanagerconfiginconsistent/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://hoanggvuong.github.io/runbooks/alertmanager/alertmanagerconfiginconsistent/</guid><description>&lt;h1 id="alertmanagerconfiginconsistent">
 AlertmanagerConfigInconsistent
 
 &lt;a class="anchor" href="#alertmanagerconfiginconsistent">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>The configuration between instances inside a cluster is inconsistent.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Configuration inconsistency can be multiple and impact is hard to predict.
Nevertheless, in most cases the alert might be lost or routed to the incorrect integration.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Run a &lt;code>diff&lt;/code> tool between all &lt;code>alertmanager.yml&lt;/code> that are deployed to find what is wrong.
You could run a job within your CI to avoid this issue in the future.&lt;/p></description></item><item><title>Alertmanager Failed Reload</title><link>https://hoanggvuong.github.io/runbooks/alertmanager/alertmanagerfailedreload/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://hoanggvuong.github.io/runbooks/alertmanager/alertmanagerfailedreload/</guid><description>&lt;h1 id="alertmanagerfailedreload">
 AlertmanagerFailedReload
 
 &lt;a class="anchor" href="#alertmanagerfailedreload">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>The alert &lt;code>AlertmanagerFailedReload&lt;/code> is triggered when the Alertmanager instance
for the cluster monitoring stack has consistently failed to reload its
configuration for a certain period.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>The impact depends on the type of the error you will find in the logs.
Most of the time, previous configuration is still working, thanks to multiple
instances, so avoid deleting existing pods.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Verify if there is an error in &lt;code>config-reloader&lt;/code> container logs.
Here an example with network issues.&lt;/p></description></item><item><title>Alertmanager Failed To Send Alerts</title><link>https://hoanggvuong.github.io/runbooks/alertmanager/alertmanagerfailedtosendalerts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://hoanggvuong.github.io/runbooks/alertmanager/alertmanagerfailedtosendalerts/</guid><description>&lt;h1 id="alertmanagerfailedtosendalerts">
 AlertmanagerFailedToSendAlerts
 
 &lt;a class="anchor" href="#alertmanagerfailedtosendalerts">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>At least one instance is unable to routed alert to the corresponding integration.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>No impact since another instance should be able to send the notification,
unless &lt;code>AlertmanagerClusterFailedToSendAlerts&lt;/code> is also triggerd for the same integration.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Verify the amount of failed notification per alert-manager-[instance] for
a specific integration.&lt;/p>
&lt;p>You can look metrics exposed in prometheus console using promQL.
For exemple the following query will display the number of failed
notifications per instance for pager duty integration.
We have 3 instances involved in the example bellow.&lt;/p></description></item><item><title>Alertmanager Members Inconsistent</title><link>https://hoanggvuong.github.io/runbooks/alertmanager/alertmanagermembersinconsistent/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://hoanggvuong.github.io/runbooks/alertmanager/alertmanagermembersinconsistent/</guid><description>&lt;h1 id="alertmanagermembersinconsistent">
 AlertmanagerMembersInconsistent
 
 &lt;a class="anchor" href="#alertmanagermembersinconsistent">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>At least one of alertmanager cluster members cannot be found.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Check if IP addresses discovered by alertmanager cluster are the same ones as in alertmanager Service. Following example show possible inconsistency in Endpoint IP addresses:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>$ kubectl describe svc alertmanager-main
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Name: alertmanager-main
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Namespace: monitoring
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Endpoints: 10.128.2.3:9095,10.129.2.5:9095,10.131.0.44:9095
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>$ kubectl get pod -o wide | grep alertmanager-main
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>alertmanager-main-0 5/5 Running &lt;span style="color:#ae81ff">0&lt;/span> 11d 10.129.2.6
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>alertmanager-main-1 5/5 Running &lt;span style="color:#ae81ff">0&lt;/span> 2d16h 10.131.0.44 
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>alertmanager-main-2 5/5 Running &lt;span style="color:#ae81ff">0&lt;/span> 6d 10.128.2.3 
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>Deleting an incorrect Endpoint should trigger its recreation with a correct IP address.&lt;/p></description></item></channel></rss>