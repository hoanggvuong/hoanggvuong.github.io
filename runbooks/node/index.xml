<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>node on kube-prometheus runbooks</title><link>https://runbooks.prometheus-operator.dev/runbooks/node/</link><description>Recent content in node on kube-prometheus runbooks</description><generator>Hugo</generator><language>en-us</language><atom:link href="https://runbooks.prometheus-operator.dev/runbooks/node/index.xml" rel="self" type="application/rss+xml"/><item><title>Node Clock Not Synchronising</title><link>https://runbooks.prometheus-operator.dev/runbooks/node/nodeclocknotsynchronising/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/node/nodeclocknotsynchronising/</guid><description>&lt;h1 id="nodeclocknotsynchronising">
 NodeClockNotSynchronising
 
 &lt;a class="anchor" href="#nodeclocknotsynchronising">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Clock not synchronising.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Time is not automatically synchronizing on the node. This can cause issues with handling TLS as well as problems with other time-sensitive applications.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>TODO&lt;/p>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>See &lt;a href="https://runbooks.prometheus-operator.dev/runbooks/node/nodeclockskewdetected/">Node Clok Skew Detected&lt;/a> for mitigation steps.&lt;/p></description></item><item><title>Node Clock Skew Detected</title><link>https://runbooks.prometheus-operator.dev/runbooks/node/nodeclockskewdetected/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/node/nodeclockskewdetected/</guid><description>&lt;h1 id="nodeclockskewdetected">
 NodeClockSkewDetected
 
 &lt;a class="anchor" href="#nodeclockskewdetected">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Clock skew detected.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Time is skewed on the node. This can cause issues with handling TLS as well as problems with other time-sensitive applications.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>TODO&lt;/p>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>Ensure time synchronization service is running.
Set proper time servers.
Esure to sync time on server start, especially when using
low power mode or hibernation.&lt;/p>
&lt;p>Some resource consuming process can cause issues on given hardware,
so move it to different servers.&lt;/p></description></item><item><title>Node File Descriptor Limit</title><link>https://runbooks.prometheus-operator.dev/runbooks/node/nodefiledescriptorlimit/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/node/nodefiledescriptorlimit/</guid><description>&lt;h1 id="nodefiledescriptorlimit">
 NodeFileDescriptorLimit
 
 &lt;a class="anchor" href="#nodefiledescriptorlimit">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>This alert is triggered when a node&amp;rsquo;s kernel is found to be running out of
available file descriptors &amp;ndash; a &lt;code>warning&lt;/code> level alert at greater than 70% usage
and a &lt;code>critical&lt;/code> level alert at greater than 90% usage.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Applications on the node may no longer be able to open and operate on
files. This is likely to have severe consequences for anything scheduled on this
node.&lt;/p></description></item><item><title>Node Filesystem Almost Out Of Files</title><link>https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutoffiles/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutoffiles/</guid><description>&lt;h1 id="nodefilesystemalmostoutoffiles">
 NodeFilesystemAlmostOutOfFiles
 
 &lt;a class="anchor" href="#nodefilesystemalmostoutoffiles">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>This alert is similar to the NodeFilesystemSpaceFillingUp alert, but rather
than being based on a prediction that a filesystem will run out of inodes in a
certain amount of time, it uses simple static thresholds. The alert will fire as
at a &lt;code>warning&lt;/code> level at 5% of available inodes left, and at a &lt;code>critical&lt;/code> level
with 3% of available inodes left.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>A node&amp;rsquo;s filesystem becoming full can have a far reaching impact, as it may
cause any or all of the applications scheduled to that node to experience
anything from performance degradation to full inoperability. Depending on the
node and filesystem involved, this could pose a critical threat to the stability
of the cluster.&lt;/p></description></item><item><title>Node Filesystem Almost Out Of Space</title><link>https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutofspace/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutofspace/</guid><description>&lt;h1 id="nodefilesystemalmostoutofspace">
 NodeFilesystemAlmostOutOfSpace
 
 &lt;a class="anchor" href="#nodefilesystemalmostoutofspace">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>This alert is similar to the NodeFilesystemSpaceFillingUp alert, but rather
than being based on a prediction that a filesystem will become full in a certain
amount of time, it uses simple static thresholds. The alert will fire as at a
&lt;code>warning&lt;/code> level at 5% space left, and at a &lt;code>critical&lt;/code> level with 3% space left.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>A node&amp;rsquo;s filesystem becoming full can have a far reaching impact, as it may
cause any or all of the applications scheduled to that node to experience
anything from performance degradation to full inoperability. Depending on the
node and filesystem involved, this could pose a critical threat to the stability
of the cluster.&lt;/p></description></item><item><title>Node Filesystem Files Filling Up</title><link>https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemfilesfillingup/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemfilesfillingup/</guid><description>&lt;h1 id="nodefilesystemfilesfillingup">
 NodeFilesystemFilesFillingUp
 
 &lt;a class="anchor" href="#nodefilesystemfilesfillingup">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>This alert is similar to the NodeFilesystemSpaceFillingUp alert, but
predicts the filesystem will run out of inodes rather than bytes of storage
space. The alert fires at a &lt;code>critical&lt;/code> level when the filesystem is predicted to
run out of available inodes within four hours.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>A node&amp;rsquo;s filesystem becoming full can have a far reaching impact, as it may
cause any or all of the applications scheduled to that node to experience
anything from performance degradation to full inoperability. Depending on the
node and filesystem involved, this could pose a critical threat to the stability
of the cluster.&lt;/p></description></item><item><title>Node Filesystem Space Filling Up</title><link>https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemspacefillingup/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemspacefillingup/</guid><description>&lt;h1 id="nodefilesystemspacefillingup">
 NodeFilesystemSpaceFillingUp
 
 &lt;a class="anchor" href="#nodefilesystemspacefillingup">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>This alert is based on an extrapolation of the space used in a file system. It
fires if both the current usage is above a certain threshold &lt;em>and&lt;/em> the
extrapolation predicts to run out of space in a certain time. This is a
warning-level alert if that time is less than 24h. It&amp;rsquo;s a critical alert if that
time is less than 4h.&lt;/p>
&lt;details>
&lt;summary>Full context&lt;/summary>
&lt;p>The filesystem on Kubernetes nodes mainly consists of the operating system,
[container ephemeral storage][1], container images, and container logs.
Since Kubelet automatically handles [cleaning up old logs][2] and
[deleting unused images][3], container ephemeral storage is a common cause of
this alert. Although this alert may be triggered before Kubelet&amp;rsquo;s garbage
collection kicks in.&lt;/p></description></item><item><title>Node High Number Conntrack Entries Used</title><link>https://runbooks.prometheus-operator.dev/runbooks/node/nodehighnumberconntrackentriesused/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/node/nodehighnumberconntrackentriesused/</guid><description>&lt;h1 id="nodehighnumberconntrackentriesused">
 NodeHighNumberConntrackEntriesUsed
 
 &lt;a class="anchor" href="#nodehighnumberconntrackentriesused">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Number of conntrack are getting close to the limit.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>When reached the limit then some connections will be dropped, degrading service quality.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Check current conntrack value on the node.
Check which apps are generating a lot of connections.&lt;/p>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>Migrate some pods to another nodes.
Bump conntrack limit directly on the node, remembering to make it persistent across node reboots.&lt;/p></description></item><item><title>Node Network Receive Errors</title><link>https://runbooks.prometheus-operator.dev/runbooks/node/nodenetworkreceiveerrs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/node/nodenetworkreceiveerrs/</guid><description>&lt;h1 id="nodenetworkreceiveerrs">
 NodeNetworkReceiveErrs
 
 &lt;a class="anchor" href="#nodenetworkreceiveerrs">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Network interface is reporting many receive errors.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Applications on the node may no longer be able to operate with other services.
Network attached storage performance issues or even data loss.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Investigate networking issues on the node and to connected hardware.
Check physical cables, check networking firewall rules and so on.&lt;/p>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>In general mitigation landscape is quite vast, some suggestions:&lt;/p></description></item><item><title>Node Network Transmit Errors</title><link>https://runbooks.prometheus-operator.dev/runbooks/node/nodenetworktransmiterrs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/node/nodenetworktransmiterrs/</guid><description>&lt;h1 id="nodenetworktransmiterrs">
 NodeNetworkTransmitErrs
 
 &lt;a class="anchor" href="#nodenetworktransmiterrs">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Network interface is reporting many transmit errors.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Applications on the node may no longer be able to operate with other services.
Network attached storage performance issues or even data loss.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>Investigate networking issues on the node and to connected hardware.
Check network interface saturation.
Check CPU usage saturation.
Check physical cables, check networking firewall rules and so on.&lt;/p></description></item><item><title>Node RAID Degraded</title><link>https://runbooks.prometheus-operator.dev/runbooks/node/noderaiddegraded/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/node/noderaiddegraded/</guid><description>&lt;h1 id="noderaiddegraded">
 NodeRAIDDegraded
 
 &lt;a class="anchor" href="#noderaiddegraded">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>RAID Array is degraded.&lt;/p>
&lt;p>This alert is triggered when a node has a storage configuration with RAID array,
and the array is reporting as being in a degraded state due to one or more disk
failures.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>The affected node could go offline at any moment if the RAID array fully fails
due to further issues with disks.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;p>You can open a shell on the node and use the standard Linux utilities to
diagnose the issue, but you may need to install additional software in the debug
container:&lt;/p></description></item><item><title>Node RAID Disk Failure</title><link>https://runbooks.prometheus-operator.dev/runbooks/node/noderaiddiskfailure/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/node/noderaiddiskfailure/</guid><description>&lt;h1 id="noderaiddiskfailure">
 NodeRAIDDiskFailure
 
 &lt;a class="anchor" href="#noderaiddiskfailure">#&lt;/a>
 
&lt;/h1>
&lt;p>See &lt;a href="https://runbooks.prometheus-operator.dev/runbooks/node/noderaiddegraded/">Node RAID Degraded&lt;/a>&lt;/p></description></item><item><title>Node Text File Collector Scrape Error</title><link>https://runbooks.prometheus-operator.dev/runbooks/node/nodetextfilecollectorscrapeerror/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://runbooks.prometheus-operator.dev/runbooks/node/nodetextfilecollectorscrapeerror/</guid><description>&lt;h1 id="nodetextfilecollectorscrapeerror">
 NodeTextFileCollectorScrapeError
 
 &lt;a class="anchor" href="#nodetextfilecollectorscrapeerror">#&lt;/a>
 
&lt;/h1>
&lt;h2 id="meaning">
 Meaning
 
 &lt;a class="anchor" href="#meaning">#&lt;/a>
 
&lt;/h2>
&lt;p>Node Exporter text file collector failed to scrape.&lt;/p>
&lt;h2 id="impact">
 Impact
 
 &lt;a class="anchor" href="#impact">#&lt;/a>
 
&lt;/h2>
&lt;p>Missing metrics from additional scripts.&lt;/p>
&lt;h2 id="diagnosis">
 Diagnosis
 
 &lt;a class="anchor" href="#diagnosis">#&lt;/a>
 
&lt;/h2>
&lt;ul>
&lt;li>Check node_exporter logs&lt;/li>
&lt;li>Check script supervisor (like systemd or cron) for more information about failed script execution&lt;/li>
&lt;/ul>
&lt;h2 id="mitigation">
 Mitigation
 
 &lt;a class="anchor" href="#mitigation">#&lt;/a>
 
&lt;/h2>
&lt;p>Check if provided configuration is valid, if files were not renamed during upgrades.&lt;/p></description></item></channel></rss>